{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa983342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import timm\n",
    "import albumentations as A\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "from skimage import io\n",
    "from custom import U2NET\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b029cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cos(np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df04cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "t_range = np.arange(0, 2*np.pi, 0.05)\n",
    "for t in t_range:\n",
    "    #print((math.sin (t) * t, math.cos(t) * t))\n",
    "    plt.plot(np.sin(t) * t, np.cos(t) * t, markersize=1, marker='o')\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_color('none')\n",
    "    ax.spines['bottom'].set_position('zero')\n",
    "    ax.spines['left'].set_position('zero')\n",
    "    ax.spines['right'].set_color('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea980cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "timm.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d33b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.Resize(width=128, height=128),\n",
    "    A.RandomRotate90(p = 1),\n",
    "    A.Transpose(p=1),\n",
    "    A.RandomBrightness(p=1),\n",
    "    A.RandomContrast(p=1),\n",
    "    A.RandomShadow(p=1),\n",
    "    A.RGBShift(p=1),\n",
    "    A.RandomContrast(p=1),\n",
    "])\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(width=128, height=128),\n",
    "    A.GridDistortion(p=1),\n",
    "    A.OpticalDistortion(p=1),\n",
    "    A.ElasticTransform(p=1)\n",
    "\n",
    "])\n",
    "\n",
    "image =  io.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\CellsFinal\\Cells\\right side\\cells\\train\\images\\05__1_5291_9286.png\")\n",
    "mask = io.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\CellsFinal\\Cells\\right side\\cells\\train\\labels\\05__1_5291_9286.png\", as_gray = True)\n",
    "io.imsave(\"orig.png\", image)\n",
    "io.imsave(\"origmask.png\", mask)\n",
    "trans = transform(image = image, mask = mask)\n",
    "image = trans[\"image\"]\n",
    "mask = trans[\"mask\"]\n",
    "io.imsave(\"image.png\", image)\n",
    "io.imsave(\"mask.png\", mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10abfd40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d8b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0195a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(encoder_name = \"timm-resnest50d\", classes = 2, in_channels = 3)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0a49e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\timm-resnest50d, Unet.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a295f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells\\Cells\\test\\images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db05a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt, im in enumerate(os.listdir(image_dir)):\n",
    "    im = io.imread(os.path.join(image_dir, im))\n",
    "    im = im/255.\n",
    "    im = transform(image = im)[\"image\"]\n",
    "    im = torch.tensor(im, dtype = torch.float).permute(2, 0, 1).unsqueeze(0)\n",
    "    print(im.shape)\n",
    "    pred = model(im)\n",
    "    pred = torch.argmax(pred, 1)[0] * 255.\n",
    "    cv2.imwrite(str(cnt) + \".png\", pred.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390b4a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = cv2.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells1\\Cells\\train\\labels\\05__1_4685_10225.png\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c9ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = ii * 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a1d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"test.png\", ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187c5777",
   "metadata": {},
   "source": [
    "# Replace 2D with 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b75530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e72369",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, modules in model.named_modules():\n",
    "    for module in modules:\n",
    "        print(module)\n",
    "    if(isinstance(module, nn.Conv2d)):\n",
    "        kernel_size = module.kernel_size[0]\n",
    "        stride = module.stride[0]\n",
    "        padding = module.padding[0]\n",
    "        weight = module.weight.unsqueeze(2) / kernel_size\n",
    "        weight = torch.cat([weight for _ in range(0, kernel_size)], dim=2)\n",
    "        bias = module.bias\n",
    "\n",
    "        if(bias is None):\n",
    "            print(modules)\n",
    "            print(modules[name])\n",
    "            modules[name] = nn.Conv3d(in_channels=module.weight.shape[1], out_channels=module.weight.shape[0],\n",
    "                               kernel_size=kernel_size, padding=padding, stride=stride, bias=False)\n",
    "        else:\n",
    "            modules[name] = nn.Conv3d(in_channels=module.weight.shape[1], out_channels=module.weight.shape[0],\n",
    "                               kernel_size=kernel_size, padding=padding, stride=stride, bias=True)\n",
    "            modules[name].bias = bias\n",
    "\n",
    "            modules[name].weight.data = weight\n",
    "\n",
    "    elif(isinstance(module, nn.BatchNorm2d)):\n",
    "        weight = module.weight\n",
    "        bias = module.bias\n",
    "        modules[name] = nn.BatchNorm3d(weight.shape[0])\n",
    "        modules[name].weight = weight\n",
    "        modules[name].bias = bias\n",
    "\n",
    "for name in modules:\n",
    "    parent_module = model\n",
    "    objs = name.split(\".\")\n",
    "    if len(objs) == 1:\n",
    "        model.__setattr__(name, modules[name])\n",
    "        continue\n",
    "\n",
    "    for obj in objs[:-1]:\n",
    "        parent_module = parent_module.__getattr__(obj)\n",
    "\n",
    "    parent_module.__setattr__(objs[-1], modules[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609929d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_modules(module):\n",
    "    for a in module:\n",
    "        get_all_modules(a.children())\n",
    "        if isinstance(a, nn.Conv2d):\n",
    "            print(a)\n",
    "            #a = nn.Conv3d(3,32,3)\n",
    "            \n",
    "        elif isinstance(a, nn.BatchNorm2d):\n",
    "            print(a)\n",
    "\n",
    "            \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2654f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_modules(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5713df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in model.encoder.named_modules():\n",
    "    print(a)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc30e742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from uformer_pytorch import Uformer\n",
    "\n",
    "model = Uformer(\n",
    "    dim = 16,           # initial dimensions after input projection, which increases by 2x each stage\n",
    "    stages = 4,         # number of stages\n",
    "    num_blocks = 2,     # number of transformer blocks per stage\n",
    "    window_size = 16,   # set window size (along one side) for which to do the attention within\n",
    "    dim_head = 64,\n",
    "    heads = 1,\n",
    "    ff_mult = 4\n",
    ")\n",
    "model.cuda()\n",
    "x = torch.randn(1, 3, 512, 512).cuda()\n",
    "pred = model(x) # (1, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb84406",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dffd467",
   "metadata": {},
   "source": [
    "# Segmentation inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cc3c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualdl import vdl\n",
    "from skimage.io import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ba3d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [imread(r\"E:\\source\\repos\\Daten\\ObjectDetection\\tumor\\train\\images\\PD-L1=2_0_37392-42804_85.png\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdl.predict(images, r\"E:\\source\\repos\\VisualDL\\tu-resnest50d, Unet.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f1c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uformer_pytorch import Uformer\n",
    "model = U2NET(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d91f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.create_model(arch = \"UnetPlusPlus\", encoder_name = \"tu-resnest50d\", classes = 2, in_channels = 3, image_size = 512, decoder_attention_type = \"scse\")\n",
    "model.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\tu-resnest50d, UnetPlusPlus.pt\")['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d196607",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()\n",
    "#second = second.cuda()\n",
    "#third = third.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4547c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.Resize(width=256, height=256),\n",
    "])\n",
    "test = r\"E:/source/repos/Daten/HER-N/hubt/dataset/Cells/valid/images\"\n",
    "for cnt, im in enumerate(os.listdir(test)):\n",
    "    image = io.imread(os.path.join(test, im))\n",
    "    image = transform(image = image)[\"image\"]\n",
    "    image = image/255.\n",
    "    s = torch.unsqueeze(torch.tensor(image, dtype = torch.float).permute(2, 0, 1), 0).cuda()\n",
    "    with torch.cuda.amp.autocast():\n",
    "        preds = model(s)\n",
    "\n",
    "    preds = torch.argmax(preds, 1)\n",
    "    preds *= 255\n",
    "#     preds[preds == 1] = 50\n",
    "#     preds[preds == 2] = 100\n",
    "#     preds[preds == 3] = 150\n",
    "#     preds[preds == 4] = 200\n",
    "#     preds[preds == 5] = 250\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    io.imsave(f\"{im}.png\", preds[0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79278e31",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d4df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells\\train\\labels\"\n",
    "ab = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells\\train\\bs\"\n",
    "for cnt, im in enumerate(os.listdir(test)):\n",
    "    img = cv2.imread(os.path.join(test, im)) * 255.\n",
    "    kernel = np.ones((3, 3), 'uint8')\n",
    "    dilate_img = cv2.dilate(img, kernel, iterations=1)\n",
    "    img1_bg = dilate_img - img\n",
    "    img1 = img1_bg[:,:,0]\n",
    "    clipped = np.clip(img1, 1, 6) # weight edges by factor (e.g. 6)\n",
    "    print(np.min(clipped))\n",
    "    cv2.imwrite(os.path.join(ab, im),clipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78dc66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch.losses import DiceLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af932e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DiceLoss(reduce = \"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8963cd",
   "metadata": {},
   "source": [
    "# Classification inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf46979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from skimage import io\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f6ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = timm.create_model(\"resnext50_32x4d\", pretrained=True, num_classes = 5).cuda()\n",
    "#second = timm.create_model(\"resnext50d_32x4d\", pretrained=True, num_classes = 5).cuda()\n",
    "first.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\resnext50_32x4d.pt\"))\n",
    "#second.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\resnext50d_32x4d.pt\"))\n",
    "first.eval()\n",
    "#second.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a827dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = timm.create_model(\"resnext50_32x4d\", pretrained=True, num_classes = 5).cuda()\n",
    "second = timm.create_model(\"efficientnet_b4\", pretrained=True, num_classes = 5).cuda()\n",
    "first.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\resnext50_32x4d.pt\"))\n",
    "#second.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\efficientnet_b4.pt\"))\n",
    "first.eval()\n",
    "#second.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30bc34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "transform = A.Compose([\n",
    "    A.Resize(width=512, height=512),\n",
    "])\n",
    "counter = 0\n",
    "counterxd = 0\n",
    "names = [\"NA\", \"TRG0\", \"TRG1\", \"TRG2\", \"TRG3\"]\n",
    "for name in names:\n",
    "    os.mkdir(name)\n",
    "values = dict()\n",
    "for cnt, name in enumerate(names):\n",
    "    values[name] = []\n",
    "    base = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset_valid/\" + name\n",
    "    for im in os.listdir(base):\n",
    "        image = io.imread(os.path.join(base, im)).astype(np.float32)\n",
    "        image = transform(image = image)[\"image\"]\n",
    "        image = image/255.\n",
    "        s = torch.unsqueeze(torch.tensor(image, dtype = torch.float).permute(2, 0, 1), 0).cuda()\n",
    "        preds =  first(s) \n",
    "        preds = torch.argmax(preds, 1)\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        values[name].append(preds[0])\n",
    "        counterxd += 1\n",
    "        io.imsave(f\"{names[preds[0]]}/{im}.png\", image)\n",
    "        if cnt == preds:\n",
    "            counter += 1\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076f2189",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter/counterxd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de14543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'NA': [4, 0, 0, 2, 0, 0],\n",
    " 'TRG0': [1, 4],\n",
    " 'TRG1': [2, 2, 2],\n",
    " 'TRG2': [3, 4, 3],\n",
    " 'TRG3': [4, 4, 4, 4, 4, 4, 4, 0, 4, 2, 4, 4, 4, 4, 4, 4, 2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d2f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset_valid\\TRG2\\E93 L X20_0_1183_3925.png\"\n",
    "base = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset_valid\\TRG2\"\n",
    "\n",
    "\n",
    "image = io.imread(path)\n",
    "image = image/255.\n",
    "s = torch.unsqueeze(torch.tensor(image, dtype = torch.float).permute(2, 0, 1), 0).cuda()\n",
    "preds = first(s) + second(s)\n",
    "preds = torch.argmax(preds, 1)\n",
    "preds = preds.detach().cpu().numpy()\n",
    "print(preds)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0872d3",
   "metadata": {},
   "source": [
    "# Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab262af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "start = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\test_data\\test_data\"\n",
    "\n",
    "for im in os.listdir(os.path.join(start, \"images\")):\n",
    "    img = cv2.imread(os.path.join(os.path.join(start, \"images\"), im), 0)\n",
    "    mask = cv2.imread(os.path.join(os.path.join(start, \"masks\"), im), 0)\n",
    "    mask[img == 0] = 0\n",
    "    mask[mask == 0] = 0\n",
    "    mask[mask == 29] = 1\n",
    "    mask[mask == 105] = 2\n",
    "    mask[mask == 117] = 3\n",
    "    mask[mask ==189] = 4\n",
    "    mask[mask == 225] = 5\n",
    "    mask[mask > 5] = 1\n",
    "    print(os.path.join(os.path.join(start, \"labels\"), im))\n",
    "    cv2.imwrite(os.path.join(os.path.join(start, \"labels\"), im), mask)\n",
    "    #img[img > 0] = 1\n",
    "    #cv2.imwrite(os.path.join(start, im), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebdae52",
   "metadata": {},
   "source": [
    "# Trian test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2884ed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = r\"E:\\source\\repos\\Daten\\PLA\\train\\images\"\n",
    "to = r\"E:\\source\\repos\\Daten\\PLA\\val\\images\"\n",
    "import random\n",
    "from shutil import copyfile\n",
    "import os\n",
    "split = os.listdir(start)\n",
    "random.shuffle(split)\n",
    "\n",
    "test = split[0:35]\n",
    "train = split[35:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c565f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in test:\n",
    "    copyfile(os.path.join(start, file), os.path.join(to, file))\n",
    "    copyfile(os.path.join(start, file).replace(\"images\", \"labels\"), os.path.join(to, file).replace(\"images\", \"labels\"))\n",
    "    os.remove(os.path.join(start, file))\n",
    "    os.remove(os.path.join(start, file).replace(\"images\", \"labels\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b88449",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Nuclei\\Nuclei\\train\\labels\"\n",
    "out = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Nuclei\\Nuclei\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(start):\n",
    "    preds = cv2.imread(os.path.join(start, file), 0)\n",
    "    preds[preds == 1] = 50\n",
    "    preds[preds == 2] = 100\n",
    "    preds[preds == 3] = 150\n",
    "    preds[preds == 4] = 200\n",
    "    preds[preds == 5] = 250\n",
    "    cv2.imwrite(os.path.join(out, file), preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3244b1a0",
   "metadata": {},
   "source": [
    "# 255 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b2d6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = r\"E:\\source\\repos\\Daten\\HER-N\\hubt\\dataset\\Cells\\valid\\labels\"\n",
    "import cv2\n",
    "import os\n",
    "for file in os.listdir(start):\n",
    "    img = cv2.imread(os.path.join(start, file), 0)\n",
    "    img[img > 0] = 1\n",
    "    cv2.imwrite(os.path.join(start, file), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0567e77b",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490756ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from uformer_pytorch import Uformer\n",
    "\n",
    "model = model = Uformer(\n",
    "                dim = 64,           # initial dimensions after input projection, which increases by 2x each stage\n",
    "                stages = 3,         # number of stages\n",
    "                num_blocks = 2,     # number of transformer blocks per stage\n",
    "                window_size = 16,   # set window size (along one side) for which to do the attention within\n",
    "                dim_head = 64,\n",
    "                heads = 4,\n",
    "                ff_mult = 2\n",
    "            ).cuda()\n",
    "\n",
    "x = torch.randn(2, 3, 128, 128).cuda()\n",
    "with torch.cuda.amp.autocast():\n",
    "    pred = model(x) # (1, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce132281",
   "metadata": {},
   "source": [
    "# Extract single instances for obj. detec + semant. seg -> bring into yolov5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080b1a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e39689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv.imread(r\"E:\\source\\repos\\Daten\\Converting\\Tumor Cells\\train\\labels\\PD-L1=2_0_37392-42804_85.png\", 0)\n",
    "\n",
    "# Creating kernel\n",
    "#kernel = np.ones((2, 2), np.uint8)\n",
    "  \n",
    "# Using cv2.erode() method \n",
    "#image = cv.erode(im, kernel) \n",
    "\n",
    "#image[image > 0] = 255\n",
    "#ret, thresh = cv.threshold(im, 127, 255, 0)\n",
    "#contours,hierachy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea0d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "im[im != 1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b1581",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(contours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e416ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.imwrite(\"xd.png\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba61ddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = {}\n",
    "start = r\"E:\\source\\repos\\Daten\\Converting\\Tumor Cells\\valid\\labels\"\n",
    "all_files = os.listdir(start)\n",
    "nc = 4\n",
    "for img in os.listdir(start):\n",
    "    for i in range(1, nc):\n",
    "        im = cv.imread(os.path.join(start, img), 0)\n",
    "        kernel = np.ones((2, 2), np.uint8)\n",
    "        im = cv.erode(im, kernel)\n",
    "        im = cv.dilate(im, kernel)\n",
    "        tmp = im.copy()\n",
    "        tmp[tmp != i] = 0\n",
    "        tmp[tmp == i] = 255\n",
    "        contours,hierachy = cv.findContours(tmp, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "        blank = np.zeros_like(tmp)\n",
    "        for cnt, cont in enumerate(contours):\n",
    "            xmin,ymin,width,height = cv.boundingRect(cont)\n",
    "            #cv.rectangle(im,(x,y),(x+width,y+height),(255),1)\n",
    "            #cv2.imwrite(\"xd.png\", im)\n",
    "            image_width = im.shape[0]\n",
    "            xcenter, ycenter = xmin + width/2, ymin + height/2\n",
    "            xcenter, ycenter, width, height = xcenter/image_width, ycenter/image_width, width/image_width, height/image_width\n",
    "            if not img in files:\n",
    "                files[img] = [(str(i - 1),str(xcenter), str(ycenter), str(width), str(height))]\n",
    "            else:\n",
    "\n",
    "                files[img] += [(str(i - 1),str(xcenter), str(ycenter), str(width), str(height))]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a3dac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a93a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "base = cv.imread(os.path.join(start.replace(\"labels\", \"images\"), 'PD-L1=2_0_37392-42804_85.png'))\n",
    "\n",
    "for rec in files['PD-L1=2_0_37392-42804_85.png']:\n",
    "    xcenter,ycenter,width,height = [int(float(xx) * base.shape[0]) for xx in rec[1:]]\n",
    "    x = int(xcenter - width/2)\n",
    "    y = int(ycenter - height/2)\n",
    "    va = int(rec[0] * 50)\n",
    "    print(rec)\n",
    "    print(\"\\nXD\")\n",
    "    cv.rectangle(base,(x,y),(x+width,y+height),(50,va * 50,va * 50),1)\n",
    "cv.imwrite(\"xd4.png\", base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c82d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv.imread(os.path.join(r\"E:\\source\\repos\\Daten\\Cells\\train\\images\", '05__1_3115_10030.png'), 0)\n",
    "kernel = np.ones((2, 2), np.uint8)\n",
    "image = cv.erode(im, kernel) \n",
    "contours,hierachy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "blank = np.zeros_like(im)\n",
    "for cnt, cont in enumerate(contours):\n",
    "    x,y,width,height = cv.boundingRect(cont)\n",
    "    cv.rectangle(im,(x,y),(x+width,y+height),(255),1)\n",
    "cv2.imwrite(\"xd.png\", im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51cb372",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_anno = [item for item in all_files if item not in list(files.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e9b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"labels\")\n",
    "for cnt, (key, val) in enumerate(files.items()):\n",
    "    with open(\"labels/\" + key.replace(\".png\", \".txt\"), \"w\") as handle:\n",
    "        for va in val:\n",
    "            handle.write(\" \".join(list(va))+ \"\\n\") \n",
    "            \n",
    "for name in no_anno:\n",
    "    with open(\"labels/\" + name.replace(\".png\", \".txt\"), \"w\") as handle:\n",
    "        handle.write(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891f647a",
   "metadata": {},
   "source": [
    "# Export for instance segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50876230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abacfd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"labels\")\n",
    "os.mkdir(\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86410f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_rec(img, rec):\n",
    "    x,y,w,h = rec\n",
    "    return img[y:y+h, x:x+w]\n",
    "def add_rec(orig, img, rec):\n",
    "    x,y,w,h = rec\n",
    "    #r,g,b = [random.randint(20, 255) for i in range(3)]\n",
    "    img[img > 0] = random.randint(20,255)\n",
    "    tmp = np.expand_dims(img.astype(np.uint8), axis=-1)\n",
    "    #tmp[np.all(tmp == (255, 255, 255), axis=-1)] = (b,g,r)\n",
    "    orig[y:y+h, x:x+w] += tmp\n",
    "    return orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e98f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pad = A.Compose([\n",
    "    A.PadIfNeeded(64,64, value = 0, border_mode = 0),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c44bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "labels = r\"E:\\source\\repos\\Daten\\Cells\\train\\labels\"\n",
    "images = r\"E:\\source\\repos\\Daten\\Cells\\train\\images\"\n",
    "to_labels = r\"E:\\source\\repos\\VisualDL\\custom_experiments\\labels\"\n",
    "to_images = r\"E:\\source\\repos\\VisualDL\\custom_experiments\\images\"\n",
    "all_files = os.listdir(labels)\n",
    "adding = 0\n",
    "for img in os.listdir(labels):\n",
    "    im = cv.imread(os.path.join(labels, img), 0)\n",
    "    original = cv.imread(os.path.join(images, img))\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    image = cv.erode(im, kernel) \n",
    "    contours,hierachy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    for cnt, cont in enumerate(contours):\n",
    "        blank = np.zeros_like(im)\n",
    "        xmin,ymin,width,height = cv.boundingRect(cont)\n",
    "        xmin -= 5\n",
    "        ymin -= 5\n",
    "        width +=5\n",
    "        height += 5\n",
    "        xmin = min(0, xmin)\n",
    "        ymin = min(0, ymin)\n",
    "        cv.drawContours(blank, [cont], -1, 1, -1)\n",
    "        final = cut_rec(blank, (xmin, ymin , width , height ))\n",
    "        orig_final = cut_rec(original, (xmin, ymin , width , height ))\n",
    "        trans = pad(image = orig_final, mask = final)\n",
    "        final = trans[\"mask\"]\n",
    "        orig_final = trans[\"image\"]\n",
    "        cv.imwrite(os.path.join(to_labels, img.replace(\".png\", f\"{cnt}.png\")), final)\n",
    "        cv.imwrite(os.path.join(to_images, img.replace(\".png\", f\"{cnt}.png\")), orig_final)\n",
    "        #cv.rectangle(im,(x,y),(x+width,y+height),(255),1)\n",
    "        \n",
    "        #cv2.imwrite(\"xd.png\", im)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b4cab0",
   "metadata": {},
   "source": [
    "# instance segmentation inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc80695",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.UnetPlusPlus(encoder_name = \"resnext50_32x4d\", classes = 2, in_channels = 3)\n",
    "model.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\resnext50_32x4d, UnetPlusPlus.pt\"))\n",
    "model.eval()\n",
    "model=model.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9078c0",
   "metadata": {},
   "source": [
    "## simple predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed2ae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    #A.PadIfNeeded(64,64, value = 0, border_mode = 0),\n",
    "    A.Resize(width=64, height=64),\n",
    "])\n",
    "test = r\"E:\\source\\repos\\VisualDL\\custom_experiments\\valid\\images\"\n",
    "for cnt, im in enumerate(os.listdir(test)):\n",
    "    image = io.imread(os.path.join(test, im))\n",
    "    image = transform(image = image)[\"image\"]\n",
    "    image = image/255.\n",
    "    s = torch.unsqueeze(torch.tensor(image, dtype = torch.float).permute(2, 0, 1), 0).cuda()\n",
    "    with torch.cuda.amp.autocast():\n",
    "        preds = model(s)\n",
    "    preds = torch.argmax(preds, 1)\n",
    "    preds *= 255\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    io.imsave(f\"{im}.png\", preds[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf9c6b5",
   "metadata": {},
   "source": [
    "# use rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8452363",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.PadIfNeeded(64,64, value = 0, border_mode = 0),\n",
    "    A.Resize(width=64, height=64),\n",
    "])\n",
    "\n",
    "files = {}\n",
    "labels = r\"E:\\source\\repos\\Daten\\Cells\\valid\\labels\"\n",
    "images = r\"E:\\source\\repos\\Daten\\Cells\\valid\\images\"\n",
    "to = r\"E:\\source\\repos\\VisualDL\\custom_experiments\\test\"\n",
    "all_files = os.listdir(labels)\n",
    "adding = 0\n",
    "for ii, img in enumerate(os.listdir(labels)):\n",
    "    im = cv.imread(os.path.join(labels, img), 0)\n",
    "    print(img)\n",
    "    original = cv.imread(os.path.join(images, img))\n",
    "    blank = np.zeros_like(original)\n",
    "    original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    image = cv.erode(im, kernel) \n",
    "    contours,hierachy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "    for cnt, cont in enumerate(contours):\n",
    "        \n",
    "        xmin,ymin,width,height = cv.boundingRect(cont)\n",
    "        orig_final = cut_rec(original, (xmin, ymin , width , height ))\n",
    "        tt = orig_final.copy()\n",
    "        orig_final = transform(image = orig_final)[\"image\"]\n",
    "        image = orig_final/255.\n",
    "        s = torch.unsqueeze(torch.tensor(image, dtype = torch.float).permute(2, 0, 1), 0).cuda()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            preds = model(s)\n",
    "        preds = torch.argmax(preds, 1)\n",
    "        preds *= 255\n",
    "        preds = preds.detach().cpu().numpy()[0]\n",
    "\n",
    "        preds = cut_rec(preds, (32 - int(width/2), 32 - int(height/2), width, height))\n",
    "        #contours,hierachy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "        blank = add_rec(blank, preds, (xmin,ymin,width,height))\n",
    "        cv2.imwrite(os.path.join(to, f\"{img}{cnt}.png\"), preds)\n",
    "        cv2.imwrite(os.path.join(to, f\"{img}{cnt}orig.png\"), tt)\n",
    "    cv2.imwrite(os.path.join(to, f\"{img}{cnt}xdddd.png\"), blank)\n",
    "    if ii == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658f0779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a3aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "skimage.skimage.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de6e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = U2NET(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aa5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_in = torch.randn(1, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0201e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, einsum\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from custom import ViT, AxialImageTransformer, AxialAttention\n",
    "# helpers\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def divisible_by(val, divisor):\n",
    "    return (val % divisor) == 0\n",
    "\n",
    "def unfold_output_size(image_size, kernel_size, stride, padding):\n",
    "    return int(((image_size - kernel_size + (2 * padding)) / stride) + 1)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = heads * dim_head\n",
    "        self.heads =  heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, d, h = *x.shape, self.heads\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
    "\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "        attn = sim.softmax(dim = -1)\n",
    "\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "# main class\n",
    "\n",
    "class TNT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        image_size,\n",
    "        patch_dim,\n",
    "        pixel_dim,\n",
    "        patch_size,\n",
    "        pixel_size,\n",
    "        depth,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        ff_dropout = 0.,\n",
    "        attn_dropout = 0.,\n",
    "        channels = 3,\n",
    "        unfold_args = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert divisible_by(image_size, patch_size), 'image size must be divisible by patch size'\n",
    "        assert divisible_by(patch_size, pixel_size), 'patch size must be divisible by pixel size for now'\n",
    "\n",
    "        num_patch_tokens = (image_size // patch_size) ** 2\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patch_tokens = nn.Parameter(torch.randn(num_patch_tokens + 1, patch_dim))\n",
    "\n",
    "        unfold_args = default(unfold_args, (pixel_size, pixel_size, 0))\n",
    "        unfold_args = (*unfold_args, 0) if len(unfold_args) == 2 else unfold_args\n",
    "        kernel_size, stride, padding = unfold_args\n",
    "\n",
    "        pixel_width = unfold_output_size(patch_size, kernel_size, stride, padding)\n",
    "        num_pixels = pixel_width ** 2\n",
    "\n",
    "        self.to_pixel_tokens = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> (b h w) c p1 p2', p1 = patch_size, p2 = patch_size),\n",
    "            nn.Unfold(kernel_size = kernel_size, stride = stride, padding = padding),\n",
    "            Rearrange('... c n -> ... n c'),\n",
    "            nn.Linear(channels * kernel_size ** 2, pixel_dim)\n",
    "        )\n",
    "\n",
    "        self.patch_pos_emb = nn.Parameter(torch.randn(num_patch_tokens + 1, patch_dim))\n",
    "        self.pixel_pos_emb = nn.Parameter(torch.randn(num_pixels, pixel_dim))\n",
    "\n",
    "        layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "\n",
    "            pixel_to_patch = nn.Sequential(\n",
    "                nn.LayerNorm(pixel_dim),\n",
    "                Rearrange('... n d -> ... (n d)'),\n",
    "                nn.Linear(pixel_dim * num_pixels, patch_dim),\n",
    "            )\n",
    "\n",
    "            layers.append(nn.ModuleList([\n",
    "                PreNorm(pixel_dim, Attention(dim = pixel_dim, heads = heads, dim_head = dim_head, dropout = attn_dropout)),\n",
    "                PreNorm(pixel_dim, FeedForward(dim = pixel_dim, dropout = ff_dropout)),\n",
    "                pixel_to_patch,\n",
    "                PreNorm(patch_dim, Attention(dim = patch_dim, heads = heads, dim_head = dim_head, dropout = attn_dropout)),\n",
    "                PreNorm(patch_dim, FeedForward(dim = patch_dim, dropout = ff_dropout)),\n",
    "            ]))\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, _, h, w, patch_size, image_size = *x.shape, self.patch_size, self.image_size\n",
    "        assert divisible_by(h, patch_size) and divisible_by(w, patch_size), f'height {h} and width {w} of input must be divisible by the patch size'\n",
    "\n",
    "        num_patches_h = h // patch_size\n",
    "        num_patches_w = w // patch_size\n",
    "        n = num_patches_w * num_patches_h\n",
    "\n",
    "        pixels = self.to_pixel_tokens(x)\n",
    "        patches = repeat(self.patch_tokens[:(n + 1)], 'n d -> b n d', b = b)\n",
    "\n",
    "        patches += rearrange(self.patch_pos_emb[:(n + 1)], 'n d -> () n d')\n",
    "        pixels += rearrange(self.pixel_pos_emb, 'n d -> () n d')\n",
    "\n",
    "        for pixel_attn, pixel_ff, pixel_to_patch_residual, patch_attn, patch_ff in self.layers:\n",
    "\n",
    "            pixels = pixel_attn(pixels) + pixels\n",
    "            pixels = pixel_ff(pixels) + pixels\n",
    "\n",
    "            patches_residual = pixel_to_patch_residual(pixels)\n",
    "\n",
    "            patches_residual = rearrange(patches_residual, '(b h w) d -> b (h w) d', h = num_patches_h, w = num_patches_w)\n",
    "            patches_residual = F.pad(patches_residual, (0, 0, 1, 0), value = 0) # cls token gets residual of 0\n",
    "            patches = patches + patches_residual\n",
    "\n",
    "            patches = patch_attn(patches) + patches\n",
    "            patches = patch_ff(patches) + patches\n",
    "        hidden_states = patches[:,1:,:]\n",
    "        B, n_patch, hidden = hidden_states.size()  # reshape from (B, n_patch, hidden) to (B, h, w, hidden)\n",
    "        h, w = int(np.sqrt(n_patch)), int(np.sqrt(n_patch))\n",
    "        x = hidden_states.permute(0, 2, 1)\n",
    "        x = x.contiguous().view(B, hidden, h, w)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Conv2dReLU(nn.Sequential):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            padding=0,\n",
    "            stride=1,\n",
    "            use_batchnorm=True,\n",
    "    ):\n",
    "\n",
    "        if use_batchnorm == \"inplace\" and InPlaceABN is None:\n",
    "            raise RuntimeError(\n",
    "                \"In order to use `use_batchnorm='inplace'` inplace_abn package must be installed. \"\n",
    "                + \"To install see: https://github.com/mapillary/inplace_abn\"\n",
    "            )\n",
    "\n",
    "        conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=not (use_batchnorm),\n",
    "        )\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        if use_batchnorm == \"inplace\":\n",
    "            bn = InPlaceABN(out_channels, activation=\"leaky_relu\", activation_param=0.0)\n",
    "            relu = nn.Identity()\n",
    "\n",
    "        elif use_batchnorm and use_batchnorm != \"inplace\":\n",
    "            bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        else:\n",
    "            bn = nn.Identity()\n",
    "\n",
    "        super(Conv2dReLU, self).__init__(conv, bn, relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1990a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "image_size = 128\n",
    "init_dim = 16\n",
    "conv = Conv2dReLU(3, init_dim, 1).cuda()\n",
    "inst = AxialImageTransformer(dim = init_dim,depth = 2,axial_pos_emb_shape = (image_size,image_size)).cuda()\n",
    "\n",
    "first = TNT(image_size = image_size, patch_size = 2, pixel_dim = 16, pixel_size = 1, patch_dim = init_dim * 2, depth = 2, heads = 4, channels = init_dim).cuda()\n",
    "second = TNT(image_size = image_size//2, patch_size = 2, pixel_dim = 16, pixel_size = 1, patch_dim = init_dim * 4, depth = 4, heads = 6, channels = init_dim * 2).cuda()\n",
    "third = TNT(image_size = image_size//4, patch_size = 2, pixel_dim = 16, pixel_size = 1, patch_dim = init_dim * 8, depth = 6, heads = 8, channels = init_dim * 4).cuda()\n",
    "fourth = TNT(image_size = image_size//8, patch_size = 2, pixel_dim = 16, pixel_size = 1, patch_dim = init_dim * 16, depth = 8, heads = 12, channels = init_dim * 8).cuda()\n",
    "cup = ViT(\n",
    "            image_size = 8,\n",
    "            patch_size = 1,\n",
    "            dim = init_dim * 32,\n",
    "            depth = 6,\n",
    "            heads = 12,\n",
    "            mlp_dim = 2048,\n",
    "            dropout = 0.1,\n",
    "            emb_dropout = 0.1,\n",
    "            channels = init_dim * 16\n",
    "        ).cuda()\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                in_channels,\n",
    "                skip_channels,\n",
    "                out_channels,\n",
    "                depth,\n",
    "                size,\n",
    "                heads= 2):\n",
    "        super().__init__()   \n",
    "        self.ax = AxialImageTransformer(dim = in_channels + skip_channels,heads= heads,depth = depth,axial_pos_emb_shape = (size,size)).cuda()\n",
    "        self.out = Conv2dReLU(in_channels + skip_channels, out_channels, 1)\n",
    "        \n",
    "    def forward(self, x, skip = None):\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        return self.out(self.ax(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2090bb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = 2\n",
    "up1 = DecoderBlock(init_dim*32, init_dim*8, init_dim*8, depth = 2, heads = 2,size = image_size//8).cuda()\n",
    "up2 = DecoderBlock(init_dim*8, init_dim*4, init_dim*4, depth = 2, heads = 2,size = image_size//4).cuda()\n",
    "up3 = DecoderBlock(init_dim*4, init_dim*2, init_dim*2, depth = 2, heads = 2,size = image_size//2).cuda()\n",
    "up4 = DecoderBlock(init_dim*2, init_dim, 2, depth = 2, heads = 2,size = image_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abe12ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=AxialAttention(32,dim_index =1).cuda()\n",
    "a= AxialImageTransformer(dim = 3072, depth = 1,axial_pos_emb_shape = (128,128), heads=1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ce9eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_in = torch.randn(2, 3072, 128, 128).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ddc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "a(dummy_in).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c640cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inst(conv(dummy_in)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b26bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(hidden_states):\n",
    "    B, n_patch, hidden = hidden_states.size()  # reshape from (B, n_patch, hidden) to (B, h, w, hidden)\n",
    "    h, w = int(np.sqrt(n_patch)), int(np.sqrt(n_patch))\n",
    "    x = hidden_states.permute(0, 2, 1)\n",
    "    x = x.contiguous().view(B, hidden, h, w)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a3629",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.cuda.amp.autocast():\n",
    "    out1 = inst(conv(dummy_in)) #128\n",
    "with torch.cuda.amp.autocast():\n",
    "    out2 = first(out1) #64\n",
    "with torch.cuda.amp.autocast():\n",
    "    out3 = second(out2) #32\n",
    "with torch.cuda.amp.autocast():\n",
    "    out4 = third(out3) #16\n",
    "with torch.cuda.amp.autocast():\n",
    "    out5 = fourth(out4) #8\n",
    "with torch.cuda.amp.autocast():\n",
    "     out6 = cup(out5) #8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca05747f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "up4(up3(up2(up1(out6, out4), out3), out2), out1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2127a1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hidden_states = out\n",
    "B, n_patch, hidden = hidden_states.size()  # reshape from (B, n_patch, hidden) to (B, h, w, hidden)\n",
    "h, w = int(np.sqrt(n_patch)), int(np.sqrt(n_patch))\n",
    "x = hidden_states.permute(0, 2, 1)\n",
    "x = x.contiguous().view(B, hidden, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ea3048",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e79fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "2**5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e0f671",
   "metadata": {},
   "source": [
    "# HRNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d30d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "from torch.nn.parallel._functions import ReduceAddCoalesced, Broadcast\n",
    "\n",
    "class SyncMaster(object):\n",
    "    \"\"\"An abstract `SyncMaster` object.\n",
    "    - During the replication, as the data parallel will trigger an callback of each module, all slave devices should\n",
    "    call `register(id)` and obtain an `SlavePipe` to communicate with the master.\n",
    "    - During the forward pass, master device invokes `run_master`, all messages from slave devices will be collected,\n",
    "    and passed to a registered callback.\n",
    "    - After receiving the messages, the master device should gather the information and determine to message passed\n",
    "    back to each slave devices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, master_callback):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            master_callback: a callback to be invoked after having collected messages from slave devices.\n",
    "        \"\"\"\n",
    "        self._master_callback = master_callback\n",
    "        self._queue = queue.Queue()\n",
    "        self._registry = collections.OrderedDict()\n",
    "        self._activated = False\n",
    "\n",
    "    def register_slave(self, identifier):\n",
    "        \"\"\"\n",
    "        Register an slave device.\n",
    "        Args:\n",
    "            identifier: an identifier, usually is the device id.\n",
    "        Returns: a `SlavePipe` object which can be used to communicate with the master device.\n",
    "        \"\"\"\n",
    "        if self._activated:\n",
    "            assert self._queue.empty(), 'Queue is not clean before next initialization.'\n",
    "            self._activated = False\n",
    "            self._registry.clear()\n",
    "        future = FutureResult()\n",
    "        self._registry[identifier] = _MasterRegistry(future)\n",
    "        return SlavePipe(identifier, self._queue, future)\n",
    "\n",
    "class _SynchronizedBatchNorm(_BatchNorm):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.001, affine=True):\n",
    "        super(_SynchronizedBatchNorm, self).__init__(num_features, eps=eps, momentum=momentum, affine=affine)\n",
    "\n",
    "        self._sync_master = SyncMaster(self._data_parallel_master)\n",
    "\n",
    "        self._is_parallel = False\n",
    "        self._parallel_id = None\n",
    "        self._slave_pipe = None\n",
    "\n",
    "        # customed batch norm statistics\n",
    "        self._moving_average_fraction = 1. - momentum\n",
    "        self.register_buffer('_tmp_running_mean', torch.zeros(self.num_features))\n",
    "        self.register_buffer('_tmp_running_var', torch.ones(self.num_features))\n",
    "        self.register_buffer('_running_iter', torch.ones(1))\n",
    "        self._tmp_running_mean = self.running_mean.clone() * self._running_iter\n",
    "        self._tmp_running_var = self.running_var.clone() * self._running_iter\n",
    "\n",
    "    def forward(self, input):\n",
    "        # If it is not parallel computation or is in evaluation mode, use PyTorch's implementation.\n",
    "        if not (self._is_parallel and self.training):\n",
    "            return F.batch_norm(\n",
    "                input, self.running_mean, self.running_var, self.weight, self.bias,\n",
    "                self.training, self.momentum, self.eps)\n",
    "\n",
    "        # Resize the input to (B, C, -1).\n",
    "        input_shape = input.size()\n",
    "        input = input.view(input.size(0), self.num_features, -1)\n",
    "\n",
    "        # Compute the sum and square-sum.\n",
    "        sum_size = input.size(0) * input.size(2)\n",
    "        input_sum = _sum_ft(input)\n",
    "        input_ssum = _sum_ft(input ** 2)\n",
    "\n",
    "        # Reduce-and-broadcast the statistics.\n",
    "        if self._parallel_id == 0:\n",
    "            mean, inv_std = self._sync_master.run_master(_ChildMessage(input_sum, input_ssum, sum_size))\n",
    "        else:\n",
    "            mean, inv_std = self._slave_pipe.run_slave(_ChildMessage(input_sum, input_ssum, sum_size))\n",
    "\n",
    "        # Compute the output.\n",
    "        if self.affine:\n",
    "            # MJY:: Fuse the multiplication for speed.\n",
    "            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std * self.weight) + _unsqueeze_ft(self.bias)\n",
    "        else:\n",
    "            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std)\n",
    "\n",
    "        # Reshape it.\n",
    "        return output.view(input_shape)\n",
    "\n",
    "    def __data_parallel_replicate__(self, ctx, copy_id):\n",
    "        self._is_parallel = True\n",
    "        self._parallel_id = copy_id\n",
    "\n",
    "        # parallel_id == 0 means master device.\n",
    "        if self._parallel_id == 0:\n",
    "            ctx.sync_master = self._sync_master\n",
    "        else:\n",
    "            self._slave_pipe = ctx.sync_master.register_slave(copy_id)\n",
    "\n",
    "    def _data_parallel_master(self, intermediates):\n",
    "        \"\"\"Reduce the sum and square-sum, compute the statistics, and broadcast it.\"\"\"\n",
    "        intermediates = sorted(intermediates, key=lambda i: i[1].sum.get_device())\n",
    "\n",
    "        to_reduce = [i[1][:2] for i in intermediates]\n",
    "        to_reduce = [j for i in to_reduce for j in i]  # flatten\n",
    "        target_gpus = [i[1].sum.get_device() for i in intermediates]\n",
    "\n",
    "        sum_size = sum([i[1].sum_size for i in intermediates])\n",
    "        sum_, ssum = ReduceAddCoalesced.apply(target_gpus[0], 2, *to_reduce)\n",
    "\n",
    "        mean, inv_std = self._compute_mean_std(sum_, ssum, sum_size)\n",
    "\n",
    "        broadcasted = Broadcast.apply(target_gpus, mean, inv_std)\n",
    "\n",
    "        outputs = []\n",
    "        for i, rec in enumerate(intermediates):\n",
    "            outputs.append((rec[0], _MasterMessage(*broadcasted[i*2:i*2+2])))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _add_weighted(self, dest, delta, alpha=1, beta=1, bias=0):\n",
    "        \"\"\"return *dest* by `dest := dest*alpha + delta*beta + bias`\"\"\"\n",
    "        return dest * alpha + delta * beta + bias\n",
    "\n",
    "    def _compute_mean_std(self, sum_, ssum, size):\n",
    "        \"\"\"Compute the mean and standard-deviation with sum and square-sum. This method\n",
    "        also maintains the moving average on the master device.\"\"\"\n",
    "        assert size > 1, 'BatchNorm computes unbiased standard-deviation, which requires size > 1.'\n",
    "        mean = sum_ / size\n",
    "        sumvar = ssum - sum_ * mean\n",
    "        unbias_var = sumvar / (size - 1)\n",
    "        bias_var = sumvar / size\n",
    "\n",
    "        self._tmp_running_mean = self._add_weighted(self._tmp_running_mean, mean.data, alpha=self._moving_average_fraction)\n",
    "        self._tmp_running_var = self._add_weighted(self._tmp_running_var, unbias_var.data, alpha=self._moving_average_fraction)\n",
    "        self._running_iter = self._add_weighted(self._running_iter, 1, alpha=self._moving_average_fraction)\n",
    "\n",
    "        self.running_mean = self._tmp_running_mean / self._running_iter\n",
    "        self.running_var = self._tmp_running_var / self._running_iter\n",
    "\n",
    "        return mean, bias_var.clamp(self.eps) ** -0.5\n",
    "    \n",
    "class SynchronizedBatchNorm2d(_SynchronizedBatchNorm):\n",
    "    r\"\"\"Applies Batch Normalization over a 4d input that is seen as a mini-batch\n",
    "    of 3d inputs\n",
    "    .. math::\n",
    "        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
    "    This module differs from the built-in PyTorch BatchNorm2d as the mean and\n",
    "    standard-deviation are reduced across all devices during training.\n",
    "    For example, when one uses `nn.DataParallel` to wrap the network during\n",
    "    training, PyTorch's implementation normalize the tensor on each device using\n",
    "    the statistics only on that device, which accelerated the computation and\n",
    "    is also easy to implement, but the statistics might be inaccurate.\n",
    "    Instead, in this synchronized version, the statistics will be computed\n",
    "    over all training samples distributed on multiple devices.\n",
    "    \n",
    "    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n",
    "    as the built-in PyTorch implementation.\n",
    "    The mean and standard-deviation are calculated per-dimension over\n",
    "    the mini-batches and gamma and beta are learnable parameter vectors\n",
    "    of size C (where C is the input size).\n",
    "    During training, this layer keeps a running estimate of its computed mean\n",
    "    and variance. The running sum is kept with a default momentum of 0.1.\n",
    "    During evaluation, this running mean/variance is used for normalization.\n",
    "    Because the BatchNorm is done over the `C` dimension, computing statistics\n",
    "    on `(N, H, W)` slices, it's common terminology to call this Spatial BatchNorm\n",
    "    Args:\n",
    "        num_features: num_features from an expected input of\n",
    "            size batch_size x num_features x height x width\n",
    "        eps: a value added to the denominator for numerical stability.\n",
    "            Default: 1e-5\n",
    "        momentum: the value used for the running_mean and running_var\n",
    "            computation. Default: 0.1\n",
    "        affine: a boolean value that when set to ``True``, gives the layer learnable\n",
    "            affine parameters. Default: ``True``\n",
    "    Shape:\n",
    "        - Input: :math:`(N, C, H, W)`\n",
    "        - Output: :math:`(N, C, H, W)` (same shape as input)\n",
    "    Examples:\n",
    "        >>> # With Learnable Parameters\n",
    "        >>> m = SynchronizedBatchNorm2d(100)\n",
    "        >>> # Without Learnable Parameters\n",
    "        >>> m = SynchronizedBatchNorm2d(100, affine=False)\n",
    "        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45))\n",
    "        >>> output = m(input)\n",
    "    \"\"\"\n",
    "\n",
    "    def _check_input_dim(self, input):\n",
    "        if input.dim() != 4:\n",
    "            raise ValueError('expected 4D input (got {}D input)'\n",
    "                             .format(input.dim()))\n",
    "        super(SynchronizedBatchNorm2d, self)._check_input_dim(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5289bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This HRNet implementation is modified from the following repository:\n",
    "https://github.com/HRNet/HRNet-Semantic-Segmentation\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "BatchNorm2d = SynchronizedBatchNorm2d\n",
    "BN_MOMENTUM = 0.1\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "__all__ = ['hrnetv2']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'hrnetv2': 'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/hrnetv2_w48-imagenet.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.bn3 = BatchNorm2d(planes * self.expansion,\n",
    "                               momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class HighResolutionModule(nn.Module):\n",
    "    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\n",
    "                 num_channels, fuse_method, multi_scale_output=True):\n",
    "        super(HighResolutionModule, self).__init__()\n",
    "        self._check_branches(\n",
    "            num_branches, blocks, num_blocks, num_inchannels, num_channels)\n",
    "\n",
    "        self.num_inchannels = num_inchannels\n",
    "        self.fuse_method = fuse_method\n",
    "        self.num_branches = num_branches\n",
    "\n",
    "        self.multi_scale_output = multi_scale_output\n",
    "\n",
    "        self.branches = self._make_branches(\n",
    "            num_branches, blocks, num_blocks, num_channels)\n",
    "        self.fuse_layers = self._make_fuse_layers()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def _check_branches(self, num_branches, blocks, num_blocks,\n",
    "                        num_inchannels, num_channels):\n",
    "        if num_branches != len(num_blocks):\n",
    "            error_msg = 'NUM_BRANCHES({}) <> NUM_BLOCKS({})'.format(\n",
    "                num_branches, len(num_blocks))\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "        if num_branches != len(num_channels):\n",
    "            error_msg = 'NUM_BRANCHES({}) <> NUM_CHANNELS({})'.format(\n",
    "                num_branches, len(num_channels))\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "        if num_branches != len(num_inchannels):\n",
    "            error_msg = 'NUM_BRANCHES({}) <> NUM_INCHANNELS({})'.format(\n",
    "                num_branches, len(num_inchannels))\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\n",
    "                         stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or \\\n",
    "           self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.num_inchannels[branch_index],\n",
    "                          num_channels[branch_index] * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                BatchNorm2d(num_channels[branch_index] * block.expansion,\n",
    "                            momentum=BN_MOMENTUM),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.num_inchannels[branch_index],\n",
    "                            num_channels[branch_index], stride, downsample))\n",
    "        self.num_inchannels[branch_index] = \\\n",
    "            num_channels[branch_index] * block.expansion\n",
    "        for i in range(1, num_blocks[branch_index]):\n",
    "            layers.append(block(self.num_inchannels[branch_index],\n",
    "                                num_channels[branch_index]))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n",
    "        branches = []\n",
    "\n",
    "        for i in range(num_branches):\n",
    "            branches.append(\n",
    "                self._make_one_branch(i, block, num_blocks, num_channels))\n",
    "\n",
    "        return nn.ModuleList(branches)\n",
    "\n",
    "    def _make_fuse_layers(self):\n",
    "        if self.num_branches == 1:\n",
    "            return None\n",
    "\n",
    "        num_branches = self.num_branches\n",
    "        num_inchannels = self.num_inchannels\n",
    "        fuse_layers = []\n",
    "        for i in range(num_branches if self.multi_scale_output else 1):\n",
    "            fuse_layer = []\n",
    "            for j in range(num_branches):\n",
    "                if j > i:\n",
    "                    fuse_layer.append(nn.Sequential(\n",
    "                        nn.Conv2d(num_inchannels[j],\n",
    "                                  num_inchannels[i],\n",
    "                                  1,\n",
    "                                  1,\n",
    "                                  0,\n",
    "                                  bias=False),\n",
    "                        BatchNorm2d(num_inchannels[i], momentum=BN_MOMENTUM)))\n",
    "                elif j == i:\n",
    "                    fuse_layer.append(None)\n",
    "                else:\n",
    "                    conv3x3s = []\n",
    "                    for k in range(i-j):\n",
    "                        if k == i - j - 1:\n",
    "                            num_outchannels_conv3x3 = num_inchannels[i]\n",
    "                            conv3x3s.append(nn.Sequential(\n",
    "                                nn.Conv2d(num_inchannels[j],\n",
    "                                          num_outchannels_conv3x3,\n",
    "                                          3, 2, 1, bias=False),\n",
    "                                BatchNorm2d(num_outchannels_conv3x3,\n",
    "                                            momentum=BN_MOMENTUM)))\n",
    "                        else:\n",
    "                            num_outchannels_conv3x3 = num_inchannels[j]\n",
    "                            conv3x3s.append(nn.Sequential(\n",
    "                                nn.Conv2d(num_inchannels[j],\n",
    "                                          num_outchannels_conv3x3,\n",
    "                                          3, 2, 1, bias=False),\n",
    "                                BatchNorm2d(num_outchannels_conv3x3,\n",
    "                                            momentum=BN_MOMENTUM),\n",
    "                                nn.ReLU(inplace=True)))\n",
    "                    fuse_layer.append(nn.Sequential(*conv3x3s))\n",
    "            fuse_layers.append(nn.ModuleList(fuse_layer))\n",
    "\n",
    "        return nn.ModuleList(fuse_layers)\n",
    "\n",
    "    def get_num_inchannels(self):\n",
    "        return self.num_inchannels\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.num_branches == 1:\n",
    "            return [self.branches[0](x[0])]\n",
    "\n",
    "        for i in range(self.num_branches):\n",
    "            x[i] = self.branches[i](x[i])\n",
    "\n",
    "        x_fuse = []\n",
    "        for i in range(len(self.fuse_layers)):\n",
    "            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n",
    "            for j in range(1, self.num_branches):\n",
    "                if i == j:\n",
    "                    y = y + x[j]\n",
    "                elif j > i:\n",
    "                    width_output = x[i].shape[-1]\n",
    "                    height_output = x[i].shape[-2]\n",
    "                    y = y + F.interpolate(\n",
    "                        self.fuse_layers[i][j](x[j]),\n",
    "                        size=(height_output, width_output),\n",
    "                        mode='bilinear',\n",
    "                        align_corners=False)\n",
    "                else:\n",
    "                    y = y + self.fuse_layers[i][j](x[j])\n",
    "            x_fuse.append(self.relu(y))\n",
    "\n",
    "        return x_fuse\n",
    "\n",
    "\n",
    "blocks_dict = {\n",
    "    'BASIC': BasicBlock,\n",
    "    'BOTTLENECK': Bottleneck\n",
    "}\n",
    "class Conv2dReLU(nn.Sequential):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            padding=0,\n",
    "            stride=1,\n",
    "            use_batchnorm=True,\n",
    "    ):\n",
    "\n",
    "        if use_batchnorm == \"inplace\" and InPlaceABN is None:\n",
    "            raise RuntimeError(\n",
    "                \"In order to use `use_batchnorm='inplace'` inplace_abn package must be installed. \"\n",
    "                + \"To install see: https://github.com/mapillary/inplace_abn\"\n",
    "            )\n",
    "\n",
    "        conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=not (use_batchnorm),\n",
    "        )\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        if use_batchnorm == \"inplace\":\n",
    "            bn = InPlaceABN(out_channels, activation=\"leaky_relu\", activation_param=0.0)\n",
    "            relu = nn.Identity()\n",
    "\n",
    "        elif use_batchnorm and use_batchnorm != \"inplace\":\n",
    "            bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        else:\n",
    "            bn = nn.Identity()\n",
    "\n",
    "        super(Conv2dReLU, self).__init__(conv, bn, relu)\n",
    "        \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            use_batchnorm=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2dReLU(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        self.conv2 = Conv2dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x        \n",
    "relu_inplace = True\n",
    "class HRNetV2(nn.Module):\n",
    "    def __init__(self, n_class, **kwargs):\n",
    "        super(HRNetV2, self).__init__()\n",
    "        extra = {\n",
    "            'STAGE1': {'NUM_MODULES': 1, 'NUM_BRANCHES': 1, 'BLOCK': 'BOTTLENECK', 'NUM_BLOCKS': (4), 'NUM_CHANNELS': (64), 'FUSE_METHOD': 'SUM'},\n",
    "            'STAGE2': {'NUM_MODULES': 1, 'NUM_BRANCHES': 2, 'BLOCK': 'BASIC', 'NUM_BLOCKS': (4, 4), 'NUM_CHANNELS': (48, 96), 'FUSE_METHOD': 'SUM'},\n",
    "            'STAGE3': {'NUM_MODULES': 4, 'NUM_BRANCHES': 3, 'BLOCK': 'BASIC', 'NUM_BLOCKS': (4, 4, 4), 'NUM_CHANNELS': (48, 96, 192), 'FUSE_METHOD': 'SUM'},\n",
    "            'STAGE4': {'NUM_MODULES': 3, 'NUM_BRANCHES': 4, 'BLOCK': 'BASIC', 'NUM_BLOCKS': (4, 4, 4, 4), 'NUM_CHANNELS': (48, 96, 192, 384), 'FUSE_METHOD': 'SUM'},\n",
    "            'FINAL_CONV_KERNEL': 1\n",
    "            }\n",
    "        ALIGN_CORNERS = False\n",
    "        relu_inplace = True\n",
    "        # stem net\n",
    "        # stem net\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn2 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        \n",
    "        self.stage1_cfg = extra['STAGE1']\n",
    "        num_channels = self.stage1_cfg['NUM_CHANNELS']\n",
    "        block = blocks_dict[self.stage1_cfg['BLOCK']]\n",
    "        num_blocks = self.stage1_cfg['NUM_BLOCKS']\n",
    "        self.layer1 = self._make_layer(block, 64, num_channels, num_blocks)\n",
    "        stage1_out_channel = block.expansion*num_channels\n",
    "        \n",
    "        \n",
    "        self.stage2_cfg = extra['STAGE2']\n",
    "        num_channels = self.stage2_cfg['NUM_CHANNELS']\n",
    "        block = blocks_dict[self.stage2_cfg['BLOCK']]\n",
    "        num_channels = [\n",
    "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
    "        self.transition1 = self._make_transition_layer(\n",
    "            [stage1_out_channel], num_channels)\n",
    "        self.stage2, pre_stage_channels = self._make_stage(\n",
    "            self.stage2_cfg, num_channels)\n",
    "\n",
    "        self.stage3_cfg = extra['STAGE3']\n",
    "        num_channels = self.stage3_cfg['NUM_CHANNELS']\n",
    "        block = blocks_dict[self.stage3_cfg['BLOCK']]\n",
    "        num_channels = [\n",
    "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
    "        self.transition2 = self._make_transition_layer(\n",
    "            pre_stage_channels, num_channels)\n",
    "        self.stage3, pre_stage_channels = self._make_stage(\n",
    "            self.stage3_cfg, num_channels)\n",
    "\n",
    "        self.stage4_cfg = extra['STAGE4']\n",
    "        num_channels = self.stage4_cfg['NUM_CHANNELS']\n",
    "        block = blocks_dict[self.stage4_cfg['BLOCK']]\n",
    "        num_channels = [\n",
    "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
    "        self.transition3 = self._make_transition_layer(\n",
    "            pre_stage_channels, num_channels)\n",
    "        self.stage4, pre_stage_channels = self._make_stage(\n",
    "            self.stage4_cfg, num_channels, multi_scale_output=True)\n",
    "        \n",
    "        last_inp_channels = np.int(np.sum(pre_stage_channels))\n",
    "        self.up1 = DecoderBlock(720, 512)\n",
    "        self.up2 = DecoderBlock(512, n_class)\n",
    "        \n",
    "        self.last_layer = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=last_inp_channels,\n",
    "                out_channels=last_inp_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0),\n",
    "            BatchNorm2d(last_inp_channels, momentum=BN_MOMENTUM),\n",
    "            nn.ReLU(inplace=relu_inplace),\n",
    "            nn.Conv2d(\n",
    "                in_channels=last_inp_channels,\n",
    "                out_channels=n_class,\n",
    "                kernel_size=extra['FINAL_CONV_KERNEL'],\n",
    "                stride=1,\n",
    "                padding=1 if extra['FINAL_CONV_KERNEL'] == 3 else 0)\n",
    "        )\n",
    "        \n",
    "    def _make_transition_layer(\n",
    "            self, num_channels_pre_layer, num_channels_cur_layer):\n",
    "        num_branches_cur = len(num_channels_cur_layer)\n",
    "        num_branches_pre = len(num_channels_pre_layer)\n",
    "\n",
    "        transition_layers = []\n",
    "        for i in range(num_branches_cur):\n",
    "            if i < num_branches_pre:\n",
    "                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n",
    "                    transition_layers.append(nn.Sequential(\n",
    "                        nn.Conv2d(num_channels_pre_layer[i],\n",
    "                                  num_channels_cur_layer[i],\n",
    "                                  3,\n",
    "                                  1,\n",
    "                                  1,\n",
    "                                  bias=False),\n",
    "                        BatchNorm2d(\n",
    "                            num_channels_cur_layer[i], momentum=BN_MOMENTUM),\n",
    "                        nn.ReLU(inplace=relu_inplace)))\n",
    "                else:\n",
    "                    transition_layers.append(None)\n",
    "            else:\n",
    "                conv3x3s = []\n",
    "                for j in range(i+1-num_branches_pre):\n",
    "                    inchannels = num_channels_pre_layer[-1]\n",
    "                    outchannels = num_channels_cur_layer[i] \\\n",
    "                        if j == i-num_branches_pre else inchannels\n",
    "                    conv3x3s.append(nn.Sequential(\n",
    "                        nn.Conv2d(\n",
    "                            inchannels, outchannels, 3, 2, 1, bias=False),\n",
    "                        BatchNorm2d(outchannels, momentum=BN_MOMENTUM),\n",
    "                        nn.ReLU(inplace=relu_inplace)))\n",
    "                transition_layers.append(nn.Sequential(*conv3x3s))\n",
    "\n",
    "        return nn.ModuleList(transition_layers)\n",
    "\n",
    "    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(inplanes, planes, stride, downsample))\n",
    "        inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_stage(self, layer_config, num_inchannels,\n",
    "                    multi_scale_output=True):\n",
    "        num_modules = layer_config['NUM_MODULES']\n",
    "        num_branches = layer_config['NUM_BRANCHES']\n",
    "        num_blocks = layer_config['NUM_BLOCKS']\n",
    "        num_channels = layer_config['NUM_CHANNELS']\n",
    "        block = blocks_dict[layer_config['BLOCK']]\n",
    "        fuse_method = layer_config['FUSE_METHOD']\n",
    "\n",
    "        modules = []\n",
    "        for i in range(num_modules):\n",
    "            # multi_scale_output is only used last module\n",
    "            if not multi_scale_output and i == num_modules - 1:\n",
    "                reset_multi_scale_output = False\n",
    "            else:\n",
    "                reset_multi_scale_output = True\n",
    "            modules.append(\n",
    "                HighResolutionModule(num_branches,\n",
    "                                      block,\n",
    "                                      num_blocks,\n",
    "                                      num_inchannels,\n",
    "                                      num_channels,\n",
    "                                      fuse_method,\n",
    "                                      reset_multi_scale_output)\n",
    "            )\n",
    "            num_inchannels = modules[-1].get_num_inchannels()\n",
    "\n",
    "        return nn.Sequential(*modules), num_inchannels\n",
    "\n",
    "    def forward(self, x, return_feature_maps=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "\n",
    "        x_list = []\n",
    "        for i in range(self.stage2_cfg['NUM_BRANCHES']):\n",
    "            if self.transition1[i] is not None:\n",
    "                x_list.append(self.transition1[i](x))\n",
    "            else:\n",
    "                x_list.append(x)\n",
    "        y_list = self.stage2(x_list)\n",
    "\n",
    "        x_list = []\n",
    "        for i in range(self.stage3_cfg['NUM_BRANCHES']):\n",
    "            if self.transition2[i] is not None:\n",
    "                if i < self.stage2_cfg['NUM_BRANCHES']:\n",
    "                    x_list.append(self.transition2[i](y_list[i]))\n",
    "                else:\n",
    "                    x_list.append(self.transition2[i](y_list[-1]))\n",
    "            else:\n",
    "                x_list.append(y_list[i])\n",
    "        y_list = self.stage3(x_list)\n",
    "\n",
    "        x_list = []\n",
    "        for i in range(self.stage4_cfg['NUM_BRANCHES']):\n",
    "            if self.transition3[i] is not None:\n",
    "                if i < self.stage3_cfg['NUM_BRANCHES']:\n",
    "                    x_list.append(self.transition3[i](y_list[i]))\n",
    "                else:\n",
    "                    x_list.append(self.transition3[i](y_list[-1]))\n",
    "            else:\n",
    "                x_list.append(y_list[i])\n",
    "        x = self.stage4(x_list)\n",
    "\n",
    "        # Upsampling\n",
    "        x0_h, x0_w = x[0].size(2), x[0].size(3)\n",
    "        x1 = F.interpolate(x[1], size=(x0_h, x0_w), mode='bilinear', align_corners=False)\n",
    "        x2 = F.interpolate(x[2], size=(x0_h, x0_w), mode='bilinear', align_corners=False)\n",
    "        x3 = F.interpolate(x[3], size=(x0_h, x0_w), mode='bilinear', align_corners=False)\n",
    "\n",
    "        x = torch.cat([x[0], x1, x2, x3], 1)\n",
    "\n",
    "        #x = self.up2(self.up1(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def hrnetv2(pretrained=False, **kwargs):\n",
    "    model = HRNetV2(n_class=2, **kwargs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e93e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "import collections\n",
    "import threading\n",
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "#model = hrnetv2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13df8596",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = eval(f'smp.create_model(arch=\"{\"FullAxialUnet\"}\", encoder_name=\"{\"resnet18\"}\", encoder_weights=\"imagenet\", in_channels={3}, classes = {2}, image_size = {256})').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48af99b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_in = torch.randn(2, 3, 256, 256).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8703cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(dummy_in).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1539c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77bcacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "li = [{'Id': 1, 'ParentId': 0, 'ClassificationSubtype': False, 'Label': 'Base ROI', 'Color': 'rgba(0, 0, 0, 0.0)', 'Tools': [{'Name': 'iam_base_roi', 'Parameters': {'invert_result': False, 'min_intensity': 28.2, 'smoothing': 23.55, 'minsize': 5, 'paramLabels': {'invert_result': 'Invert Base ROI', 'min_intensity': 'Minimum Intensity (%)', 'smoothing': 'Smoothing', 'minsize': 'Minimum Size (%)'}}}], 'SelectedToolName': None, 'Dynamic': False, 'HasChild': False, 'IsSubtype': False, 'SubtypeLevel': 0, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': [], 'DefaultSelected': False}, {'Id': 2, 'ParentId': 0, 'ClassificationSubtype': False, 'Label': 'Tumor Cells', 'Color': '#e6194b', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': True, 'IsSubtype': False, 'SubtypeLevel': 0, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 4, 'ParentId': 2, 'ClassificationSubtype': True, 'Label': 'Positive', 'Color': '#ffe119', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 1, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 3, 'ParentId': 2, 'ClassificationSubtype': True, 'Label': 'Negative', 'Color': '#3cb44b', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 1, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 5, 'ParentId': 0, 'ClassificationSubtype': False, 'Label': 'Immun Cells', 'Color': '#4363d8', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': True, 'IsSubtype': False, 'SubtypeLevel': 0, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 7, 'ParentId': 5, 'ClassificationSubtype': True, 'Label': 'Lymphozyten', 'Color': '#911eb4', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': True, 'IsSubtype': True, 'SubtypeLevel': 1, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 11, 'ParentId': 7, 'ClassificationSubtype': True, 'Label': 'Lymphozyten +', 'Color': '#fabebe', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 2, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 12, 'ParentId': 7, 'ClassificationSubtype': True, 'Label': 'Lymphozyten -', 'Color': '#008080', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 2, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 6, 'ParentId': 5, 'ClassificationSubtype': True, 'Label': 'Granulozyten', 'Color': '#f58231', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': True, 'IsSubtype': True, 'SubtypeLevel': 1,'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 14, 'ParentId': 6, 'ClassificationSubtype': True, 'Label': 'Granulozyten +', 'Color': '#9a6324', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 2, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 13, 'ParentId': 6, 'ClassificationSubtype': True, 'Label': 'Granulozyten -', 'Color': '#e6beff', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 2, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 10, 'ParentId': 5, 'ClassificationSubtype': True, 'Label': 'Dendritische Zellen', 'Color': '#bcf60c', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 1, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 9, 'ParentId': 5, 'ClassificationSubtype': True, 'Label': 'Makrophagen', 'Color': '#f032e6', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 1, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 8, 'ParentId': 5, 'ClassificationSubtype': True, 'Label': 'Plasmazellen', 'Color': '#46f0f0', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 1, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 15, 'ParentId': 0, 'ClassificationSubtype': False, 'Label': 'Grid', 'Color': '#00000000', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': False, 'SubtypeLevel': 0, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373d0a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in list(filter(lambda x: x['HasChild'], li)):\n",
    "    if layer['ParentId'] == idd:\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e1a149",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt, layer in enumerate(list(filter(lambda x: not x['HasChild'] and x['ParentId'] == selectedId, li))):\n",
    "    print(layer['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b0429",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedId"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7395f6d",
   "metadata": {},
   "source": [
    "# Paul Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf2f0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "fi = cv2.imread(r\"E:\\source\\repos\\VisualDL\\custom_experiments\\2.png\")\n",
    "se = cv2.imread(r\"E:\\source\\repos\\VisualDL\\custom_experiments\\3.png\")\n",
    "res = cv2.absdiff(fi, se)\n",
    "res = res.astype(np.uint8)\n",
    "#--- find percentage difference based on number of pixels that are not zero ---\n",
    "percentage = (np.count_nonzero(res) * 100)/ res.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343a8acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01da989",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = cv2.subtract(cv2.imread(r\"E:\\source\\repos\\VisualDL\\custom_experiments\\2.png\"), cv2.imread(r\"E:\\source\\repos\\VisualDL\\custom_experiments\\3.png\"))\n",
    "difference = cv2.cvtColor(difference, cv2.COLOR_BGR2GRAY)\n",
    "difference[difference>0]=255\n",
    "print(1 - (np.count_nonzero(difference) / (1920*1080)))\n",
    "\n",
    "cv2.imwrite(\"Xd.png\", difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a3aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(im1, im2):\n",
    "    res = cv2.absdiff(im1, im2)\n",
    "    res = res.astype(np.uint8)\n",
    "    return (np.count_nonzero(res) * 100)/ res.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbc1df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video = cv2.VideoCapture(r\"C:\\Users\\phili\\Downloads\\ALM scam.mp4\")\n",
    "frames_per_second = int(input_video.get(cv2.CAP_PROP_FPS))\n",
    "frame_width = int(input_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(input_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6ecd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcd687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "last_image = None\n",
    "threshold = 0.9991\n",
    "# Loop through all the frames in the video\n",
    "\n",
    "while 1:\n",
    "    # Read the video to retrieve individual frames. 'frame' will reference the inidivdual frames read from the video.\n",
    "    ret, frame = input_video.read()\n",
    "    if not ret:\n",
    "        print('Processed all frames')\n",
    "        break\n",
    "    if last_image is None:\n",
    "        last_image = frame\n",
    "        continue\n",
    "    \n",
    "    di = diff(frame, last_image)\n",
    "    last_image = frame\n",
    "    if di > threshold:\n",
    "        count += 1\n",
    "        cv2.imwrite(f\"{count}.png\", frame)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b865a40e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
