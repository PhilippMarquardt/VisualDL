{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for Classification models trained using VDL\n",
    "\n",
    "### General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tqdm import tqdm\n",
    "from visualdl import vdl\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"1.5 dataset wo-Vac_complex_class_512px_-1_rgb_resnext50_32x4d.pt\"\n",
    "data_dir = r\"1.5 dataset wo-Vac_complex_class_512px_-1_rgb\\valid\"\n",
    "metrics_output_path = r\"1.5 dataset wo-Vac_complex_class_512px_-1_rgb\\metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vdl.get_inference_model(model_path, type=\"classification\")\n",
    "image_size = model.state[\"custom_data\"][\"image_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to hold predictions and labels\n",
    "y_true = []\n",
    "y_pred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to map subfolder names to numerical labels\n",
    "class_names = os.listdir(data_dir)\n",
    "class_mapping = {name: i for i, name in enumerate(class_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and make predictions\n",
    "for class_name in tqdm(class_names, desc=\"Classes\"):\n",
    "    class_dir = os.path.join(data_dir, class_name)\n",
    "    for img_name in tqdm(os.listdir(class_dir), desc=\"Images\", leave=False):\n",
    "        img_path = os.path.join(class_dir, img_name)\n",
    "\n",
    "        img = cv2.resize(\n",
    "            cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB),\n",
    "            (image_size, image_size),\n",
    "        )\n",
    "        logits = model.predict([img])[0]  # Assuming model.predict returns logits\n",
    "        probabilities = softmax(logits)  # Convert logits to probabilities if necessary\n",
    "\n",
    "        y_true.append(class_mapping[class_name])\n",
    "        y_pred.append(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to numpy arrays\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "n_classes = len(class_names)\n",
    "y_true_binarized = label_binarize(y_true, classes=range(n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "if n_classes == 2:\n",
    "    # Assuming y_pred[:, 1] is the probability of the positive class\n",
    "    fpr[0], tpr[0], _ = roc_curve(y_true, y_pred[:, 1])\n",
    "    roc_auc[0] = auc(fpr[0], tpr[0])\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true, y_pred[:, 1])\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "else:\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_binarized[:, i], y_pred[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_binarized.ravel(), y_pred.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute macro-average ROC curve and ROC area\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(\n",
    "    np.concatenate([fpr[i] for i in range(n_classes)] if n_classes > 2 else [fpr[0]])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then interpolate all ROC curves at these points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(\n",
    "        all_fpr, fpr[i if n_classes > 2 else 0], tpr[i if n_classes > 2 else 0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(\n",
    "    fpr[\"micro\"],\n",
    "    tpr[\"micro\"],\n",
    "    label=\"Micro-average ROC curve (area = {0:0.2f})\" \"\".format(roc_auc[\"micro\"]),\n",
    "    color=\"deeppink\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=4,\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    fpr[\"macro\"],\n",
    "    tpr[\"macro\"],\n",
    "    label=\"Macro-average ROC curve (area = {0:0.2f})\" \"\".format(roc_auc[\"macro\"]),\n",
    "    color=\"navy\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=4,\n",
    ")\n",
    "\n",
    "colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\"])\n",
    "if n_classes == 2:\n",
    "    plt.plot(\n",
    "        fpr[0],\n",
    "        tpr[0],\n",
    "        color=\"darkorange\",\n",
    "        lw=2,\n",
    "        label=\"ROC curve (area = {:.2f})\".format(roc_auc[0]),\n",
    "    )\n",
    "else:\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(\n",
    "            fpr[i],\n",
    "            tpr[i],\n",
    "            color=color,\n",
    "            lw=2,\n",
    "            label=\"ROC curve of class {} (area = {:.2f})\".format(\n",
    "                class_names[i], roc_auc[i]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Plot\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"micro_macro_average_roc_curve.png\", dpi=500)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, class_name in enumerate(class_names):\n",
    "    if n_classes == 2 and i > 0:\n",
    "        # Skip the first class in binary classification (usually representing the negative class)\n",
    "        continue\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true == i, y_pred[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(\n",
    "        fpr, tpr, color=\"darkorange\", lw=2, label=\"ROC curve (area = %0.2f)\" % roc_auc\n",
    "    )\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "\n",
    "    if n_classes == 2:\n",
    "        plt.title(f\"ROC Curve\")\n",
    "    else:\n",
    "        plt.title(f\"ROC Curve for {class_name}\")\n",
    "\n",
    "    plt.title(f\"ROC Curve for {class_name}\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)  # Add grid lines for better readability\n",
    "    plt.tight_layout()  # Adjust the layout to make room for the legend and labels\n",
    "    figure_name = f\"ROC_Curve_for_{class_name}.png\"\n",
    "    plt.savefig(figure_name, dpi=500)\n",
    "    plt.close()\n",
    "\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix, accuracy, and F1 score\n",
    "conf_mat = confusion_matrix(y_true, y_pred_labels)\n",
    "accuracy = accuracy_score(y_true, y_pred_labels)\n",
    "precision = precision_score(y_true, y_pred_labels, average=\"macro\")\n",
    "recall = recall_score(y_true, y_pred_labels, average=\"macro\")\n",
    "f1 = f1_score(y_true, y_pred_labels, average=\"macro\")\n",
    "specificity_scores = []\n",
    "for i in range(n_classes):  # Assuming n_classes is the number of unique classes\n",
    "    # For each class, calculate specificity\n",
    "    true_negatives = np.sum(np.delete(np.delete(conf_mat, i, 0), i, 1))\n",
    "    false_positives = np.sum(np.delete(conf_mat[:, i], i))\n",
    "    total_actual_negatives = true_negatives + false_positives\n",
    "    specificity_score = (\n",
    "        true_negatives / total_actual_negatives if total_actual_negatives != 0 else 0\n",
    "    )\n",
    "    specificity_scores.append(specificity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macro-average Specificity\n",
    "specificity = np.mean(specificity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these lines after the confusion matrix, accuracy, and F1 score calculations\n",
    "print(f\"Precision (Macro): {precision:.4f}\")\n",
    "print(f\"Recall (Macro): {recall:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix with annotations\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    conf_mat,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    ")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.tight_layout()  # Adjust layout to make room for the additional text\n",
    "os.makedirs(metrics_output_path, exist_ok=True)\n",
    "plt.savefig(os.path.join(metrics_output_path, \"confusion.png\"), dpi=500)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting the position of annotations\n",
    "# Increase the y-offset for the annotations if your class_names list is long\n",
    "y_offset_accuracy = len(class_names) + 0.7  # Adjusted y-offset for accuracy\n",
    "y_offset_f1 = len(class_names) + 1.0  # Adjusted y-offset for F1 score\n",
    "\n",
    "precision_offset = len(class_names) + 1.5  # Adjusted y-offset for F1 score\n",
    "recall_offset = len(class_names) + 2.0  # Adjusted y-offset for F1 score\n",
    "specifitiy_offset = len(class_names) + 2.5  # Adjusted y-offset for F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(metrics_output_path, \"averaged_metrics.txt\"), \"w\") as handle:\n",
    "    handle.write(f\"Accuarcy: {accuracy}\\n\")\n",
    "    handle.write(f\"F1: {f1}\\n\")\n",
    "    handle.write(f\"Precision: {precision}\\n\")\n",
    "    handle.write(f\"Recall: {recall}\\n\")\n",
    "    handle.write(f\"Specifitiy: {specificity}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
