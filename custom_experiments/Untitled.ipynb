{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa983342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from visualdl import vdl\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import timm\n",
    "import albumentations as A\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "from skimage import io\n",
    "#from custom import U2NET\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8d42c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vdl.get_inference_model(r\"F:\\source\\repos\\VisualDL\\resnet18, UnetPlusPlus.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414a67b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(r\"C:\\Users\\phili\\Documents\\001.pt\").keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b029cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cos(np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df04cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "t_range = np.arange(0, 2*np.pi, 0.05)\n",
    "for t in t_range:\n",
    "    #print((math.sin (t) * t, math.cos(t) * t))\n",
    "    plt.plot(np.sin(t) * t, np.cos(t) * t, markersize=1, marker='o')\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_color('none')\n",
    "    ax.spines['bottom'].set_position('zero')\n",
    "    ax.spines['left'].set_position('zero')\n",
    "    ax.spines['right'].set_color('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea980cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "timm.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d33b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.Resize(width=128, height=128),\n",
    "    A.RandomRotate90(p = 1),\n",
    "    A.Transpose(p=1),\n",
    "    A.RandomBrightness(p=1),\n",
    "    A.RandomContrast(p=1),\n",
    "    A.RandomShadow(p=1),\n",
    "    A.RGBShift(p=1),\n",
    "    A.RandomContrast(p=1),\n",
    "])\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(width=128, height=128),\n",
    "    A.GridDistortion(p=1),\n",
    "    A.OpticalDistortion(p=1),\n",
    "    A.ElasticTransform(p=1)\n",
    "\n",
    "])\n",
    "\n",
    "image =  io.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\CellsFinal\\Cells\\right side\\cells\\train\\images\\05__1_5291_9286.png\")\n",
    "mask = io.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\CellsFinal\\Cells\\right side\\cells\\train\\labels\\05__1_5291_9286.png\", as_gray = True)\n",
    "io.imsave(\"orig.png\", image)\n",
    "io.imsave(\"origmask.png\", mask)\n",
    "trans = transform(image = image, mask = mask)\n",
    "image = trans[\"image\"]\n",
    "mask = trans[\"mask\"]\n",
    "io.imsave(\"image.png\", image)\n",
    "io.imsave(\"mask.png\", mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10abfd40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d8b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0195a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(encoder_name = \"timm-resnest50d\", classes = 2, in_channels = 3)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0a49e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\timm-resnest50d, Unet.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a295f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells\\Cells\\test\\images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db05a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt, im in enumerate(os.listdir(image_dir)):\n",
    "    im = io.imread(os.path.join(image_dir, im))\n",
    "    im = im/255.\n",
    "    im = transform(image = im)[\"image\"]\n",
    "    im = torch.tensor(im, dtype = torch.float).permute(2, 0, 1).unsqueeze(0)\n",
    "    print(im.shape)\n",
    "    pred = model(im)\n",
    "    pred = torch.argmax(pred, 1)[0] * 255.\n",
    "    cv2.imwrite(str(cnt) + \".png\", pred.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390b4a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = cv2.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells1\\Cells\\train\\labels\\05__1_4685_10225.png\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c9ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = ii * 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a1d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"test.png\", ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187c5777",
   "metadata": {},
   "source": [
    "# Replace 2D with 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b75530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e72369",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, modules in model.named_modules():\n",
    "    for module in modules:\n",
    "        print(module)\n",
    "    if(isinstance(module, nn.Conv2d)):\n",
    "        kernel_size = module.kernel_size[0]\n",
    "        stride = module.stride[0]\n",
    "        padding = module.padding[0]\n",
    "        weight = module.weight.unsqueeze(2) / kernel_size\n",
    "        weight = torch.cat([weight for _ in range(0, kernel_size)], dim=2)\n",
    "        bias = module.bias\n",
    "\n",
    "        if(bias is None):\n",
    "            print(modules)\n",
    "            print(modules[name])\n",
    "            modules[name] = nn.Conv3d(in_channels=module.weight.shape[1], out_channels=module.weight.shape[0],\n",
    "                               kernel_size=kernel_size, padding=padding, stride=stride, bias=False)\n",
    "        else:\n",
    "            modules[name] = nn.Conv3d(in_channels=module.weight.shape[1], out_channels=module.weight.shape[0],\n",
    "                               kernel_size=kernel_size, padding=padding, stride=stride, bias=True)\n",
    "            modules[name].bias = bias\n",
    "\n",
    "            modules[name].weight.data = weight\n",
    "\n",
    "    elif(isinstance(module, nn.BatchNorm2d)):\n",
    "        weight = module.weight\n",
    "        bias = module.bias\n",
    "        modules[name] = nn.BatchNorm3d(weight.shape[0])\n",
    "        modules[name].weight = weight\n",
    "        modules[name].bias = bias\n",
    "\n",
    "for name in modules:\n",
    "    parent_module = model\n",
    "    objs = name.split(\".\")\n",
    "    if len(objs) == 1:\n",
    "        model.__setattr__(name, modules[name])\n",
    "        continue\n",
    "\n",
    "    for obj in objs[:-1]:\n",
    "        parent_module = parent_module.__getattr__(obj)\n",
    "\n",
    "    parent_module.__setattr__(objs[-1], modules[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609929d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_modules(module):\n",
    "    for a in module:\n",
    "        get_all_modules(a.children())\n",
    "        if isinstance(a, nn.Conv2d):\n",
    "            print(a)\n",
    "            #a = nn.Conv3d(3,32,3)\n",
    "            \n",
    "        elif isinstance(a, nn.BatchNorm2d):\n",
    "            print(a)\n",
    "\n",
    "            \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2654f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_modules(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5713df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in model.encoder.named_modules():\n",
    "    print(a)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc30e742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from uformer_pytorch import Uformer\n",
    "\n",
    "model = Uformer(\n",
    "    dim = 16,           # initial dimensions after input projection, which increases by 2x each stage\n",
    "    stages = 4,         # number of stages\n",
    "    num_blocks = 2,     # number of transformer blocks per stage\n",
    "    window_size = 16,   # set window size (along one side) for which to do the attention within\n",
    "    dim_head = 64,\n",
    "    heads = 1,\n",
    "    ff_mult = 4\n",
    ")\n",
    "model.cuda()\n",
    "x = torch.randn(1, 3, 512, 512).cuda()\n",
    "pred = model(x) # (1, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb84406",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dffd467",
   "metadata": {},
   "source": [
    "# Segmentation inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cc3c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from visualdl import vdl\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.transform import rescale, resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdl.predict(images, r\"E:\\source\\repos\\VisualDL\\tu-resnest50d, Unet.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f1c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uformer_pytorch import Uformer\n",
    "model = U2NET(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d91f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.create_model(arch = \"UnetPlusPlus\", encoder_name = \"tu-resnest50d\", classes = 3, in_channels = 3, image_size = 512,decoder_attention_type = None)\n",
    "#model1 = smp.create_model(arch = \"TransUnet\", encoder_name = \"tu-resnest101e\", classes = 2, in_channels = 3, image_size = 512,decoder_attention_type = \"scse\")\n",
    "#model = smp.create_model(arch = \"UnetPlusPlus\", encoder_name = \"resnet18\", classes = 6, in_channels = 3, image_size = 512, decoder_attention_type = \"scse\")\n",
    "model.load_state_dict(torch.load(r\"F:\\source\\repos\\VisualDL\\tu-resnest50d, UnetPlusPlus.pt\")['model_state_dict'])\n",
    "#model1.load_state_dict(torch.load(r\"F:\\source\\repos\\VisualDL\\custom_experiments\\512Best\\tu-resnest101e, TransUnet.pt\")['model_state_dict'])\n",
    "model.eval()\n",
    "#model1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d196607",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()\n",
    "#model1 = model1.cuda()\n",
    "#second = second.cuda()\n",
    "#third = third.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67a97c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4547c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.Resize(width=128, height=128),\n",
    "])\n",
    "#test = r\"F:\\source\\repos\\Daten\\HER-N\\Pdl1Combined\\Tumor Cells 512\\valid\\images\"\n",
    "test = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\job_instance_analysis\"\n",
    "test = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells her2 128\\valid\\images\"\n",
    "#test = r\"E:/source/repos/Daten/HER-N/hubt/dataset/Cells/valid/images\"\n",
    "for cnt, im in enumerate(os.listdir(test)):\n",
    "    image = io.imread(os.path.join(test, im))\n",
    "    image = transform(image = image)[\"image\"]\n",
    "    image = image/255.\n",
    "    s = torch.unsqueeze(torch.tensor(image, dtype = torch.float).permute(2, 0, 1), 0).cuda()\n",
    "    with torch.cuda.amp.autocast():\n",
    "        preds = model(s)\n",
    "        class_preds = preds[:,0:-1]\n",
    "        dist_map = m(preds[:,-1])\n",
    "        #dist_map[dist_map < 0.45] = 0\n",
    "        #preds += model1(s)\n",
    "    preds = torch.argmax(class_preds, 1)\n",
    "    dist_map[preds == 0] = 0.0\n",
    "    #class_for_dist = preds.clone()\n",
    "    #class_for_dist[class_for_dist>0] = 1\n",
    "    #dist_map[class_for_dist == 0] = 0\n",
    "\n",
    "    #preds *= 255\n",
    "    #preds[preds == 1] = 50\n",
    "    #preds[preds == 2] = 100\n",
    "    #preds[preds == 3] = 150\n",
    "    #preds[preds == 4] = 200\n",
    "    #preds[preds == 5] = 250\n",
    "    dist_map = dist_map.detach().cpu().numpy()\n",
    "    maps = dist_map[0]\n",
    "    #maps[maps>0] = 255\n",
    "    #maps = maps.astype(np.uint8)\n",
    "    print(image.shape)\n",
    "    print(maps.dtype)\n",
    "    #markers\t= cv2.watershed(preds[0].detach().cpu().numpy().astype(np.int), maps)\n",
    "    io.imsave(f\"{im}.png\", maps)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7bef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b= cv2.imread(r\"F:\\source\\repos\\VisualDL\\custom_experiments\\05__1_3118-10375_0.png.png\", 0)\n",
    "b[b>160] = 255\n",
    "b[b <=160] = 0\n",
    "cv2.imwrite(\"xd.png\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f19f593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92423dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5127b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79278e31",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d4df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells\\train\\labels\"\n",
    "ab = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells\\train\\bs\"\n",
    "for cnt, im in enumerate(os.listdir(test)):\n",
    "    img = cv2.imread(os.path.join(test, im)) * 255.\n",
    "    kernel = np.ones((3, 3), 'uint8')\n",
    "    dilate_img = cv2.dilate(img, kernel, iterations=1)\n",
    "    img1_bg = dilate_img - img\n",
    "    img1 = img1_bg[:,:,0]\n",
    "    clipped = np.clip(img1, 1, 6) # weight edges by factor (e.g. 6)\n",
    "    print(np.min(clipped))\n",
    "    cv2.imwrite(os.path.join(ab, im),clipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78dc66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch.losses import DiceLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af932e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DiceLoss(reduce = \"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8963cd",
   "metadata": {},
   "source": [
    "# Classification inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf46979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from skimage import io\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f6ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = timm.create_model(\"resnext50_32x4d\", pretrained=True, num_classes = 5).cuda()\n",
    "#second = timm.create_model(\"resnext50d_32x4d\", pretrained=True, num_classes = 5).cuda()\n",
    "first.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\resnext50_32x4d.pt\"))\n",
    "#second.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\resnext50d_32x4d.pt\"))\n",
    "first.eval()\n",
    "#second.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a827dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = timm.create_model(\"resnext50_32x4d\", pretrained=True, num_classes = 5).cuda()\n",
    "second = timm.create_model(\"efficientnet_b4\", pretrained=True, num_classes = 5).cuda()\n",
    "first.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\resnext50_32x4d.pt\"))\n",
    "#second.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\efficientnet_b4.pt\"))\n",
    "first.eval()\n",
    "#second.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30bc34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "transform = A.Compose([\n",
    "    A.Resize(width=512, height=512),\n",
    "])\n",
    "counter = 0\n",
    "counterxd = 0\n",
    "names = [\"NA\", \"TRG0\", \"TRG1\", \"TRG2\", \"TRG3\"]\n",
    "for name in names:\n",
    "    os.mkdir(name)\n",
    "values = dict()\n",
    "for cnt, name in enumerate(names):\n",
    "    values[name] = []\n",
    "    base = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset_valid/\" + name\n",
    "    for im in os.listdir(base):\n",
    "        image = io.imread(os.path.join(base, im)).astype(np.float32)\n",
    "        image = transform(image = image)[\"image\"]\n",
    "        image = image/255.\n",
    "        s = torch.unsqueeze(torch.tensor(image, dtype = torch.float).permute(2, 0, 1), 0).cuda()\n",
    "        preds =  first(s) \n",
    "        preds = torch.argmax(preds, 1)\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        values[name].append(preds[0])\n",
    "        counterxd += 1\n",
    "        io.imsave(f\"{names[preds[0]]}/{im}.png\", image)\n",
    "        if cnt == preds:\n",
    "            counter += 1\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076f2189",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter/counterxd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de14543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'NA': [4, 0, 0, 2, 0, 0],\n",
    " 'TRG0': [1, 4],\n",
    " 'TRG1': [2, 2, 2],\n",
    " 'TRG2': [3, 4, 3],\n",
    " 'TRG3': [4, 4, 4, 4, 4, 4, 4, 0, 4, 2, 4, 4, 4, 4, 4, 4, 2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d2f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset_valid\\TRG2\\E93 L X20_0_1183_3925.png\"\n",
    "base = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset_valid\\TRG2\"\n",
    "\n",
    "\n",
    "image = io.imread(path)\n",
    "image = image/255.\n",
    "s = torch.unsqueeze(torch.tensor(image, dtype = torch.float).permute(2, 0, 1), 0).cuda()\n",
    "preds = first(s) + second(s)\n",
    "preds = torch.argmax(preds, 1)\n",
    "preds = preds.detach().cpu().numpy()\n",
    "print(preds)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0872d3",
   "metadata": {},
   "source": [
    "# Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab262af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "start = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\test_data\\test_data\"\n",
    "\n",
    "for im in os.listdir(os.path.join(start, \"images\")):\n",
    "    img = cv2.imread(os.path.join(os.path.join(start, \"images\"), im), 0)\n",
    "    mask = cv2.imread(os.path.join(os.path.join(start, \"masks\"), im), 0)\n",
    "    mask[img == 0] = 0\n",
    "    mask[mask == 0] = 0\n",
    "    mask[mask == 29] = 1\n",
    "    mask[mask == 105] = 2\n",
    "    mask[mask == 117] = 3\n",
    "    mask[mask ==189] = 4\n",
    "    mask[mask == 225] = 5\n",
    "    mask[mask > 5] = 1\n",
    "    print(os.path.join(os.path.join(start, \"labels\"), im))\n",
    "    cv2.imwrite(os.path.join(os.path.join(start, \"labels\"), im), mask)\n",
    "    #img[img > 0] = 1\n",
    "    #cv2.imwrite(os.path.join(start, im), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebdae52",
   "metadata": {},
   "source": [
    "# Trian test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2884ed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = r\"E:\\source\\repos\\Daten\\PLA\\train\\images\"\n",
    "to = r\"E:\\source\\repos\\Daten\\PLA\\val\\images\"\n",
    "import random\n",
    "from shutil import copyfile\n",
    "import os\n",
    "split = os.listdir(start)\n",
    "random.shuffle(split)\n",
    "\n",
    "test = split[0:35]\n",
    "train = split[35:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c565f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in test:\n",
    "    copyfile(os.path.join(start, file), os.path.join(to, file))\n",
    "    copyfile(os.path.join(start, file).replace(\"images\", \"labels\"), os.path.join(to, file).replace(\"images\", \"labels\"))\n",
    "    os.remove(os.path.join(start, file))\n",
    "    os.remove(os.path.join(start, file).replace(\"images\", \"labels\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b88449",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Nuclei\\Nuclei\\train\\labels\"\n",
    "out = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Nuclei\\Nuclei\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(start):\n",
    "    preds = cv2.imread(os.path.join(start, file), 0)\n",
    "    preds[preds == 1] = 50\n",
    "    preds[preds == 2] = 100\n",
    "    preds[preds == 3] = 150\n",
    "    preds[preds == 4] = 200\n",
    "    preds[preds == 5] = 250\n",
    "    cv2.imwrite(os.path.join(out, file), preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3244b1a0",
   "metadata": {},
   "source": [
    "# 255 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b2d6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = r\"E:\\source\\repos\\Daten\\HER-N\\hubt\\dataset\\Cells\\valid\\labels\"\n",
    "import cv2\n",
    "import os\n",
    "for file in os.listdir(start):\n",
    "    img = cv2.imread(os.path.join(start, file), 0)\n",
    "    img[img > 0] = 1\n",
    "    cv2.imwrite(os.path.join(start, file), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0567e77b",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490756ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from uformer_pytorch import Uformer\n",
    "\n",
    "model = model = Uformer(\n",
    "                dim = 64,           # initial dimensions after input projection, which increases by 2x each stage\n",
    "                stages = 3,         # number of stages\n",
    "                num_blocks = 2,     # number of transformer blocks per stage\n",
    "                window_size = 16,   # set window size (along one side) for which to do the attention within\n",
    "                dim_head = 64,\n",
    "                heads = 4,\n",
    "                ff_mult = 2\n",
    "            ).cuda()\n",
    "\n",
    "x = torch.randn(2, 3, 128, 128).cuda()\n",
    "with torch.cuda.amp.autocast():\n",
    "    pred = model(x) # (1, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce132281",
   "metadata": {},
   "source": [
    "# Extract single instances for obj. detec + semant. seg -> bring into yolov5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080b1a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e39689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv.imread(r\"E:\\source\\repos\\Daten\\Converting\\Tumor Cells\\train\\labels\\PD-L1=2_0_37392-42804_85.png\", 0)\n",
    "\n",
    "# Creating kernel\n",
    "#kernel = np.ones((2, 2), np.uint8)\n",
    "  \n",
    "# Using cv2.erode() method \n",
    "#image = cv.erode(im, kernel) \n",
    "\n",
    "#image[image > 0] = 255\n",
    "#ret, thresh = cv.threshold(im, 127, 255, 0)\n",
    "#contours,hierachy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea0d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "im[im != 1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b1581",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(contours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e416ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.imwrite(\"xd.png\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba61ddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = {}\n",
    "start = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells her2 256\\valid\\labels\"\n",
    "all_files = os.listdir(start)\n",
    "nc = 2\n",
    "for img in os.listdir(start):\n",
    "    for i in range(1, nc):\n",
    "        im = cv.imread(os.path.join(start, img), 0)\n",
    "        kernel = np.ones((2, 2), np.uint8)\n",
    "        #im = cv.erode(im, kernel)\n",
    "        #im = cv.dilate(im, kernel)\n",
    "        tmp = im.copy()\n",
    "        tmp[tmp != i] = 0\n",
    "        tmp[tmp == i] = 255\n",
    "        #if img == \"05__1_3130-9263_11.png\":\n",
    "        #    cv2.imwrite(\"xd.png\", tmp)\n",
    "        contours,hierachy = cv.findContours(tmp, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "        blank = np.zeros_like(tmp)\n",
    "        for cnt, cont in enumerate(contours):\n",
    "            xmin,ymin,width,height = cv.boundingRect(cont)\n",
    "            if width <= 3 or height <= 3:\n",
    "                continue\n",
    "            #cv.rectangle(im,(x,y),(x+width,y+height),(255),1)\n",
    "            #cv2.imwrite(\"xd.png\", im)\n",
    "            image_width = im.shape[0]\n",
    "            xcenter, ycenter = xmin + width/2, ymin + height/2\n",
    "            xcenter, ycenter, width, height = xcenter/image_width, ycenter/image_width, width/image_width, height/image_width\n",
    "            if not img in files:\n",
    "                files[img] = [(str(i - 1),str(xcenter), str(ycenter), str(width), str(height))]\n",
    "            else:\n",
    "\n",
    "                files[img] += [(str(i - 1),str(xcenter), str(ycenter), str(width), str(height))]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a93a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "base = cv.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells her2 256\\train\\images\\05__1_3130-9263_11.png\")\n",
    "\n",
    "for rec in files['05__1_3130-9263_11.png']:\n",
    "    xcenter,ycenter,width,height = [int(float(xx) * base.shape[0]) for xx in rec[1:]]\n",
    "    x = int(xcenter - width/2)\n",
    "    y = int(ycenter - height/2)\n",
    "    va = int(rec[0] * 50)\n",
    "    print(rec)\n",
    "    print(\"\\nXD\")\n",
    "    cv.rectangle(base,(x,y),(x+width,y+height),(50,va * 50,va * 50),1)\n",
    "cv.imwrite(\"xd4.png\", base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c82d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv.imread(os.path.join(r\"E:\\source\\repos\\Daten\\Cells\\train\\images\", '05__1_3115_10030.png'), 0)\n",
    "kernel = np.ones((2, 2), np.uint8)\n",
    "image = cv.erode(im, kernel) \n",
    "contours,hierachy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "blank = np.zeros_like(im)\n",
    "for cnt, cont in enumerate(contours):\n",
    "    x,y,width,height = cv.boundingRect(cont)\n",
    "    cv.rectangle(im,(x,y),(x+width,y+height),(255),1)\n",
    "cv2.imwrite(\"xd.png\", im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51cb372",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_anno = [item for item in all_files if item not in list(files.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e9b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"labels\")\n",
    "for cnt, (key, val) in enumerate(files.items()):\n",
    "    with open(\"labels/\" + key.replace(\".png\", \".txt\"), \"w\") as handle:\n",
    "        for va in val:\n",
    "            handle.write(\" \".join(list(va))+ \"\\n\") \n",
    "            \n",
    "for name in no_anno:\n",
    "    with open(\"labels/\" + name.replace(\".png\", \".txt\"), \"w\") as handle:\n",
    "        handle.write(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891f647a",
   "metadata": {},
   "source": [
    "# Export for instance segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50876230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abacfd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"labels\")\n",
    "os.mkdir(\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86410f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_rec(img, rec):\n",
    "    x,y,w,h = rec\n",
    "    return img[y:y+h, x:x+w]\n",
    "def add_rec(orig, img, rec):\n",
    "    x,y,w,h = rec\n",
    "    #r,g,b = [random.randint(20, 255) for i in range(3)]\n",
    "    img[img > 0] = random.randint(20,255)\n",
    "    tmp = np.expand_dims(img.astype(np.uint8), axis=-1)\n",
    "    #tmp[np.all(tmp == (255, 255, 255), axis=-1)] = (b,g,r)\n",
    "    orig[y:y+h, x:x+w] += tmp\n",
    "    return orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e98f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pad = A.Compose([\n",
    "    A.PadIfNeeded(64,64, value = 0, border_mode = 0),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c44bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "labels = r\"E:\\source\\repos\\Daten\\Cells\\train\\labels\"\n",
    "images = r\"E:\\source\\repos\\Daten\\Cells\\train\\images\"\n",
    "to_labels = r\"E:\\source\\repos\\VisualDL\\custom_experiments\\labels\"\n",
    "to_images = r\"E:\\source\\repos\\VisualDL\\custom_experiments\\images\"\n",
    "all_files = os.listdir(labels)\n",
    "adding = 0\n",
    "for img in os.listdir(labels):\n",
    "    im = cv.imread(os.path.join(labels, img), 0)\n",
    "    original = cv.imread(os.path.join(images, img))\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    image = cv.erode(im, kernel) \n",
    "    contours,hierachy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    for cnt, cont in enumerate(contours):\n",
    "        blank = np.zeros_like(im)\n",
    "        xmin,ymin,width,height = cv.boundingRect(cont)\n",
    "        xmin -= 5\n",
    "        ymin -= 5\n",
    "        width +=5\n",
    "        height += 5\n",
    "        xmin = min(0, xmin)\n",
    "        ymin = min(0, ymin)\n",
    "        cv.drawContours(blank, [cont], -1, 1, -1)\n",
    "        final = cut_rec(blank, (xmin, ymin , width , height ))\n",
    "        orig_final = cut_rec(original, (xmin, ymin , width , height ))\n",
    "        trans = pad(image = orig_final, mask = final)\n",
    "        final = trans[\"mask\"]\n",
    "        orig_final = trans[\"image\"]\n",
    "        cv.imwrite(os.path.join(to_labels, img.replace(\".png\", f\"{cnt}.png\")), final)\n",
    "        cv.imwrite(os.path.join(to_images, img.replace(\".png\", f\"{cnt}.png\")), orig_final)\n",
    "        #cv.rectangle(im,(x,y),(x+width,y+height),(255),1)\n",
    "        \n",
    "        #cv2.imwrite(\"xd.png\", im)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b4cab0",
   "metadata": {},
   "source": [
    "# instance segmentation inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc80695",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.UnetPlusPlus(encoder_name = \"resnext50_32x4d\", classes = 2, in_channels = 3)\n",
    "model.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\resnext50_32x4d, UnetPlusPlus.pt\"))\n",
    "model.eval()\n",
    "model=model.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9078c0",
   "metadata": {},
   "source": [
    "## simple predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed2ae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    #A.PadIfNeeded(64,64, value = 0, border_mode = 0),\n",
    "    A.Resize(width=64, height=64),\n",
    "])\n",
    "test = r\"E:\\source\\repos\\VisualDL\\custom_experiments\\valid\\images\"\n",
    "for cnt, im in enumerate(os.listdir(test)):\n",
    "    image = io.imread(os.path.join(test, im))\n",
    "    image = transform(image = image)[\"image\"]\n",
    "    image = image/255.\n",
    "    s = torch.unsqueeze(torch.tensor(image, dtype = torch.float).permute(2, 0, 1), 0).cuda()\n",
    "    with torch.cuda.amp.autocast():\n",
    "        preds = model(s)\n",
    "    preds = torch.argmax(preds, 1)\n",
    "    preds *= 255\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    io.imsave(f\"{im}.png\", preds[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf9c6b5",
   "metadata": {},
   "source": [
    "# use rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8452363",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.PadIfNeeded(64,64, value = 0, border_mode = 0),\n",
    "    A.Resize(width=64, height=64),\n",
    "])\n",
    "\n",
    "files = {}\n",
    "labels = r\"E:\\source\\repos\\Daten\\Cells\\valid\\labels\"\n",
    "images = r\"E:\\source\\repos\\Daten\\Cells\\valid\\images\"\n",
    "to = r\"E:\\source\\repos\\VisualDL\\custom_experiments\\test\"\n",
    "all_files = os.listdir(labels)\n",
    "adding = 0\n",
    "for ii, img in enumerate(os.listdir(labels)):\n",
    "    im = cv.imread(os.path.join(labels, img), 0)\n",
    "    print(img)\n",
    "    original = cv.imread(os.path.join(images, img))\n",
    "    blank = np.zeros_like(original)\n",
    "    original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    image = cv.erode(im, kernel) \n",
    "    contours,hierachy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "    for cnt, cont in enumerate(contours):\n",
    "        \n",
    "        xmin,ymin,width,height = cv.boundingRect(cont)\n",
    "        orig_final = cut_rec(original, (xmin, ymin , width , height ))\n",
    "        tt = orig_final.copy()\n",
    "        orig_final = transform(image = orig_final)[\"image\"]\n",
    "        image = orig_final/255.\n",
    "        s = torch.unsqueeze(torch.tensor(image, dtype = torch.float).permute(2, 0, 1), 0).cuda()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            preds = model(s)\n",
    "        preds = torch.argmax(preds, 1)\n",
    "        preds *= 255\n",
    "        preds = preds.detach().cpu().numpy()[0]\n",
    "\n",
    "        preds = cut_rec(preds, (32 - int(width/2), 32 - int(height/2), width, height))\n",
    "        #contours,hierachy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "        blank = add_rec(blank, preds, (xmin,ymin,width,height))\n",
    "        cv2.imwrite(os.path.join(to, f\"{img}{cnt}.png\"), preds)\n",
    "        cv2.imwrite(os.path.join(to, f\"{img}{cnt}orig.png\"), tt)\n",
    "    cv2.imwrite(os.path.join(to, f\"{img}{cnt}xdddd.png\"), blank)\n",
    "    if ii == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658f0779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a3aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "skimage.skimage.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de6e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = U2NET(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aa5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_in = torch.randn(1, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0201e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, einsum\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from custom import ViT, AxialImageTransformer, AxialAttention\n",
    "# helpers\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def divisible_by(val, divisor):\n",
    "    return (val % divisor) == 0\n",
    "\n",
    "def unfold_output_size(image_size, kernel_size, stride, padding):\n",
    "    return int(((image_size - kernel_size + (2 * padding)) / stride) + 1)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = heads * dim_head\n",
    "        self.heads =  heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, d, h = *x.shape, self.heads\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
    "\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "        attn = sim.softmax(dim = -1)\n",
    "\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "# main class\n",
    "\n",
    "class TNT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        image_size,\n",
    "        patch_dim,\n",
    "        pixel_dim,\n",
    "        patch_size,\n",
    "        pixel_size,\n",
    "        depth,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        ff_dropout = 0.,\n",
    "        attn_dropout = 0.,\n",
    "        channels = 3,\n",
    "        unfold_args = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert divisible_by(image_size, patch_size), 'image size must be divisible by patch size'\n",
    "        assert divisible_by(patch_size, pixel_size), 'patch size must be divisible by pixel size for now'\n",
    "\n",
    "        num_patch_tokens = (image_size // patch_size) ** 2\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patch_tokens = nn.Parameter(torch.randn(num_patch_tokens + 1, patch_dim))\n",
    "\n",
    "        unfold_args = default(unfold_args, (pixel_size, pixel_size, 0))\n",
    "        unfold_args = (*unfold_args, 0) if len(unfold_args) == 2 else unfold_args\n",
    "        kernel_size, stride, padding = unfold_args\n",
    "\n",
    "        pixel_width = unfold_output_size(patch_size, kernel_size, stride, padding)\n",
    "        num_pixels = pixel_width ** 2\n",
    "\n",
    "        self.to_pixel_tokens = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> (b h w) c p1 p2', p1 = patch_size, p2 = patch_size),\n",
    "            nn.Unfold(kernel_size = kernel_size, stride = stride, padding = padding),\n",
    "            Rearrange('... c n -> ... n c'),\n",
    "            nn.Linear(channels * kernel_size ** 2, pixel_dim)\n",
    "        )\n",
    "\n",
    "        self.patch_pos_emb = nn.Parameter(torch.randn(num_patch_tokens + 1, patch_dim))\n",
    "        self.pixel_pos_emb = nn.Parameter(torch.randn(num_pixels, pixel_dim))\n",
    "\n",
    "        layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "\n",
    "            pixel_to_patch = nn.Sequential(\n",
    "                nn.LayerNorm(pixel_dim),\n",
    "                Rearrange('... n d -> ... (n d)'),\n",
    "                nn.Linear(pixel_dim * num_pixels, patch_dim),\n",
    "            )\n",
    "\n",
    "            layers.append(nn.ModuleList([\n",
    "                PreNorm(pixel_dim, Attention(dim = pixel_dim, heads = heads, dim_head = dim_head, dropout = attn_dropout)),\n",
    "                PreNorm(pixel_dim, FeedForward(dim = pixel_dim, dropout = ff_dropout)),\n",
    "                pixel_to_patch,\n",
    "                PreNorm(patch_dim, Attention(dim = patch_dim, heads = heads, dim_head = dim_head, dropout = attn_dropout)),\n",
    "                PreNorm(patch_dim, FeedForward(dim = patch_dim, dropout = ff_dropout)),\n",
    "            ]))\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, _, h, w, patch_size, image_size = *x.shape, self.patch_size, self.image_size\n",
    "        assert divisible_by(h, patch_size) and divisible_by(w, patch_size), f'height {h} and width {w} of input must be divisible by the patch size'\n",
    "\n",
    "        num_patches_h = h // patch_size\n",
    "        num_patches_w = w // patch_size\n",
    "        n = num_patches_w * num_patches_h\n",
    "\n",
    "        pixels = self.to_pixel_tokens(x)\n",
    "        patches = repeat(self.patch_tokens[:(n + 1)], 'n d -> b n d', b = b)\n",
    "\n",
    "        patches += rearrange(self.patch_pos_emb[:(n + 1)], 'n d -> () n d')\n",
    "        pixels += rearrange(self.pixel_pos_emb, 'n d -> () n d')\n",
    "\n",
    "        for pixel_attn, pixel_ff, pixel_to_patch_residual, patch_attn, patch_ff in self.layers:\n",
    "\n",
    "            pixels = pixel_attn(pixels) + pixels\n",
    "            pixels = pixel_ff(pixels) + pixels\n",
    "\n",
    "            patches_residual = pixel_to_patch_residual(pixels)\n",
    "\n",
    "            patches_residual = rearrange(patches_residual, '(b h w) d -> b (h w) d', h = num_patches_h, w = num_patches_w)\n",
    "            patches_residual = F.pad(patches_residual, (0, 0, 1, 0), value = 0) # cls token gets residual of 0\n",
    "            patches = patches + patches_residual\n",
    "\n",
    "            patches = patch_attn(patches) + patches\n",
    "            patches = patch_ff(patches) + patches\n",
    "        hidden_states = patches[:,1:,:]\n",
    "        B, n_patch, hidden = hidden_states.size()  # reshape from (B, n_patch, hidden) to (B, h, w, hidden)\n",
    "        h, w = int(np.sqrt(n_patch)), int(np.sqrt(n_patch))\n",
    "        x = hidden_states.permute(0, 2, 1)\n",
    "        x = x.contiguous().view(B, hidden, h, w)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Conv2dReLU(nn.Sequential):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            padding=0,\n",
    "            stride=1,\n",
    "            use_batchnorm=True,\n",
    "    ):\n",
    "\n",
    "        if use_batchnorm == \"inplace\" and InPlaceABN is None:\n",
    "            raise RuntimeError(\n",
    "                \"In order to use `use_batchnorm='inplace'` inplace_abn package must be installed. \"\n",
    "                + \"To install see: https://github.com/mapillary/inplace_abn\"\n",
    "            )\n",
    "\n",
    "        conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=not (use_batchnorm),\n",
    "        )\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        if use_batchnorm == \"inplace\":\n",
    "            bn = InPlaceABN(out_channels, activation=\"leaky_relu\", activation_param=0.0)\n",
    "            relu = nn.Identity()\n",
    "\n",
    "        elif use_batchnorm and use_batchnorm != \"inplace\":\n",
    "            bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        else:\n",
    "            bn = nn.Identity()\n",
    "\n",
    "        super(Conv2dReLU, self).__init__(conv, bn, relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1990a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "image_size = 128\n",
    "init_dim = 16\n",
    "conv = Conv2dReLU(3, init_dim, 1).cuda()\n",
    "inst = AxialImageTransformer(dim = init_dim,depth = 2,axial_pos_emb_shape = (image_size,image_size)).cuda()\n",
    "\n",
    "first = TNT(image_size = image_size, patch_size = 2, pixel_dim = 16, pixel_size = 1, patch_dim = init_dim * 2, depth = 2, heads = 4, channels = init_dim).cuda()\n",
    "second = TNT(image_size = image_size//2, patch_size = 2, pixel_dim = 16, pixel_size = 1, patch_dim = init_dim * 4, depth = 4, heads = 6, channels = init_dim * 2).cuda()\n",
    "third = TNT(image_size = image_size//4, patch_size = 2, pixel_dim = 16, pixel_size = 1, patch_dim = init_dim * 8, depth = 6, heads = 8, channels = init_dim * 4).cuda()\n",
    "fourth = TNT(image_size = image_size//8, patch_size = 2, pixel_dim = 16, pixel_size = 1, patch_dim = init_dim * 16, depth = 8, heads = 12, channels = init_dim * 8).cuda()\n",
    "cup = ViT(\n",
    "            image_size = 8,\n",
    "            patch_size = 1,\n",
    "            dim = init_dim * 32,\n",
    "            depth = 6,\n",
    "            heads = 12,\n",
    "            mlp_dim = 2048,\n",
    "            dropout = 0.1,\n",
    "            emb_dropout = 0.1,\n",
    "            channels = init_dim * 16\n",
    "        ).cuda()\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                in_channels,\n",
    "                skip_channels,\n",
    "                out_channels,\n",
    "                depth,\n",
    "                size,\n",
    "                heads= 2):\n",
    "        super().__init__()   \n",
    "        self.ax = AxialImageTransformer(dim = in_channels + skip_channels,heads= heads,depth = depth,axial_pos_emb_shape = (size,size)).cuda()\n",
    "        self.out = Conv2dReLU(in_channels + skip_channels, out_channels, 1)\n",
    "        \n",
    "    def forward(self, x, skip = None):\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        return self.out(self.ax(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2090bb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = 2\n",
    "up1 = DecoderBlock(init_dim*32, init_dim*8, init_dim*8, depth = 2, heads = 2,size = image_size//8).cuda()\n",
    "up2 = DecoderBlock(init_dim*8, init_dim*4, init_dim*4, depth = 2, heads = 2,size = image_size//4).cuda()\n",
    "up3 = DecoderBlock(init_dim*4, init_dim*2, init_dim*2, depth = 2, heads = 2,size = image_size//2).cuda()\n",
    "up4 = DecoderBlock(init_dim*2, init_dim, 2, depth = 2, heads = 2,size = image_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abe12ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=AxialAttention(32,dim_index =1).cuda()\n",
    "a= AxialImageTransformer(dim = 3072, depth = 1,axial_pos_emb_shape = (128,128), heads=1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ce9eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_in = torch.randn(2, 3072, 128, 128).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ddc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "a(dummy_in).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c640cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inst(conv(dummy_in)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b26bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(hidden_states):\n",
    "    B, n_patch, hidden = hidden_states.size()  # reshape from (B, n_patch, hidden) to (B, h, w, hidden)\n",
    "    h, w = int(np.sqrt(n_patch)), int(np.sqrt(n_patch))\n",
    "    x = hidden_states.permute(0, 2, 1)\n",
    "    x = x.contiguous().view(B, hidden, h, w)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a3629",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.cuda.amp.autocast():\n",
    "    out1 = inst(conv(dummy_in)) #128\n",
    "with torch.cuda.amp.autocast():\n",
    "    out2 = first(out1) #64\n",
    "with torch.cuda.amp.autocast():\n",
    "    out3 = second(out2) #32\n",
    "with torch.cuda.amp.autocast():\n",
    "    out4 = third(out3) #16\n",
    "with torch.cuda.amp.autocast():\n",
    "    out5 = fourth(out4) #8\n",
    "with torch.cuda.amp.autocast():\n",
    "     out6 = cup(out5) #8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca05747f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "up4(up3(up2(up1(out6, out4), out3), out2), out1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2127a1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hidden_states = out\n",
    "B, n_patch, hidden = hidden_states.size()  # reshape from (B, n_patch, hidden) to (B, h, w, hidden)\n",
    "h, w = int(np.sqrt(n_patch)), int(np.sqrt(n_patch))\n",
    "x = hidden_states.permute(0, 2, 1)\n",
    "x = x.contiguous().view(B, hidden, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ea3048",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e79fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "2**5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e0f671",
   "metadata": {},
   "source": [
    "# HRNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d30d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "from torch.nn.parallel._functions import ReduceAddCoalesced, Broadcast\n",
    "\n",
    "class SyncMaster(object):\n",
    "    \"\"\"An abstract `SyncMaster` object.\n",
    "    - During the replication, as the data parallel will trigger an callback of each module, all slave devices should\n",
    "    call `register(id)` and obtain an `SlavePipe` to communicate with the master.\n",
    "    - During the forward pass, master device invokes `run_master`, all messages from slave devices will be collected,\n",
    "    and passed to a registered callback.\n",
    "    - After receiving the messages, the master device should gather the information and determine to message passed\n",
    "    back to each slave devices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, master_callback):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            master_callback: a callback to be invoked after having collected messages from slave devices.\n",
    "        \"\"\"\n",
    "        self._master_callback = master_callback\n",
    "        self._queue = queue.Queue()\n",
    "        self._registry = collections.OrderedDict()\n",
    "        self._activated = False\n",
    "\n",
    "    def register_slave(self, identifier):\n",
    "        \"\"\"\n",
    "        Register an slave device.\n",
    "        Args:\n",
    "            identifier: an identifier, usually is the device id.\n",
    "        Returns: a `SlavePipe` object which can be used to communicate with the master device.\n",
    "        \"\"\"\n",
    "        if self._activated:\n",
    "            assert self._queue.empty(), 'Queue is not clean before next initialization.'\n",
    "            self._activated = False\n",
    "            self._registry.clear()\n",
    "        future = FutureResult()\n",
    "        self._registry[identifier] = _MasterRegistry(future)\n",
    "        return SlavePipe(identifier, self._queue, future)\n",
    "\n",
    "class _SynchronizedBatchNorm(_BatchNorm):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.001, affine=True):\n",
    "        super(_SynchronizedBatchNorm, self).__init__(num_features, eps=eps, momentum=momentum, affine=affine)\n",
    "\n",
    "        self._sync_master = SyncMaster(self._data_parallel_master)\n",
    "\n",
    "        self._is_parallel = False\n",
    "        self._parallel_id = None\n",
    "        self._slave_pipe = None\n",
    "\n",
    "        # customed batch norm statistics\n",
    "        self._moving_average_fraction = 1. - momentum\n",
    "        self.register_buffer('_tmp_running_mean', torch.zeros(self.num_features))\n",
    "        self.register_buffer('_tmp_running_var', torch.ones(self.num_features))\n",
    "        self.register_buffer('_running_iter', torch.ones(1))\n",
    "        self._tmp_running_mean = self.running_mean.clone() * self._running_iter\n",
    "        self._tmp_running_var = self.running_var.clone() * self._running_iter\n",
    "\n",
    "    def forward(self, input):\n",
    "        # If it is not parallel computation or is in evaluation mode, use PyTorch's implementation.\n",
    "        if not (self._is_parallel and self.training):\n",
    "            return F.batch_norm(\n",
    "                input, self.running_mean, self.running_var, self.weight, self.bias,\n",
    "                self.training, self.momentum, self.eps)\n",
    "\n",
    "        # Resize the input to (B, C, -1).\n",
    "        input_shape = input.size()\n",
    "        input = input.view(input.size(0), self.num_features, -1)\n",
    "\n",
    "        # Compute the sum and square-sum.\n",
    "        sum_size = input.size(0) * input.size(2)\n",
    "        input_sum = _sum_ft(input)\n",
    "        input_ssum = _sum_ft(input ** 2)\n",
    "\n",
    "        # Reduce-and-broadcast the statistics.\n",
    "        if self._parallel_id == 0:\n",
    "            mean, inv_std = self._sync_master.run_master(_ChildMessage(input_sum, input_ssum, sum_size))\n",
    "        else:\n",
    "            mean, inv_std = self._slave_pipe.run_slave(_ChildMessage(input_sum, input_ssum, sum_size))\n",
    "\n",
    "        # Compute the output.\n",
    "        if self.affine:\n",
    "            # MJY:: Fuse the multiplication for speed.\n",
    "            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std * self.weight) + _unsqueeze_ft(self.bias)\n",
    "        else:\n",
    "            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std)\n",
    "\n",
    "        # Reshape it.\n",
    "        return output.view(input_shape)\n",
    "\n",
    "    def __data_parallel_replicate__(self, ctx, copy_id):\n",
    "        self._is_parallel = True\n",
    "        self._parallel_id = copy_id\n",
    "\n",
    "        # parallel_id == 0 means master device.\n",
    "        if self._parallel_id == 0:\n",
    "            ctx.sync_master = self._sync_master\n",
    "        else:\n",
    "            self._slave_pipe = ctx.sync_master.register_slave(copy_id)\n",
    "\n",
    "    def _data_parallel_master(self, intermediates):\n",
    "        \"\"\"Reduce the sum and square-sum, compute the statistics, and broadcast it.\"\"\"\n",
    "        intermediates = sorted(intermediates, key=lambda i: i[1].sum.get_device())\n",
    "\n",
    "        to_reduce = [i[1][:2] for i in intermediates]\n",
    "        to_reduce = [j for i in to_reduce for j in i]  # flatten\n",
    "        target_gpus = [i[1].sum.get_device() for i in intermediates]\n",
    "\n",
    "        sum_size = sum([i[1].sum_size for i in intermediates])\n",
    "        sum_, ssum = ReduceAddCoalesced.apply(target_gpus[0], 2, *to_reduce)\n",
    "\n",
    "        mean, inv_std = self._compute_mean_std(sum_, ssum, sum_size)\n",
    "\n",
    "        broadcasted = Broadcast.apply(target_gpus, mean, inv_std)\n",
    "\n",
    "        outputs = []\n",
    "        for i, rec in enumerate(intermediates):\n",
    "            outputs.append((rec[0], _MasterMessage(*broadcasted[i*2:i*2+2])))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _add_weighted(self, dest, delta, alpha=1, beta=1, bias=0):\n",
    "        \"\"\"return *dest* by `dest := dest*alpha + delta*beta + bias`\"\"\"\n",
    "        return dest * alpha + delta * beta + bias\n",
    "\n",
    "    def _compute_mean_std(self, sum_, ssum, size):\n",
    "        \"\"\"Compute the mean and standard-deviation with sum and square-sum. This method\n",
    "        also maintains the moving average on the master device.\"\"\"\n",
    "        assert size > 1, 'BatchNorm computes unbiased standard-deviation, which requires size > 1.'\n",
    "        mean = sum_ / size\n",
    "        sumvar = ssum - sum_ * mean\n",
    "        unbias_var = sumvar / (size - 1)\n",
    "        bias_var = sumvar / size\n",
    "\n",
    "        self._tmp_running_mean = self._add_weighted(self._tmp_running_mean, mean.data, alpha=self._moving_average_fraction)\n",
    "        self._tmp_running_var = self._add_weighted(self._tmp_running_var, unbias_var.data, alpha=self._moving_average_fraction)\n",
    "        self._running_iter = self._add_weighted(self._running_iter, 1, alpha=self._moving_average_fraction)\n",
    "\n",
    "        self.running_mean = self._tmp_running_mean / self._running_iter\n",
    "        self.running_var = self._tmp_running_var / self._running_iter\n",
    "\n",
    "        return mean, bias_var.clamp(self.eps) ** -0.5\n",
    "    \n",
    "class SynchronizedBatchNorm2d(_SynchronizedBatchNorm):\n",
    "    r\"\"\"Applies Batch Normalization over a 4d input that is seen as a mini-batch\n",
    "    of 3d inputs\n",
    "    .. math::\n",
    "        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
    "    This module differs from the built-in PyTorch BatchNorm2d as the mean and\n",
    "    standard-deviation are reduced across all devices during training.\n",
    "    For example, when one uses `nn.DataParallel` to wrap the network during\n",
    "    training, PyTorch's implementation normalize the tensor on each device using\n",
    "    the statistics only on that device, which accelerated the computation and\n",
    "    is also easy to implement, but the statistics might be inaccurate.\n",
    "    Instead, in this synchronized version, the statistics will be computed\n",
    "    over all training samples distributed on multiple devices.\n",
    "    \n",
    "    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n",
    "    as the built-in PyTorch implementation.\n",
    "    The mean and standard-deviation are calculated per-dimension over\n",
    "    the mini-batches and gamma and beta are learnable parameter vectors\n",
    "    of size C (where C is the input size).\n",
    "    During training, this layer keeps a running estimate of its computed mean\n",
    "    and variance. The running sum is kept with a default momentum of 0.1.\n",
    "    During evaluation, this running mean/variance is used for normalization.\n",
    "    Because the BatchNorm is done over the `C` dimension, computing statistics\n",
    "    on `(N, H, W)` slices, it's common terminology to call this Spatial BatchNorm\n",
    "    Args:\n",
    "        num_features: num_features from an expected input of\n",
    "            size batch_size x num_features x height x width\n",
    "        eps: a value added to the denominator for numerical stability.\n",
    "            Default: 1e-5\n",
    "        momentum: the value used for the running_mean and running_var\n",
    "            computation. Default: 0.1\n",
    "        affine: a boolean value that when set to ``True``, gives the layer learnable\n",
    "            affine parameters. Default: ``True``\n",
    "    Shape:\n",
    "        - Input: :math:`(N, C, H, W)`\n",
    "        - Output: :math:`(N, C, H, W)` (same shape as input)\n",
    "    Examples:\n",
    "        >>> # With Learnable Parameters\n",
    "        >>> m = SynchronizedBatchNorm2d(100)\n",
    "        >>> # Without Learnable Parameters\n",
    "        >>> m = SynchronizedBatchNorm2d(100, affine=False)\n",
    "        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45))\n",
    "        >>> output = m(input)\n",
    "    \"\"\"\n",
    "\n",
    "    def _check_input_dim(self, input):\n",
    "        if input.dim() != 4:\n",
    "            raise ValueError('expected 4D input (got {}D input)'\n",
    "                             .format(input.dim()))\n",
    "        super(SynchronizedBatchNorm2d, self)._check_input_dim(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5289bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This HRNet implementation is modified from the following repository:\n",
    "https://github.com/HRNet/HRNet-Semantic-Segmentation\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "BatchNorm2d = SynchronizedBatchNorm2d\n",
    "BN_MOMENTUM = 0.1\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "__all__ = ['hrnetv2']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'hrnetv2': 'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/hrnetv2_w48-imagenet.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.bn3 = BatchNorm2d(planes * self.expansion,\n",
    "                               momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class HighResolutionModule(nn.Module):\n",
    "    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\n",
    "                 num_channels, fuse_method, multi_scale_output=True):\n",
    "        super(HighResolutionModule, self).__init__()\n",
    "        self._check_branches(\n",
    "            num_branches, blocks, num_blocks, num_inchannels, num_channels)\n",
    "\n",
    "        self.num_inchannels = num_inchannels\n",
    "        self.fuse_method = fuse_method\n",
    "        self.num_branches = num_branches\n",
    "\n",
    "        self.multi_scale_output = multi_scale_output\n",
    "\n",
    "        self.branches = self._make_branches(\n",
    "            num_branches, blocks, num_blocks, num_channels)\n",
    "        self.fuse_layers = self._make_fuse_layers()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def _check_branches(self, num_branches, blocks, num_blocks,\n",
    "                        num_inchannels, num_channels):\n",
    "        if num_branches != len(num_blocks):\n",
    "            error_msg = 'NUM_BRANCHES({}) <> NUM_BLOCKS({})'.format(\n",
    "                num_branches, len(num_blocks))\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "        if num_branches != len(num_channels):\n",
    "            error_msg = 'NUM_BRANCHES({}) <> NUM_CHANNELS({})'.format(\n",
    "                num_branches, len(num_channels))\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "        if num_branches != len(num_inchannels):\n",
    "            error_msg = 'NUM_BRANCHES({}) <> NUM_INCHANNELS({})'.format(\n",
    "                num_branches, len(num_inchannels))\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\n",
    "                         stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or \\\n",
    "           self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.num_inchannels[branch_index],\n",
    "                          num_channels[branch_index] * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                BatchNorm2d(num_channels[branch_index] * block.expansion,\n",
    "                            momentum=BN_MOMENTUM),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.num_inchannels[branch_index],\n",
    "                            num_channels[branch_index], stride, downsample))\n",
    "        self.num_inchannels[branch_index] = \\\n",
    "            num_channels[branch_index] * block.expansion\n",
    "        for i in range(1, num_blocks[branch_index]):\n",
    "            layers.append(block(self.num_inchannels[branch_index],\n",
    "                                num_channels[branch_index]))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n",
    "        branches = []\n",
    "\n",
    "        for i in range(num_branches):\n",
    "            branches.append(\n",
    "                self._make_one_branch(i, block, num_blocks, num_channels))\n",
    "\n",
    "        return nn.ModuleList(branches)\n",
    "\n",
    "    def _make_fuse_layers(self):\n",
    "        if self.num_branches == 1:\n",
    "            return None\n",
    "\n",
    "        num_branches = self.num_branches\n",
    "        num_inchannels = self.num_inchannels\n",
    "        fuse_layers = []\n",
    "        for i in range(num_branches if self.multi_scale_output else 1):\n",
    "            fuse_layer = []\n",
    "            for j in range(num_branches):\n",
    "                if j > i:\n",
    "                    fuse_layer.append(nn.Sequential(\n",
    "                        nn.Conv2d(num_inchannels[j],\n",
    "                                  num_inchannels[i],\n",
    "                                  1,\n",
    "                                  1,\n",
    "                                  0,\n",
    "                                  bias=False),\n",
    "                        BatchNorm2d(num_inchannels[i], momentum=BN_MOMENTUM)))\n",
    "                elif j == i:\n",
    "                    fuse_layer.append(None)\n",
    "                else:\n",
    "                    conv3x3s = []\n",
    "                    for k in range(i-j):\n",
    "                        if k == i - j - 1:\n",
    "                            num_outchannels_conv3x3 = num_inchannels[i]\n",
    "                            conv3x3s.append(nn.Sequential(\n",
    "                                nn.Conv2d(num_inchannels[j],\n",
    "                                          num_outchannels_conv3x3,\n",
    "                                          3, 2, 1, bias=False),\n",
    "                                BatchNorm2d(num_outchannels_conv3x3,\n",
    "                                            momentum=BN_MOMENTUM)))\n",
    "                        else:\n",
    "                            num_outchannels_conv3x3 = num_inchannels[j]\n",
    "                            conv3x3s.append(nn.Sequential(\n",
    "                                nn.Conv2d(num_inchannels[j],\n",
    "                                          num_outchannels_conv3x3,\n",
    "                                          3, 2, 1, bias=False),\n",
    "                                BatchNorm2d(num_outchannels_conv3x3,\n",
    "                                            momentum=BN_MOMENTUM),\n",
    "                                nn.ReLU(inplace=True)))\n",
    "                    fuse_layer.append(nn.Sequential(*conv3x3s))\n",
    "            fuse_layers.append(nn.ModuleList(fuse_layer))\n",
    "\n",
    "        return nn.ModuleList(fuse_layers)\n",
    "\n",
    "    def get_num_inchannels(self):\n",
    "        return self.num_inchannels\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.num_branches == 1:\n",
    "            return [self.branches[0](x[0])]\n",
    "\n",
    "        for i in range(self.num_branches):\n",
    "            x[i] = self.branches[i](x[i])\n",
    "\n",
    "        x_fuse = []\n",
    "        for i in range(len(self.fuse_layers)):\n",
    "            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n",
    "            for j in range(1, self.num_branches):\n",
    "                if i == j:\n",
    "                    y = y + x[j]\n",
    "                elif j > i:\n",
    "                    width_output = x[i].shape[-1]\n",
    "                    height_output = x[i].shape[-2]\n",
    "                    y = y + F.interpolate(\n",
    "                        self.fuse_layers[i][j](x[j]),\n",
    "                        size=(height_output, width_output),\n",
    "                        mode='bilinear',\n",
    "                        align_corners=False)\n",
    "                else:\n",
    "                    y = y + self.fuse_layers[i][j](x[j])\n",
    "            x_fuse.append(self.relu(y))\n",
    "\n",
    "        return x_fuse\n",
    "\n",
    "\n",
    "blocks_dict = {\n",
    "    'BASIC': BasicBlock,\n",
    "    'BOTTLENECK': Bottleneck\n",
    "}\n",
    "class Conv2dReLU(nn.Sequential):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            padding=0,\n",
    "            stride=1,\n",
    "            use_batchnorm=True,\n",
    "    ):\n",
    "\n",
    "        if use_batchnorm == \"inplace\" and InPlaceABN is None:\n",
    "            raise RuntimeError(\n",
    "                \"In order to use `use_batchnorm='inplace'` inplace_abn package must be installed. \"\n",
    "                + \"To install see: https://github.com/mapillary/inplace_abn\"\n",
    "            )\n",
    "\n",
    "        conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=not (use_batchnorm),\n",
    "        )\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        if use_batchnorm == \"inplace\":\n",
    "            bn = InPlaceABN(out_channels, activation=\"leaky_relu\", activation_param=0.0)\n",
    "            relu = nn.Identity()\n",
    "\n",
    "        elif use_batchnorm and use_batchnorm != \"inplace\":\n",
    "            bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        else:\n",
    "            bn = nn.Identity()\n",
    "\n",
    "        super(Conv2dReLU, self).__init__(conv, bn, relu)\n",
    "        \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            use_batchnorm=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2dReLU(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        self.conv2 = Conv2dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x        \n",
    "relu_inplace = True\n",
    "class HRNetV2(nn.Module):\n",
    "    def __init__(self, n_class, **kwargs):\n",
    "        super(HRNetV2, self).__init__()\n",
    "        extra = {\n",
    "            'STAGE1': {'NUM_MODULES': 1, 'NUM_BRANCHES': 1, 'BLOCK': 'BOTTLENECK', 'NUM_BLOCKS': (4), 'NUM_CHANNELS': (64), 'FUSE_METHOD': 'SUM'},\n",
    "            'STAGE2': {'NUM_MODULES': 1, 'NUM_BRANCHES': 2, 'BLOCK': 'BASIC', 'NUM_BLOCKS': (4, 4), 'NUM_CHANNELS': (48, 96), 'FUSE_METHOD': 'SUM'},\n",
    "            'STAGE3': {'NUM_MODULES': 4, 'NUM_BRANCHES': 3, 'BLOCK': 'BASIC', 'NUM_BLOCKS': (4, 4, 4), 'NUM_CHANNELS': (48, 96, 192), 'FUSE_METHOD': 'SUM'},\n",
    "            'STAGE4': {'NUM_MODULES': 3, 'NUM_BRANCHES': 4, 'BLOCK': 'BASIC', 'NUM_BLOCKS': (4, 4, 4, 4), 'NUM_CHANNELS': (48, 96, 192, 384), 'FUSE_METHOD': 'SUM'},\n",
    "            'FINAL_CONV_KERNEL': 1\n",
    "            }\n",
    "        ALIGN_CORNERS = False\n",
    "        relu_inplace = True\n",
    "        # stem net\n",
    "        # stem net\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn2 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        \n",
    "        self.stage1_cfg = extra['STAGE1']\n",
    "        num_channels = self.stage1_cfg['NUM_CHANNELS']\n",
    "        block = blocks_dict[self.stage1_cfg['BLOCK']]\n",
    "        num_blocks = self.stage1_cfg['NUM_BLOCKS']\n",
    "        self.layer1 = self._make_layer(block, 64, num_channels, num_blocks)\n",
    "        stage1_out_channel = block.expansion*num_channels\n",
    "        \n",
    "        \n",
    "        self.stage2_cfg = extra['STAGE2']\n",
    "        num_channels = self.stage2_cfg['NUM_CHANNELS']\n",
    "        block = blocks_dict[self.stage2_cfg['BLOCK']]\n",
    "        num_channels = [\n",
    "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
    "        self.transition1 = self._make_transition_layer(\n",
    "            [stage1_out_channel], num_channels)\n",
    "        self.stage2, pre_stage_channels = self._make_stage(\n",
    "            self.stage2_cfg, num_channels)\n",
    "\n",
    "        self.stage3_cfg = extra['STAGE3']\n",
    "        num_channels = self.stage3_cfg['NUM_CHANNELS']\n",
    "        block = blocks_dict[self.stage3_cfg['BLOCK']]\n",
    "        num_channels = [\n",
    "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
    "        self.transition2 = self._make_transition_layer(\n",
    "            pre_stage_channels, num_channels)\n",
    "        self.stage3, pre_stage_channels = self._make_stage(\n",
    "            self.stage3_cfg, num_channels)\n",
    "\n",
    "        self.stage4_cfg = extra['STAGE4']\n",
    "        num_channels = self.stage4_cfg['NUM_CHANNELS']\n",
    "        block = blocks_dict[self.stage4_cfg['BLOCK']]\n",
    "        num_channels = [\n",
    "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
    "        self.transition3 = self._make_transition_layer(\n",
    "            pre_stage_channels, num_channels)\n",
    "        self.stage4, pre_stage_channels = self._make_stage(\n",
    "            self.stage4_cfg, num_channels, multi_scale_output=True)\n",
    "        \n",
    "        last_inp_channels = np.int(np.sum(pre_stage_channels))\n",
    "        self.up1 = DecoderBlock(720, 512)\n",
    "        self.up2 = DecoderBlock(512, n_class)\n",
    "        \n",
    "        self.last_layer = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=last_inp_channels,\n",
    "                out_channels=last_inp_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0),\n",
    "            BatchNorm2d(last_inp_channels, momentum=BN_MOMENTUM),\n",
    "            nn.ReLU(inplace=relu_inplace),\n",
    "            nn.Conv2d(\n",
    "                in_channels=last_inp_channels,\n",
    "                out_channels=n_class,\n",
    "                kernel_size=extra['FINAL_CONV_KERNEL'],\n",
    "                stride=1,\n",
    "                padding=1 if extra['FINAL_CONV_KERNEL'] == 3 else 0)\n",
    "        )\n",
    "        \n",
    "    def _make_transition_layer(\n",
    "            self, num_channels_pre_layer, num_channels_cur_layer):\n",
    "        num_branches_cur = len(num_channels_cur_layer)\n",
    "        num_branches_pre = len(num_channels_pre_layer)\n",
    "\n",
    "        transition_layers = []\n",
    "        for i in range(num_branches_cur):\n",
    "            if i < num_branches_pre:\n",
    "                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n",
    "                    transition_layers.append(nn.Sequential(\n",
    "                        nn.Conv2d(num_channels_pre_layer[i],\n",
    "                                  num_channels_cur_layer[i],\n",
    "                                  3,\n",
    "                                  1,\n",
    "                                  1,\n",
    "                                  bias=False),\n",
    "                        BatchNorm2d(\n",
    "                            num_channels_cur_layer[i], momentum=BN_MOMENTUM),\n",
    "                        nn.ReLU(inplace=relu_inplace)))\n",
    "                else:\n",
    "                    transition_layers.append(None)\n",
    "            else:\n",
    "                conv3x3s = []\n",
    "                for j in range(i+1-num_branches_pre):\n",
    "                    inchannels = num_channels_pre_layer[-1]\n",
    "                    outchannels = num_channels_cur_layer[i] \\\n",
    "                        if j == i-num_branches_pre else inchannels\n",
    "                    conv3x3s.append(nn.Sequential(\n",
    "                        nn.Conv2d(\n",
    "                            inchannels, outchannels, 3, 2, 1, bias=False),\n",
    "                        BatchNorm2d(outchannels, momentum=BN_MOMENTUM),\n",
    "                        nn.ReLU(inplace=relu_inplace)))\n",
    "                transition_layers.append(nn.Sequential(*conv3x3s))\n",
    "\n",
    "        return nn.ModuleList(transition_layers)\n",
    "\n",
    "    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(inplanes, planes, stride, downsample))\n",
    "        inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_stage(self, layer_config, num_inchannels,\n",
    "                    multi_scale_output=True):\n",
    "        num_modules = layer_config['NUM_MODULES']\n",
    "        num_branches = layer_config['NUM_BRANCHES']\n",
    "        num_blocks = layer_config['NUM_BLOCKS']\n",
    "        num_channels = layer_config['NUM_CHANNELS']\n",
    "        block = blocks_dict[layer_config['BLOCK']]\n",
    "        fuse_method = layer_config['FUSE_METHOD']\n",
    "\n",
    "        modules = []\n",
    "        for i in range(num_modules):\n",
    "            # multi_scale_output is only used last module\n",
    "            if not multi_scale_output and i == num_modules - 1:\n",
    "                reset_multi_scale_output = False\n",
    "            else:\n",
    "                reset_multi_scale_output = True\n",
    "            modules.append(\n",
    "                HighResolutionModule(num_branches,\n",
    "                                      block,\n",
    "                                      num_blocks,\n",
    "                                      num_inchannels,\n",
    "                                      num_channels,\n",
    "                                      fuse_method,\n",
    "                                      reset_multi_scale_output)\n",
    "            )\n",
    "            num_inchannels = modules[-1].get_num_inchannels()\n",
    "\n",
    "        return nn.Sequential(*modules), num_inchannels\n",
    "\n",
    "    def forward(self, x, return_feature_maps=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "\n",
    "        x_list = []\n",
    "        for i in range(self.stage2_cfg['NUM_BRANCHES']):\n",
    "            if self.transition1[i] is not None:\n",
    "                x_list.append(self.transition1[i](x))\n",
    "            else:\n",
    "                x_list.append(x)\n",
    "        y_list = self.stage2(x_list)\n",
    "\n",
    "        x_list = []\n",
    "        for i in range(self.stage3_cfg['NUM_BRANCHES']):\n",
    "            if self.transition2[i] is not None:\n",
    "                if i < self.stage2_cfg['NUM_BRANCHES']:\n",
    "                    x_list.append(self.transition2[i](y_list[i]))\n",
    "                else:\n",
    "                    x_list.append(self.transition2[i](y_list[-1]))\n",
    "            else:\n",
    "                x_list.append(y_list[i])\n",
    "        y_list = self.stage3(x_list)\n",
    "\n",
    "        x_list = []\n",
    "        for i in range(self.stage4_cfg['NUM_BRANCHES']):\n",
    "            if self.transition3[i] is not None:\n",
    "                if i < self.stage3_cfg['NUM_BRANCHES']:\n",
    "                    x_list.append(self.transition3[i](y_list[i]))\n",
    "                else:\n",
    "                    x_list.append(self.transition3[i](y_list[-1]))\n",
    "            else:\n",
    "                x_list.append(y_list[i])\n",
    "        x = self.stage4(x_list)\n",
    "\n",
    "        # Upsampling\n",
    "        x0_h, x0_w = x[0].size(2), x[0].size(3)\n",
    "        x1 = F.interpolate(x[1], size=(x0_h, x0_w), mode='bilinear', align_corners=False)\n",
    "        x2 = F.interpolate(x[2], size=(x0_h, x0_w), mode='bilinear', align_corners=False)\n",
    "        x3 = F.interpolate(x[3], size=(x0_h, x0_w), mode='bilinear', align_corners=False)\n",
    "\n",
    "        x = torch.cat([x[0], x1, x2, x3], 1)\n",
    "\n",
    "        #x = self.up2(self.up1(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def hrnetv2(pretrained=False, **kwargs):\n",
    "    model = HRNetV2(n_class=2, **kwargs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e93e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "import collections\n",
    "import threading\n",
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "#model = hrnetv2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13df8596",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = eval(f'smp.create_model(arch=\"{\"FullAxialUnet\"}\", encoder_name=\"{\"resnet18\"}\", encoder_weights=\"imagenet\", in_channels={3}, classes = {2}, image_size = {256})').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48af99b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_in = torch.randn(2, 3, 256, 256).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8703cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(dummy_in).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1539c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77bcacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "li = [{'Id': 1, 'ParentId': 0, 'ClassificationSubtype': False, 'Label': 'Base ROI', 'Color': 'rgba(0, 0, 0, 0.0)', 'Tools': [{'Name': 'iam_base_roi', 'Parameters': {'invert_result': False, 'min_intensity': 28.2, 'smoothing': 23.55, 'minsize': 5, 'paramLabels': {'invert_result': 'Invert Base ROI', 'min_intensity': 'Minimum Intensity (%)', 'smoothing': 'Smoothing', 'minsize': 'Minimum Size (%)'}}}], 'SelectedToolName': None, 'Dynamic': False, 'HasChild': False, 'IsSubtype': False, 'SubtypeLevel': 0, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': [], 'DefaultSelected': False}, {'Id': 2, 'ParentId': 0, 'ClassificationSubtype': False, 'Label': 'Tumor Cells', 'Color': '#e6194b', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': True, 'IsSubtype': False, 'SubtypeLevel': 0, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 4, 'ParentId': 2, 'ClassificationSubtype': True, 'Label': 'Positive', 'Color': '#ffe119', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 1, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 3, 'ParentId': 2, 'ClassificationSubtype': True, 'Label': 'Negative', 'Color': '#3cb44b', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 1, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 5, 'ParentId': 0, 'ClassificationSubtype': False, 'Label': 'Immun Cells', 'Color': '#4363d8', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': True, 'IsSubtype': False, 'SubtypeLevel': 0, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 7, 'ParentId': 5, 'ClassificationSubtype': True, 'Label': 'Lymphozyten', 'Color': '#911eb4', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': True, 'IsSubtype': True, 'SubtypeLevel': 1, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 11, 'ParentId': 7, 'ClassificationSubtype': True, 'Label': 'Lymphozyten +', 'Color': '#fabebe', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 2, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 12, 'ParentId': 7, 'ClassificationSubtype': True, 'Label': 'Lymphozyten -', 'Color': '#008080', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 2, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 6, 'ParentId': 5, 'ClassificationSubtype': True, 'Label': 'Granulozyten', 'Color': '#f58231', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': True, 'IsSubtype': True, 'SubtypeLevel': 1,'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 14, 'ParentId': 6, 'ClassificationSubtype': True, 'Label': 'Granulozyten +', 'Color': '#9a6324', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 2, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 13, 'ParentId': 6, 'ClassificationSubtype': True, 'Label': 'Granulozyten -', 'Color': '#e6beff', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 2, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 10, 'ParentId': 5, 'ClassificationSubtype': True, 'Label': 'Dendritische Zellen', 'Color': '#bcf60c', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 1, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 9, 'ParentId': 5, 'ClassificationSubtype': True, 'Label': 'Makrophagen', 'Color': '#f032e6', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 1, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 8, 'ParentId': 5, 'ClassificationSubtype': True, 'Label': 'Plasmazellen', 'Color': '#46f0f0', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 1, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 15, 'ParentId': 0, 'ClassificationSubtype': False, 'Label': 'Grid', 'Color': '#00000000', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': False, 'SubtypeLevel': 0, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373d0a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in list(filter(lambda x: x['HasChild'], li)):\n",
    "    if layer['ParentId'] == idd:\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e1a149",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt, layer in enumerate(list(filter(lambda x: not x['HasChild'] and x['ParentId'] == selectedId, li))):\n",
    "    print(layer['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b0429",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedId"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7395f6d",
   "metadata": {},
   "source": [
    "# Paul Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf2f0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "fi = cv2.imread(r\"E:\\source\\repos\\VisualDL\\custom_experiments\\2.png\")\n",
    "se = cv2.imread(r\"E:\\source\\repos\\VisualDL\\custom_experiments\\3.png\")\n",
    "res = cv2.absdiff(fi, se)\n",
    "res = res.astype(np.uint8)\n",
    "#--- find percentage difference based on number of pixels that are not zero ---\n",
    "percentage = (np.count_nonzero(res) * 100)/ res.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343a8acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01da989",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = cv2.subtract(cv2.imread(r\"E:\\source\\repos\\VisualDL\\custom_experiments\\2.png\"), cv2.imread(r\"E:\\source\\repos\\VisualDL\\custom_experiments\\3.png\"))\n",
    "difference = cv2.cvtColor(difference, cv2.COLOR_BGR2GRAY)\n",
    "difference[difference>0]=255\n",
    "print(1 - (np.count_nonzero(difference) / (1920*1080)))\n",
    "\n",
    "cv2.imwrite(\"Xd.png\", difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a3aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(im1, im2):\n",
    "    res = cv2.absdiff(im1, im2)\n",
    "    res = res.astype(np.uint8)\n",
    "    return (np.count_nonzero(res) * 100)/ res.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbc1df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video = cv2.VideoCapture(r\"C:\\Users\\phili\\Downloads\\ALM scam.mp4\")\n",
    "frames_per_second = int(input_video.get(cv2.CAP_PROP_FPS))\n",
    "frame_width = int(input_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(input_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6ecd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcd687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "last_image = None\n",
    "threshold = 0.9991\n",
    "# Loop through all the frames in the video\n",
    "\n",
    "while 1:\n",
    "    # Read the video to retrieve individual frames. 'frame' will reference the inidivdual frames read from the video.\n",
    "    ret, frame = input_video.read()\n",
    "    if not ret:\n",
    "        print('Processed all frames')\n",
    "        break\n",
    "    if last_image is None:\n",
    "        last_image = frame\n",
    "        continue\n",
    "    \n",
    "    di = diff(frame, last_image)\n",
    "    last_image = frame\n",
    "    if di > threshold:\n",
    "        count += 1\n",
    "        cv2.imwrite(f\"{count}.png\", frame)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab14dc32",
   "metadata": {},
   "source": [
    "# Convert MakeSenseAi Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c798da6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "image_dir = r\"C:\\Users\\phili\\Videos\\Call of Duty  Modern Warfare 2019\\trainnig\"\n",
    "csv_file = r\"C:\\Users\\phili\\Videos\\Call of Duty  Modern Warfare 2019\\labels_my-project-name_2021-07-31-07-17-36.csv\"\n",
    "label_dir = \"labels\"\n",
    "with open(csv_file, \"r\", encoding = \"utf-8\") as handle:\n",
    "    lines = handle.readlines()\n",
    "files = {}\n",
    "for line in lines:\n",
    "    x_min, y_min, width, height = [int(x) for x in line.split(\",\")[1:5]]\n",
    "    image_width = int(line.split(\",\")[-2])\n",
    "    image_height = int(line.split(\",\")[-1])\n",
    "    name = line.split(\",\")[-3]\n",
    "    \n",
    "    middle_x = (x_min + width / 2) / image_width\n",
    "    middle_y = (y_min + height / 2) / image_height\n",
    "    width /= image_width\n",
    "    height /= image_height\n",
    "    if not name in files:\n",
    "        files[name] = [f\"0 {middle_x} {middle_y} {width} {height}\\n\"]\n",
    "    else:\n",
    "        files[name].append(f\"0 {middle_x} {middle_y} {width} {height}\\n\")\n",
    "all_images = os.listdir(image_dir)   \n",
    "no_annotations = list(set(all_images) - set(files.keys()))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bdc6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in files.items():\n",
    "    with open(os.path.join(label_dir, key).replace(\".png\", \".txt\"), \"w\") as handle:\n",
    "        for it in val:\n",
    "            handle.write(it)\n",
    "for file in no_annotations:\n",
    "    with open(os.path.join(label_dir, file).replace(\".png\", \".txt\"), \"w\") as handle:\n",
    "        handle.write(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103986cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = r\"F:\\source\\repos\\Daten\\HER-N\\Pdl1Combined\\Tumor Cells 512\\train\\labels\"\n",
    "co = set()\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "for file in os.listdir(start):\n",
    "    im = cv2.imread(os.path.join(start, file), 0)\n",
    "    un = np.unique(im)\n",
    "    for u in un:\n",
    "        co.add(u)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26736ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56eb57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.load(r\"F:\\source\\repos\\VisualDL\\tu-resnest200e, UnetPlusPlus.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00556bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32024869",
   "metadata": {},
   "outputs": [],
   "source": [
    "t['custom_data'] = {\"structure_indices\" : [2], \"image_size\": 1024, \"object_based\" : False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db69afd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t['custom_data']['structure_indices'] = [4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f60a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "                    'epoch': t['epoch'],\n",
    "                    'model_state_dict': t['model_state_dict'],\n",
    "                    'optimizer_state_dict': t['optimizer_state_dict'],\n",
    "                    'model':t['model'],\n",
    "                    'best_metric':t['best_metric'],\n",
    "                    'custom_data': t['custom_data']}, os.path.join(\"001.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a37ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "t['custom_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f57412",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.load(r\"001.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baa5282",
   "metadata": {},
   "outputs": [],
   "source": [
    "t['custom_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d953a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7421ceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Test\\Cells128\\train\\labels\"\n",
    "for im in os.listdir(start):\n",
    "    img = io.imread(os.path.join(start, im), as_gray = True).astype(np.float32)\n",
    "    print(np.unique(img))\n",
    "\n",
    "    cv2.imwrite(os.path.join(start, im), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6a5f63",
   "metadata": {},
   "source": [
    "# Single class per contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a497d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45b8e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06bff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(r\"F:\\source\\repos\\Daten\\Nuclei\\Nuclei\\Nuclei\\val\\images\\06_PAS_1_8911_5853.png\", 0)\n",
    "orig = img.copy()\n",
    "img[img > 0] = 255\n",
    "\n",
    "contours, hierarchy = cv2.findContours(image=img, mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE)\n",
    "for i, cnt in enumerate(contours):        \n",
    "    area = cv2.contourArea(cnt)         \n",
    "    #if area > threshold_area:\n",
    "    \n",
    "    mask = np.zeros_like(orig)\n",
    "    cv2.drawContours(mask, [cnt], -1, 255, -1)\n",
    "    cv2.imwrite(f\"dsa{i}dsa.png\",mask)\n",
    "\n",
    "    pts = np.where(mask == 255)\n",
    "    vals = np.unique(orig[pts[0], pts[1]], return_counts = True)\n",
    "  \n",
    "    orig[pts[0], pts[1]] = vals[0][np.argmax(vals[1])]\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d862714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"orig.png\", orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd6a94f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b90313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualdl import vdl\n",
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.cvtColor(cv2.imread(r\"F:\\source\\repos\\Daten\\Nuclei\\Nuclei\\Nuclei\\val\\images\\06_PAS_1_8911_5853.png\"), cv2.COLOR_BGR2RGB)\n",
    "img = cv2.resize(img, (512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2098ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = vdl.get_inference_model(r\"C:\\Users\\phili\\Documents\\002.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f80d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pred.predict([img], True, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e4ccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"preds1.png\", preds[0]*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dd804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddaf80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6b02d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25350e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337e61db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path=r\"F:\\source\\repos\\hsayolo\\runs\\train\\exp5\\weights\\best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b0c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from visualdl import vdl\n",
    "imgs = [cv2.imread(r\"F:\\source\\repos\\Daten\\ObjectDetection\\Her1\\train\\images\\PD-L1=2_0_41328-42312_75.png\")[..., ::-1]]\n",
    "model = vdl.get_inference_model(r\"F:\\source\\repos\\hsayolo\\runs\\train\\exp5\\weights\\best.pt\", type=\"od\")\n",
    "print(model.predict(imgs))\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8e5065",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(imgs, size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe245c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds.xyxy[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d600392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(weightpath, imgs, confidence = 0.45):\n",
    "    model = torch.hub.load('ultralytics/yolov5', 'custom', path=weightpath, force_reload=True)\n",
    "    size = imgs[0].shape[0]\n",
    "    model.conf = confidence\n",
    "    preds = model(imgs, size=size)\n",
    "    finals = []\n",
    "    for cnt, img in enumerate(imgs):\n",
    "        tmp = []\n",
    "        boxes = preds.xyxy[cnt]\n",
    "        for box in boxes:\n",
    "            middlex = int(box[0] + (box[2] - box[0]) / 2)\n",
    "            middley = int(box[1] + (box[3] - box[1]) / 2)\n",
    "            data = list(box.detach().cpu().numpy())\n",
    "            data.append((middlex, middley))\n",
    "            tmp.append(tuple(data))\n",
    "        finals.append(tmp)\n",
    "    return finals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83841932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "imgs = [cv2.imread(r\"F:\\source\\repos\\Daten\\ObjectDetection\\Her1\\train\\images\\PD-L1=2_0_41328-42312_75.png\")[..., ::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edaa91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualdl import vdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1031513",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = vdl.get_inference_model(r\"F:\\source\\repos\\hsayolo\\runs\\train\\exp5\\weights\\001.pt\", type=\"od\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5569eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vdl.get_inference_model(r\"F:\\source\\repos\\VisualDL\\tu-resnet18, UnetPlusPlus.pt\", type=\"segmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34538d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.predict(imgs, confidence = 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8caceb5",
   "metadata": {},
   "source": [
    "# Create valid folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec111f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from shutil import copyfile\n",
    "from random import shuffle\n",
    "#copyfile(src, dst)\n",
    "folder = r\"F:\\source\\repos\\Daten\\knochenpartikel\\knochenpartikel\"\n",
    "to = r\"F:\\source\\repos\\Daten\\knochenpartikel\\knochenpartikel\\valid\"\n",
    "all_files = os.listdir(os.path.join(folder, \"images\"))\n",
    "shuffle(all_files)\n",
    "le = int(len(all_files) * 0.1)\n",
    "all_images = all_images[:le]\n",
    "for file in all_images:\n",
    "    image = os.path.join(folder, \"images\")\n",
    "    label = os.path.join(folder, \"labels\")\n",
    "    image = os.path.join(image, file)\n",
    "    label = os.path.join(label, file)\n",
    "    #print(os.path.join(os.path.join(to, \"images\"), image))\n",
    "    #print(os.path.join(os.path.join(to, \"images\"), file))\n",
    "    copyfile(image, os.path.join(os.path.join(to, \"images\"), file))\n",
    "    copyfile(label, os.path.join(os.path.join(to, \"labels\"), file))\n",
    "    os.remove(image)\n",
    "    os.remove(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b73211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5705c249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad687010",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load(r\"F:\\source\\repos\\VisualDL\\tu-resnest50d, UnetPlusPlus.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ec7eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"validation_metrics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8610c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dog:\n",
    "    def __init__(self):\n",
    "        s = \"s\"\n",
    "        self.a = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1557a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Dog()\n",
    "for i in range(5):\n",
    "    a.a.append(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5221a357",
   "metadata": {},
   "outputs": [],
   "source": [
    "del a.a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afa6881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2799df",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np.delete(np.array(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dbbbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21afa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = cv2.imread(r\"F:\\source\\repos\\VisualDL\\0a.png\",0)\n",
    "a = cv2.distanceTransform(a, cv2.DIST_L2, 5)\n",
    "a = cv2.normalize(a, a, 0, 1.0, cv2.NORM_MINMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161200cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd.png\", a * 255.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bb00ba",
   "metadata": {},
   "source": [
    "# Create distance map for each object 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459b329",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells her2 128\\train\\labels\\05__1_3120-9167_15.png\", 0) *255.\n",
    "to = np.zeros_like(img, dtype=np.float32)\n",
    "img = img.astype(np.uint8)\n",
    "orig = img.copy()\n",
    "img[img > 0] = 255\n",
    "contours, hierarchy = cv2.findContours(image=img, mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE)\n",
    "for i, cnt in enumerate(contours):        \n",
    "    area = cv2.contourArea(cnt) \n",
    "    mask = np.zeros_like(orig)\n",
    "    cv2.drawContours(mask, [cnt], -1, 255, -1)\n",
    "    dist = cv2.distanceTransform(mask, cv2.DIST_L2, 5)\n",
    "    ab = cv2.normalize(dist, dist, 0, 1.0, cv2.NORM_MINMAX)\n",
    "    ab[ab < 0.7] = 0\n",
    "    pts = np.where(ab > 0)\n",
    "    to[pts[0], pts[1]] = ab[pts[0], pts[1]]\n",
    "to = to*255.\n",
    "cv2.imwrite(\"xd.png\", to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3030dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4390240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file = r\"../visualdl/dependencies/yolov5/data/coco128.yaml\"\n",
    "with open(yaml_file, \"r\",  encoding = \"utf-8\") as handle:\n",
    "    a = yaml.load(handle, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cf1d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a['train'] = r\"F:\\source\\repos\\Daten\\ObjectDetection\\tumor\\train\"\n",
    "a['val'] = r\"F:\\source\\repos\\Daten\\ObjectDetection\\tumor\\valid\"\n",
    "a['names'] = [\"xd\", \"bc\", \"bd\", \"ab\"]\n",
    "a['nc'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62754097",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"../visualdl/dependencies/yolov5/data/hsa.yaml\", 'w') as yaml_file:\n",
    "    yaml.dump(a, yaml_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93be3a12",
   "metadata": {},
   "source": [
    "# Test parse argmeunts from code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843c4635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('img_size', type=int, default = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f770cf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430dc33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abc(opt):\n",
    "    print(opt.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82767518",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc(parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63660a47",
   "metadata": {},
   "source": [
    "# Bring mask into yolo format on the fly\n",
    " The starting point for this are the labels that are used to extract the contours and finally the bounding boxes of those.\n",
    " After that they only have to be put in the right format for yolo and saved in an appropiate folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8fd0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe40cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "train_folder = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells her2 256\\train\"\n",
    "valid_folder = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells her2 256\\valid\"\n",
    "test_folder = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells her2 256\\valid\"\n",
    "def create_files(folder, nc = 1, single_class = False):\n",
    "    start = os.path.join(folder, \"labels\")\n",
    "    all_files = os.listdir(start)\n",
    "    if single_class:\n",
    "        nc = 1\n",
    "    for img in os.listdir(start):\n",
    "        for i in range(0, nc):\n",
    "            im = cv2.imread(os.path.join(start, img), 0)\n",
    "            #kernel = np.ones((2, 2), np.uint8)\n",
    "            #im = cv.erode(im, kernel)\n",
    "            #im = cv.dilate(im, kernel)\n",
    "            tmp = im.copy()\n",
    "            if not single_class:\n",
    "                tmp[tmp != (i+1)] = 0\n",
    "                tmp[tmp == (i+1)] = 255\n",
    "            else:\n",
    "                tmp[tmp > 0] = 255\n",
    "            contours,hierachy = cv2.findContours(tmp, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            blank = np.zeros_like(tmp)\n",
    "            for cnt, cont in enumerate(contours):\n",
    "                xmin,ymin,width,height = cv2.boundingRect(cont)\n",
    "                #cv.rectangle(im,(x,y),(x+width,y+height),(255),1)\n",
    "                #cv2.imwrite(\"xd.png\", im)\n",
    "                image_width = im.shape[0]\n",
    "                xcenter, ycenter = xmin + width/2, ymin + height/2\n",
    "                xcenter, ycenter, width, height = xcenter/image_width, ycenter/image_width, width/image_width, height/image_width\n",
    "                if not img in files:\n",
    "                    files[img] = [(str(i),str(xcenter), str(ycenter), str(width), str(height))]\n",
    "                else:\n",
    "                    files[img] += [(str(i),str(xcenter), str(ycenter), str(width), str(height))]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9df1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_files(train_folder, 4 , False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06740a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"labels\")\n",
    "for cnt, (key, val) in enumerate(files.items()):\n",
    "    with open(\"labels/\" + key.replace(\".png\", \".txt\"), \"w\") as handle:\n",
    "        for va in val:\n",
    "            handle.write(\" \".join(list(va))+ \"\\n\") \n",
    "            \n",
    "for name in no_anno:\n",
    "    with open(\"labels/\" + name.replace(\".png\", \".txt\"), \"w\") as handle:\n",
    "        handle.write(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6994bb9d",
   "metadata": {},
   "source": [
    "# Test ob inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4ba4f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualdl import vdl \n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage import morphology\n",
    "from scipy import ndimage as ndi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a403afd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\phili/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2021-9-25 torch 1.9.0 CUDA:0 (NVIDIA GeForce RTX 3090, 24576.0MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 308 layers, 21037638 parameters, 0 gradients, 50.3 GFLOPs\n",
      "Adding AutoShape... \n",
      "Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-resnest/resnest50-528c19ca.pth)\n"
     ]
    }
   ],
   "source": [
    "inf = vdl.get_inference_model(r\"F:\\source\\repos\\VisualDL\\runs\\train\\exp\\weights\\best.pt\",type = \"od\")\n",
    "inf2 = vdl.get_inference_model(r\"F:\\source\\repos\\VisualDL\\tu-resnest50d, UnetPlusPlus.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5522a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "xddd = torch.load(r\"F:\\source\\repos\\VisualDL\\tu-resnest50d, UnetPlusPlus.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2f2251f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.898529052734375,\n",
       " 'IoU': 0.8076459765434265,\n",
       " 'train_loss': 0.3860720545053482}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xddd['train_metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "06e58b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = cv2.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells her2 256\\valid\\images\\05__1_7372-10273_190.png\")#[:,:,::-1]\n",
    "orig = rgb.copy()\n",
    "box_image = np.zeros_like(rgb)\n",
    "box_image = cv2.cvtColor(box_image, cv2.COLOR_BGR2GRAY)\n",
    "boxes = inf.predict([rgb[:,:,::-1]], confidence = 0.45)[0]\n",
    "maps = inf2.predict([rgb[:,:,::-1]])[0] *255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1c015de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = np.uint8(np.zeros_like(maps))\n",
    "p = 1\n",
    "for box in boxes:\n",
    "    label_map = cv2.circle(label_map, box[-1], 1, p, -1)\n",
    "    p += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "75baee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapss = np.uint8(ndi.binary_fill_holes(maps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "cb88315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = ndi.distance_transform_edt(maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9cf89a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = watershed(-distance, label_map, mask=mapss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5cd0d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdd = cv2.add(np.uint8(labels), np.uint8(label_map * 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ddb22551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66]),\n",
       " array([28044,   879,   805,   830,   771,   457,   631,   425,   803,  1447,   765,   781,   428,   380,   638,   569,   967,   503,  1494,   343,  1589,   391,   374,  1691,   252,   690,   578,   630,   564,   465,   393,   459,  1401,   703,   529,  1110,   644,   485,   331,    97,   466,   316,   276,   205,\n",
       "          490,     7,     7,  1342,  1179,     6,   376,   984,   369,   604,   111,   595,   474,    94,   430,     2,   452,  1023,    13,   175,     5,   199], dtype=int64))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b431128c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(\"xd1.png\", orig)\n",
    "cv2.imwrite(\"xd2.png\", xdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cce3a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "from skimage.segmentation import watershed\n",
    "from skimage.feature import peak_local_max\n",
    "\n",
    "\n",
    "# Generate an initial image with two overlapping circles\n",
    "x, y = np.indices((80, 80))\n",
    "x1, y1, x2, y2 = 28, 28, 44, 52\n",
    "r1, r2 = 16, 20\n",
    "mask_circle1 = (x - x1)**2 + (y - y1)**2 < r1**2\n",
    "mask_circle2 = (x - x2)**2 + (y - y2)**2 < r2**2\n",
    "image = np.logical_or(mask_circle1, mask_circle2)\n",
    "\n",
    "# Now we want to separate the two objects in image\n",
    "# Generate the markers as local maxima of the distance to the background\n",
    "distance = ndi.distance_transform_edt(image)\n",
    "coords = peak_local_max(distance, footprint=np.ones((3, 3)), labels=image)\n",
    "mask = np.zeros(distance.shape, dtype=bool)\n",
    "mask[tuple(coords.T)] = True\n",
    "markers, _ = ndi.label(mask)\n",
    "labels = watershed(-distance, markers, mask=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e984ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22dfc87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fe9c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ce88d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea22dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705871d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = np.uint8(maps[0] * 255)\n",
    "#kernel = np.ones((2,2),np.uint8)\n",
    "#sure_bg = cv2.dilate(maps,kernel,iterations=3)\n",
    "sure_bg = maps\n",
    "\n",
    "sure_fg = np.uint8(maps)\n",
    "unknown = cv2.subtract(sure_bg,sure_fg)\n",
    "ret, markers = cv2.connectedComponents(sure_fg)\n",
    "markers = box_image\n",
    "# Add one to all labels so that sure background is not 0, but 1\n",
    "markers = markers+1\n",
    "markers[unknown==255] = 0\n",
    "markers = cv2.watershed(orig,markers)\n",
    "orig[markers == -1] = [255,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eec82c4",
   "metadata": {},
   "source": [
    "# watershedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97e85272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "from skimage.segmentation import watershed\n",
    "from skimage.feature import peak_local_max\n",
    "\n",
    "\n",
    "# Generate an initial image with two overlapping circles\n",
    "x, y = np.indices((80, 80))\n",
    "x1, y1, x2, y2 = 28, 28, 44, 52\n",
    "r1, r2 = 16, 20\n",
    "mask_circle1 = (x - x1)**2 + (y - y1)**2 < r1**2\n",
    "mask_circle2 = (x - x2)**2 + (y - y2)**2 < r2**2\n",
    "image = np.logical_or(mask_circle1, mask_circle2)\n",
    "\n",
    "# Now we want to separate the two objects in image\n",
    "# Generate the markers as local maxima of the distance to the background\n",
    "distance = ndi.distance_transform_edt(image)\n",
    "coords = peak_local_max(distance, footprint=np.ones((3, 3)), labels=image)\n",
    "mask = np.zeros(distance.shape, dtype=bool)\n",
    "mask[tuple(coords.T)] = True\n",
    "markers, _ = ndi.label(mask)\n",
    "labels = watershed(-distance, markers, mask=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b5c7ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-37-6561edf06158>:15: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x216 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(9, 3), sharex=True, sharey=True)\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(image, cmap=plt.cm.gray)\n",
    "ax[0].set_title('Overlapping objects')\n",
    "ax[1].imshow(-distance, cmap=plt.cm.gray)\n",
    "ax[1].set_title('Distances')\n",
    "ax[2].imshow(labels, cmap=plt.cm.nipy_spectral)\n",
    "ax[2].set_title('Separated objects')\n",
    "\n",
    "for a in ax:\n",
    "    a.set_axis_off()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f3a40d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.7977e+308,           0,           1,      1.4142,           2,      2.2361,      2.8284,           3,      3.1623,      3.6056,           4,      4.1231,      4.2426,      4.4721,           5,       5.099,      5.3852,      5.6569,       5.831,           6,      6.0828,      6.3246,      6.4031,      6.7082,\n",
       "                  7,      7.0711,      7.2111,      7.2801,      7.6158,      7.8102,           8,      8.0623,      8.2462,      8.4853,       8.544,      8.6023,      8.9443,           9,      9.0554,      9.2195,       9.434,      9.4868,      9.8489,      9.8995,          10,       10.05,      10.198,      10.296,\n",
       "              10.44,       10.63,       10.77,      10.817,          11,      11.045,       11.18,      11.314,      11.402,      11.662,      11.705,          12,      12.042,      12.083,      12.166,      12.207,      12.369,       12.53,      12.649,      12.806,          13,      13.038,      13.153,      13.342,\n",
       "             13.416,      13.601,      13.892,      13.928,          14,      14.036,      14.142,      14.213,      14.318,      14.422,       14.56,      14.765,      14.866,          15,      15.033,      15.133,      15.232,      15.297,      15.524,       15.62,      15.811,          16,      16.031,      16.125,\n",
       "             16.279,      16.401,      16.492,      16.763,          17,      17.029,      17.117,      17.205,       17.72,      17.804,          18,      18.028,      18.601,      18.974,          19,          20]),\n",
       " array([1071, 3391,  161,   57,  100,   53,   57,   57,   35,   53,   57,   35,   14,   35,   97,    5,   35,   14,   35,   44,    5,   32,   13,   35,   44,   19,   22,    5,   32,   13,   44,   27,    5,   14,   17,   10,   32,   32,    4,   18,   22,    5,   29,    2,   42,    4,    5,   19,    5,   13,   17,   10,\n",
       "          32,    4,   21,    2,   13,   17,    3,   32,   13,    3,    4,    6,    3,   14,    3,    6,   35,    5,    2,    3,    9,    9,   12,    3,   16,    2,    2,    6,    3,    3,    3,    9,    1,   21,    1,    2,    1,    2,    1,    6,    4,   12,    1,    1,    1,    6,    1,    1,   11,    1,    1,    3,\n",
       "           1,    6,    5,    1,    3,    1,    4,    1], dtype=int64))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(distance, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97dd91d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[          0,           0,           0, ...,           0,           0,           0],\n",
       "       [          0,           0,           0, ...,           0,           0,           0],\n",
       "       [          0,           0,           0, ...,           0,           0,           0],\n",
       "       ...,\n",
       "       [          0,           0,           0, ...,           0,           0,           0],\n",
       "       [          0,           0,           0, ...,           0,           0,           0],\n",
       "       [          0,           0,           0, ...,           0,           0,           0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f634f6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
