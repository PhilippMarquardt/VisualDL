{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for Segmentation models trained using VDL\n",
    "\n",
    "### General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Selection of Graphical Processing Unit (GPU) for training\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\n",
    "os.environ[\n",
    "    \"CUDA_VISIBLE_DEVICES\"\n",
    "] = \"0\"  # Only nVidia GPUs are counted, not integrated GPUs\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "from visualdl import vdl\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Paths here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = vdl.get_inference_model(\n",
    "    r\"1.5 dataset Hyphe_seg_Hyphe_i-fi_-1_rgb_tu-resnest50d, Unet.pt\"\n",
    ")\n",
    "validation_images_path = r\"1.5 dataset Hyphe_seg_Hyphe_i-fi_-1_rgb\\valid\\images\"\n",
    "validation_labels_path = r\"1.5 dataset Hyphe_seg_Hyphe_i-fi_-1_rgb\\valid\\labels\"\n",
    "metrics_output_dir = r\"1.5 dataset Hyphe_seg_Hyphe_i-fi_-1_rgb\\metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(metrics_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create metrics with visualdl models segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_inference(model, image):\n",
    "    \"\"\"Resize image to 1024x1024 and run inference\"\"\"\n",
    "    # Resize image to 1024x1024\n",
    "    resized_image = cv2.resize(image, (2048, 2048))\n",
    "\n",
    "    # Get prediction\n",
    "    prediction = model.predict([resized_image], single_class_per_contour=False)\n",
    "    prediction = prediction[0][0]\n",
    "    # Resize prediction back to original size\n",
    "    original_h, original_w = image.shape[:2]\n",
    "    final_pred = cv2.resize(\n",
    "        prediction.astype(np.uint8),\n",
    "        (original_w, original_h),\n",
    "        interpolation=cv2.INTER_NEAREST,\n",
    "    )\n",
    "\n",
    "    return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_overlay(original_img, mask, alpha=0.5, color=[255, 0, 0]):\n",
    "    \"\"\"Create an overlay of the mask on the original image\"\"\"\n",
    "    overlay = np.zeros_like(original_img)\n",
    "    overlay[mask > 0] = color  # Apply the specified color for positive regions\n",
    "    return cv2.addWeighted(original_img, 1 - alpha, overlay, alpha, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(pred, target):\n",
    "    \"\"\"Calculate Intersection over Union\"\"\"\n",
    "    intersection = np.logical_and(pred, target).sum()\n",
    "    union = np.logical_or(pred, target).sum()\n",
    "    return intersection / (union + 1e-6)  # Add small epsilon to avoid division by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, output_path):\n",
    "    \"\"\"Plot confusion matrix using seaborn\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate all metrics and return as dictionary\"\"\"\n",
    "    # Convert to binary (0 or 1)\n",
    "    y_true_bin = y_true > 0\n",
    "    y_pred_bin = y_pred > 0\n",
    "\n",
    "    # Flatten arrays\n",
    "    y_true_flat = y_true_bin.flatten()\n",
    "    y_pred_flat = y_pred_bin.flatten()\n",
    "\n",
    "    # Calculate metrics\n",
    "    cm = confusion_matrix(y_true_flat, y_pred_flat)\n",
    "    iou = calculate_iou(y_pred_bin, y_true_bin)\n",
    "    precision = precision_score(y_true_flat, y_pred_flat)\n",
    "    recall = recall_score(y_true_flat, y_pred_flat)\n",
    "    f1 = f1_score(y_true_flat, y_pred_flat)\n",
    "\n",
    "    # Calculate additional metrics from confusion matrix\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    specificity = tn / (tn + fp + 1e-6)\n",
    "\n",
    "    return {\n",
    "        \"IoU\": iou,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Specificity\": specificity,\n",
    "        \"Confusion Matrix\": cm,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize aggregated metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = []\n",
    "\n",
    "# Create metric output files\n",
    "metrics_file = os.path.join(metrics_output_dir, \"metrics.txt\")\n",
    "aggregate_metrics_file = os.path.join(metrics_output_dir, \"aggregate_metrics.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:40<00:00,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Check the output directory for results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm(os.listdir(validation_images_path)):\n",
    "    image_path = os.path.join(validation_images_path, file)\n",
    "    label_path = os.path.join(validation_labels_path, file)\n",
    "\n",
    "    img = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "    orig_img = cv2.imread(image_path)\n",
    "\n",
    "    # Use simple inference instead of sliding window\n",
    "    preds = simple_inference(inference_model, img)\n",
    "\n",
    "    label = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    metrics = calculate_metrics(label, preds)\n",
    "    all_metrics.append(metrics)\n",
    "\n",
    "    plot_confusion_matrix(\n",
    "        metrics[\"Confusion Matrix\"],\n",
    "        os.path.join(\n",
    "            metrics_output_dir, f\"confusion_matrix_{os.path.splitext(file)[0]}.png\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    with open(metrics_file, \"a\") as f:\n",
    "        f.write(f\"\\nMetrics for {file}:\\n\")\n",
    "        for metric_name, value in metrics.items():\n",
    "            if metric_name != \"Confusion Matrix\":\n",
    "                f.write(f\"{metric_name}: {value:.4f}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "    pred_overlay = create_overlay(\n",
    "        orig_img.copy(), preds > 0, alpha=0.25, color=[255, 0, 0]\n",
    "    )\n",
    "    truth_overlay = create_overlay(\n",
    "        orig_img.copy(), label > 0, alpha=0.25, color=[0, 255, 0]\n",
    "    )\n",
    "\n",
    "    preds_colored = cv2.cvtColor(preds.astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "    label_colored = cv2.cvtColor(label.astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    combined_img = cv2.hconcat(\n",
    "        [preds_colored, label_colored, pred_overlay, truth_overlay]\n",
    "    )\n",
    "    cv2.imwrite(os.path.join(metrics_output_dir, file), combined_img)\n",
    "\n",
    "with open(aggregate_metrics_file, \"w\") as f:\n",
    "    f.write(\"Aggregate Metrics (Mean ± Std):\\n\")\n",
    "    for metric in [\"IoU\", \"Precision\", \"Recall\", \"F1-Score\", \"Accuracy\", \"Specificity\"]:\n",
    "        values = [m[metric] for m in all_metrics]\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        f.write(f\"{metric}: {mean_val:.4f} ± {std_val:.4f}\\n\")\n",
    "\n",
    "print(\"Processing complete. Check the output directory for results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract information from model\n",
    "#### print all custom data information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_model.state[\"custom_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
