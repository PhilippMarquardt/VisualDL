{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa983342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import timm\n",
    "import albumentations as A\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea980cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "timm.list_models(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d33b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.Resize(width=256, height=256)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0195a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(encoder_name = \"timm-resnest50d\", classes = 2, in_channels = 3)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0a49e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\timm-resnest50d, Unet.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a295f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells\\Cells\\test\\images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db05a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt, im in enumerate(os.listdir(image_dir)):\n",
    "    im = io.imread(os.path.join(image_dir, im))\n",
    "    im = im/255.\n",
    "    im = transform(image = im)[\"image\"]\n",
    "    im = torch.tensor(im, dtype = torch.float).permute(2, 0, 1).unsqueeze(0)\n",
    "    print(im.shape)\n",
    "    pred = model(im)\n",
    "    pred = torch.argmax(pred, 1)[0] * 255.\n",
    "    cv2.imwrite(str(cnt) + \".png\", pred.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390b4a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = cv2.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells1\\Cells\\train\\labels\\05__1_4685_10225.png\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c9ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = ii * 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a1d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"test.png\", ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187c5777",
   "metadata": {},
   "source": [
    "# Replace 2D with 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b75530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e72369",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, modules in model.named_modules():\n",
    "    for module in modules:\n",
    "        print(module)\n",
    "    if(isinstance(module, nn.Conv2d)):\n",
    "        kernel_size = module.kernel_size[0]\n",
    "        stride = module.stride[0]\n",
    "        padding = module.padding[0]\n",
    "        weight = module.weight.unsqueeze(2) / kernel_size\n",
    "        weight = torch.cat([weight for _ in range(0, kernel_size)], dim=2)\n",
    "        bias = module.bias\n",
    "\n",
    "        if(bias is None):\n",
    "            print(modules)\n",
    "            print(modules[name])\n",
    "            modules[name] = nn.Conv3d(in_channels=module.weight.shape[1], out_channels=module.weight.shape[0],\n",
    "                               kernel_size=kernel_size, padding=padding, stride=stride, bias=False)\n",
    "        else:\n",
    "            modules[name] = nn.Conv3d(in_channels=module.weight.shape[1], out_channels=module.weight.shape[0],\n",
    "                               kernel_size=kernel_size, padding=padding, stride=stride, bias=True)\n",
    "            modules[name].bias = bias\n",
    "\n",
    "            modules[name].weight.data = weight\n",
    "\n",
    "    elif(isinstance(module, nn.BatchNorm2d)):\n",
    "        weight = module.weight\n",
    "        bias = module.bias\n",
    "        modules[name] = nn.BatchNorm3d(weight.shape[0])\n",
    "        modules[name].weight = weight\n",
    "        modules[name].bias = bias\n",
    "\n",
    "for name in modules:\n",
    "    parent_module = model\n",
    "    objs = name.split(\".\")\n",
    "    if len(objs) == 1:\n",
    "        model.__setattr__(name, modules[name])\n",
    "        continue\n",
    "\n",
    "    for obj in objs[:-1]:\n",
    "        parent_module = parent_module.__getattr__(obj)\n",
    "\n",
    "    parent_module.__setattr__(objs[-1], modules[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609929d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_modules(module):\n",
    "    for a in module:\n",
    "        get_all_modules(a.children())\n",
    "        if isinstance(a, nn.Conv2d):\n",
    "            print(a)\n",
    "            #a = nn.Conv3d(3,32,3)\n",
    "            \n",
    "        elif isinstance(a, nn.BatchNorm2d):\n",
    "            print(a)\n",
    "\n",
    "            \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2654f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_modules(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5713df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in model.encoder.named_modules():\n",
    "    print(a)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc30e742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from uformer_pytorch import Uformer\n",
    "\n",
    "model = Uformer(\n",
    "    dim = 16,           # initial dimensions after input projection, which increases by 2x each stage\n",
    "    stages = 4,         # number of stages\n",
    "    num_blocks = 2,     # number of transformer blocks per stage\n",
    "    window_size = 16,   # set window size (along one side) for which to do the attention within\n",
    "    dim_head = 64,\n",
    "    heads = 1,\n",
    "    ff_mult = 4\n",
    ")\n",
    "model.cuda()\n",
    "x = torch.randn(1, 3, 512, 512).cuda()\n",
    "pred = model(x) # (1, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb84406",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dffd467",
   "metadata": {},
   "source": [
    "# Segmentation inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f1c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells\\train\\labels\"\n",
    "to = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells\\train\\rabs\"\n",
    "for im in os.listdir(start):\n",
    "    ii = cv2.imread(os.path.join(start, im),0)\n",
    "    print(np.max(ii))\n",
    "    #ii = ii*255.\n",
    "    #cv2.imwrite(os.path.join(to, im), ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d91f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.UnetPlusPlus(encoder_name = \"timm-regnety_064\", classes = 2, in_channels = 3)\n",
    "model.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\timm-regnety_064, UnetPlusPlus.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d196607",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4547c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = r\"E:\\source\\repos\\HSAData\\Evican\\train\\images\"\n",
    "transform = A.Compose([\n",
    "    A.Resize(width=512, height=512)\n",
    "])\n",
    "for cnt, im in enumerate(os.listdir(test)):\n",
    "    image = io.imread(os.path.join(test, im))\n",
    "    image = image/255.\n",
    "    image = transform(image = image)[\"image\"]\n",
    "    s = torch.unsqueeze(torch.tensor(image, dtype = torch.float).permute(2, 0, 1), 0).cuda()\n",
    "    \n",
    "    preds = model(s)\n",
    "    preds = torch.argmax(preds, 1)\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    io.imsave(f\"{cnt}.png\", preds[0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79278e31",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d4df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells\\train\\labels\"\n",
    "ab = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells\\train\\bs\"\n",
    "for cnt, im in enumerate(os.listdir(test)):\n",
    "    img = cv2.imread(os.path.join(test, im)) * 255.\n",
    "    kernel = np.ones((3, 3), 'uint8')\n",
    "    dilate_img = cv2.dilate(img, kernel, iterations=1)\n",
    "    img1_bg = dilate_img - img\n",
    "    img1 = img1_bg[:,:,0]\n",
    "    clipped = np.clip(img1, 1, 6) # weight edges by factor (e.g. 6)\n",
    "    print(np.min(clipped))\n",
    "    cv2.imwrite(os.path.join(ab, im),clipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78dc66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch.losses import DiceLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af932e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DiceLoss(reduce = \"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8963cd",
   "metadata": {},
   "source": [
    "# Classification inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf46979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from skimage import io\n",
    "import torch\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f6ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = timm.create_model(\"regnety_004\", pretrained=True, num_classes = 5).cuda()\n",
    "#second = timm.create_model(\"resnext50d_32x4d\", pretrained=True, num_classes = 5).cuda()\n",
    "first.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\regnety_004.pt\"))\n",
    "#second.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\resnext50d_32x4d.pt\"))\n",
    "first.eval()\n",
    "#second.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a827dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = timm.create_model(\"resnest50d\", pretrained=True, num_classes = 5).cuda()\n",
    "second = timm.create_model(\"resnext50d_32x4d\", pretrained=True, num_classes = 5).cuda()\n",
    "first.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\resnest50d.pt\"))\n",
    "second.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\resnext50d_32x4d.pt\"))\n",
    "first.eval()\n",
    "second.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30bc34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "names = [\"NA\", \"TRG0\", \"TRG1\", \"TRG2\", \"TRG3\"]\n",
    "values = dict()\n",
    "for name in names:\n",
    "    values[name] = []\n",
    "    base = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset_valid/\" + name\n",
    "    for im in os.listdir(base):\n",
    "        image = io.imread(os.path.join(base, im)).astype(np.float32)\n",
    "        image = image/255.\n",
    "        s = torch.unsqueeze(torch.tensor(image, dtype = torch.float).permute(2, 0, 1), 0).cuda()\n",
    "        preds =  first(s)# +  second(s)\n",
    "        preds = torch.argmax(preds, 1)\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        values[name].append(preds[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076f2189",
   "metadata": {},
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d2f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset_valid\\TRG2\\E93 L X20_0_1183_3925.png\"\n",
    "base = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset_valid\\TRG2\"\n",
    "\n",
    "\n",
    "image = io.imread(path)\n",
    "image = image/255.\n",
    "s = torch.unsqueeze(torch.tensor(image, dtype = torch.float).permute(2, 0, 1), 0).cuda()\n",
    "preds = first(s) + second(s)\n",
    "preds = torch.argmax(preds, 1)\n",
    "preds = preds.detach().cpu().numpy()\n",
    "print(preds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec88af1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
