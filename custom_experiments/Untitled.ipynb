{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa983342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from visualdl import vdl\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import os\n",
    "import cv2i\n",
    "import timm\n",
    "import albumentations as A\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "from skimage import io\n",
    "#from custom import U2NET\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8d42c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vdl.get_inference_model(r\"F:\\source\\repos\\VisualDL\\resnet18, UnetPlusPlus.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414a67b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(r\"C:\\Users\\phili\\Documents\\001.pt\").keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b029cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cos(np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df04cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "t_range = np.arange(0, 2*np.pi, 0.05)\n",
    "for t in t_range:\n",
    "    #print((math.sin (t) * t, math.cos(t) * t))\n",
    "    plt.plot(np.sin(t) * t, np.cos(t) * t, markersize=1, marker='o')\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_color('none')\n",
    "    ax.spines['bottom'].set_position('zero')\n",
    "    ax.spines['left'].set_position('zero')\n",
    "    ax.spines['right'].set_color('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea980cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "timm.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d33b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.Resize(width=128, height=128),\n",
    "    A.RandomRotate90(p = 1),\n",
    "    A.Transpose(p=1),\n",
    "    A.RandomBrightness(p=1),\n",
    "    A.RandomContrast(p=1),\n",
    "    A.RandomShadow(p=1),\n",
    "    A.RGBShift(p=1),\n",
    "    A.RandomContrast(p=1),\n",
    "])\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(width=128, height=128),\n",
    "    A.GridDistortion(p=1),\n",
    "    A.OpticalDistortion(p=1),\n",
    "    A.ElasticTransform(p=1)\n",
    "\n",
    "])\n",
    "\n",
    "image =  io.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\CellsFinal\\Cells\\right side\\cells\\train\\images\\05__1_5291_9286.png\")\n",
    "mask = io.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\CellsFinal\\Cells\\right side\\cells\\train\\labels\\05__1_5291_9286.png\", as_gray = True)\n",
    "io.imsave(\"orig.png\", image)\n",
    "io.imsave(\"origmask.png\", mask)\n",
    "trans = transform(image = image, mask = mask)\n",
    "image = trans[\"image\"]\n",
    "mask = trans[\"mask\"]\n",
    "io.imsave(\"image.png\", image)\n",
    "io.imsave(\"mask.png\", mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10abfd40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d8b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0195a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(encoder_name = \"timm-resnest50d\", classes = 2, in_channels = 3)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0a49e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\timm-resnest50d, Unet.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a295f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells\\Cells\\test\\images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db05a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt, im in enumerate(os.listdir(image_dir)):\n",
    "    im = io.imread(os.path.join(image_dir, im))\n",
    "    im = im/255.\n",
    "    im = transform(image = im)[\"image\"]\n",
    "    im = torch.tensor(im, dtype = torch.float).permute(2, 0, 1).unsqueeze(0)\n",
    "    print(im.shape)\n",
    "    pred = model(im)\n",
    "    pred = torch.argmax(pred, 1)[0] * 255.\n",
    "    cv2.imwrite(str(cnt) + \".png\", pred.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390b4a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = cv2.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells1\\Cells\\train\\labels\\05__1_4685_10225.png\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c9ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = ii * 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a1d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"test.png\", ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187c5777",
   "metadata": {},
   "source": [
    "# Replace 2D with 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b75530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e72369",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, modules in model.named_modules():\n",
    "    for module in modules:\n",
    "        print(module)\n",
    "    if(isinstance(module, nn.Conv2d)):\n",
    "        kernel_size = module.kernel_size[0]\n",
    "        stride = module.stride[0]\n",
    "        padding = module.padding[0]\n",
    "        weight = module.weight.unsqueeze(2) / kernel_size\n",
    "        weight = torch.cat([weight for _ in range(0, kernel_size)], dim=2)\n",
    "        bias = module.bias\n",
    "\n",
    "        if(bias is None):\n",
    "            print(modules)\n",
    "            print(modules[name])\n",
    "            modules[name] = nn.Conv3d(in_channels=module.weight.shape[1], out_channels=module.weight.shape[0],\n",
    "                               kernel_size=kernel_size, padding=padding, stride=stride, bias=False)\n",
    "        else:\n",
    "            modules[name] = nn.Conv3d(in_channels=module.weight.shape[1], out_channels=module.weight.shape[0],\n",
    "                               kernel_size=kernel_size, padding=padding, stride=stride, bias=True)\n",
    "            modules[name].bias = bias\n",
    "\n",
    "            modules[name].weight.data = weight\n",
    "\n",
    "    elif(isinstance(module, nn.BatchNorm2d)):\n",
    "        weight = module.weight\n",
    "        bias = module.bias\n",
    "        modules[name] = nn.BatchNorm3d(weight.shape[0])\n",
    "        modules[name].weight = weight\n",
    "        modules[name].bias = bias\n",
    "\n",
    "for name in modules:\n",
    "    parent_module = model\n",
    "    objs = name.split(\".\")\n",
    "    if len(objs) == 1:\n",
    "        model.__setattr__(name, modules[name])\n",
    "        continue\n",
    "\n",
    "    for obj in objs[:-1]:\n",
    "        parent_module = parent_module.__getattr__(obj)\n",
    "\n",
    "    parent_module.__setattr__(objs[-1], modules[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609929d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_modules(module):\n",
    "    for a in module:\n",
    "        get_all_modules(a.children())\n",
    "        if isinstance(a, nn.Conv2d):\n",
    "            print(a)\n",
    "            #a = nn.Conv3d(3,32,3)\n",
    "            \n",
    "        elif isinstance(a, nn.BatchNorm2d):\n",
    "            print(a)\n",
    "\n",
    "            \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2654f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_modules(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5713df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in model.encoder.named_modules():\n",
    "    print(a)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc30e742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from uformer_pytorch import Uformer\n",
    "\n",
    "model = Uformer(\n",
    "    dim = 16,           # initial dimensions after input projection, which increases by 2x each stage\n",
    "    stages = 4,         # number of stages\n",
    "    num_blocks = 2,     # number of transformer blocks per stage\n",
    "    window_size = 16,   # set window size (along one side) for which to do the attention within\n",
    "    dim_head = 64,\n",
    "    heads = 1,\n",
    "    ff_mult = 4\n",
    ")\n",
    "model.cuda()\n",
    "x = torch.randn(1, 3, 512, 512).cuda()\n",
    "pred = model(x) # (1, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb84406",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dffd467",
   "metadata": {},
   "source": [
    "# Segmentation inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cc3c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from visualdl import vdl\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.transform import rescale, resize\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "from visualdl.models.doubleunet.doubleunet import DoubleUnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdl.predict(images, r\"E:\\source\\repos\\VisualDL\\tu-resnest50d, Unet.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f1c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uformer_pytorch import Uformer\n",
    "model = U2NET(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d91f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.create_model(arch = \"TransUnet\", encoder_name = \"tu-efficientnetv2_rw_m\", classes = 2, in_channels = 3, image_size = 512,decoder_attention_type = None)\n",
    "#model1 = smp.create_model(arch = \"DoubleUnet\", encoder_name = \"tu-efficientnetv2_rw_m\", classes = 2, in_channels = 3, image_size = 512,decoder_attention_type = None)\n",
    "model1 = DoubleUnet(encoder_name = \"tu-efficientnetv2_rw_m\", classes = 2)\n",
    "#model = smp.create_model(arch = \"UnetPlusPlus\", encoder_name = \"resnet18\", classes = 6, in_channels = 3, image_size = 512, decoder_attention_type = \"scse\")\n",
    "model.load_state_dict(torch.load(r\"F:\\source\\repos\\VisualDL\\CurrentBesterEfficientnet.pt\")['model_state_dict'])\n",
    "model1.load_state_dict(torch.load(r\"F:\\source\\repos\\VisualDL\\tu-efficientnetv2_rw_m, DoubleUnet.pt\")['model_state_dict'])\n",
    "model.eval()\n",
    "model1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c3c501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d196607",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()\n",
    "model1 = model1.cuda()\n",
    "#second = second.cuda()\n",
    "#third = third.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67a97c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4547c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.Resize(width=512, height=512),\n",
    "])\n",
    "#test = r\"F:\\source\\repos\\Daten\\HER-N\\Pdl1Combined\\Tumor Cells 512\\valid\\images\"\n",
    "test = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\job_instance_analysis\"\n",
    "test = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells her2 128\\valid\\images\"\n",
    "test = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\imgs\\imgs\"\n",
    "for cnt, im in enumerate(os.listdir(test)):\n",
    "    image = io.imread(os.path.join(test, im))\n",
    "    image = transform(image = image)[\"image\"]\n",
    "    image = image/255.\n",
    "    s = torch.unsqueeze(torch.tensor(image, dtype = torch.float).permute(2, 0, 1), 0).cuda()\n",
    "    with torch.cuda.amp.autocast():\n",
    "        preds = model(s)\n",
    "        preds += model1(s)\n",
    "        class_preds = preds#[:,0:-1]\n",
    "        #dist_map = m(preds[:,-1])\n",
    "        #dist_map[dist_map < 0.45] = 0\n",
    "        #preds += model1(s)\n",
    "    preds = torch.argmax(class_preds, 1)\n",
    "    #dist_map[preds == 0] = 0.0\n",
    "    #class_for_dist = preds.clone()\n",
    "    #class_for_dist[class_for_dist>0] = 1\n",
    "    #dist_map[class_for_dist == 0] = 0\n",
    "\n",
    "    preds *= 255\n",
    "    #preds[preds == 1] = 50\n",
    "    #preds[preds == 2] = 100\n",
    "    #preds[preds == 3] = 150\n",
    "    #preds[preds == 4] = 200\n",
    "    #preds[preds == 5] = 250\n",
    "    #dist_map = dist_map.detach().cpu().numpy()\n",
    "    #maps = dist_map[0]\n",
    "    #maps[maps>0] = 255\n",
    "    #maps = maps.astype(np.uint8)\n",
    "    #print(image.shape)\n",
    "    #print(maps.dtype)\n",
    "    #markers\t= cv2.watershed(preds[0].detach().cpu().numpy().astype(np.int), maps)\n",
    "    io.imsave(f\"{im}.png\", preds[0].detach().cpu().numpy())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7bef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b= cv2.imread(r\"F:\\source\\repos\\VisualDL\\custom_experiments\\05__1_3118-10375_0.png.png\", 0)\n",
    "b[b>160] = 255\n",
    "b[b <=160] = 0\n",
    "cv2.imwrite(\"xd.png\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f19f593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92423dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5127b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79278e31",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d4df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells\\train\\labels\"\n",
    "ab = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells\\train\\bs\"\n",
    "for cnt, im in enumerate(os.listdir(test)):\n",
    "    img = cv2.imread(os.path.join(test, im)) * 255.\n",
    "    kernel = np.ones((3, 3), 'uint8')\n",
    "    dilate_img = cv2.dilate(img, kernel, iterations=1)\n",
    "    img1_bg = dilate_img - img\n",
    "    img1 = img1_bg[:,:,0]\n",
    "    clipped = np.clip(img1, 1, 6) # weight edges by factor (e.g. 6)\n",
    "    print(np.min(clipped))\n",
    "    cv2.imwrite(os.path.join(ab, im),clipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78dc66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch.losses import DiceLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af932e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DiceLoss(reduce = \"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8963cd",
   "metadata": {},
   "source": [
    "# Classification inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf46979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from skimage import io\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f6ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = timm.create_model(\"resnext50_32x4d\", pretrained=True, num_classes = 5).cuda()\n",
    "#second = timm.create_model(\"resnext50d_32x4d\", pretrained=True, num_classes = 5).cuda()\n",
    "first.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\resnext50_32x4d.pt\"))\n",
    "#second.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\resnext50d_32x4d.pt\"))\n",
    "first.eval()\n",
    "#second.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a827dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = timm.create_model(\"resnext50_32x4d\", pretrained=True, num_classes = 5).cuda()\n",
    "second = timm.create_model(\"efficientnet_b4\", pretrained=True, num_classes = 5).cuda()\n",
    "first.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\resnext50_32x4d.pt\"))\n",
    "#second.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\efficientnet_b4.pt\"))\n",
    "first.eval()\n",
    "#second.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30bc34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "transform = A.Compose([\n",
    "    A.Resize(width=512, height=512),\n",
    "])\n",
    "counter = 0\n",
    "counterxd = 0\n",
    "names = [\"NA\", \"TRG0\", \"TRG1\", \"TRG2\", \"TRG3\"]\n",
    "for name in names:\n",
    "    os.mkdir(name)\n",
    "values = dict()\n",
    "for cnt, name in enumerate(names):\n",
    "    values[name] = []\n",
    "    base = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset_valid/\" + name\n",
    "    for im in os.listdir(base):\n",
    "        image = io.imread(os.path.join(base, im)).astype(np.float32)\n",
    "        image = transform(image = image)[\"image\"]\n",
    "        image = image/255.\n",
    "        s = torch.unsqueeze(torch.tensor(image, dtype = torch.float).permute(2, 0, 1), 0).cuda()\n",
    "        preds =  first(s) \n",
    "        preds = torch.argmax(preds, 1)\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        values[name].append(preds[0])\n",
    "        counterxd += 1\n",
    "        io.imsave(f\"{names[preds[0]]}/{im}.png\", image)\n",
    "        if cnt == preds:\n",
    "            counter += 1\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076f2189",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter/counterxd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de14543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'NA': [4, 0, 0, 2, 0, 0],\n",
    " 'TRG0': [1, 4],\n",
    " 'TRG1': [2, 2, 2],\n",
    " 'TRG2': [3, 4, 3],\n",
    " 'TRG3': [4, 4, 4, 4, 4, 4, 4, 0, 4, 2, 4, 4, 4, 4, 4, 4, 2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d2f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset_valid\\TRG2\\E93 L X20_0_1183_3925.png\"\n",
    "base = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset_valid\\TRG2\"\n",
    "\n",
    "\n",
    "image = io.imread(path)\n",
    "image = image/255.\n",
    "s = torch.unsqueeze(torch.tensor(image, dtype = torch.float).permute(2, 0, 1), 0).cuda()\n",
    "preds = first(s) + second(s)\n",
    "preds = torch.argmax(preds, 1)\n",
    "preds = preds.detach().cpu().numpy()\n",
    "print(preds)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0872d3",
   "metadata": {},
   "source": [
    "# Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab262af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "start = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\test_data\\test_data\"\n",
    "\n",
    "for im in os.listdir(os.path.join(start, \"images\")):\n",
    "    img = cv2.imread(os.path.join(os.path.join(start, \"images\"), im), 0)\n",
    "    mask = cv2.imread(os.path.join(os.path.join(start, \"masks\"), im), 0)\n",
    "    mask[img == 0] = 0\n",
    "    mask[mask == 0] = 0\n",
    "    mask[mask == 29] = 1\n",
    "    mask[mask == 105] = 2\n",
    "    mask[mask == 117] = 3\n",
    "    mask[mask ==189] = 4\n",
    "    mask[mask == 225] = 5\n",
    "    mask[mask > 5] = 1\n",
    "    print(os.path.join(os.path.join(start, \"labels\"), im))\n",
    "    cv2.imwrite(os.path.join(os.path.join(start, \"labels\"), im), mask)\n",
    "    #img[img > 0] = 1\n",
    "    #cv2.imwrite(os.path.join(start, im), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebdae52",
   "metadata": {},
   "source": [
    "# Trian test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2884ed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = r\"E:\\source\\repos\\Daten\\PLA\\train\\images\"\n",
    "to = r\"E:\\source\\repos\\Daten\\PLA\\val\\images\"\n",
    "import random\n",
    "from shutil import copyfile\n",
    "import os\n",
    "split = os.listdir(start)\n",
    "random.shuffle(split)\n",
    "\n",
    "test = split[0:35]\n",
    "train = split[35:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c565f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in test:\n",
    "    copyfile(os.path.join(start, file), os.path.join(to, file))\n",
    "    copyfile(os.path.join(start, file).replace(\"images\", \"labels\"), os.path.join(to, file).replace(\"images\", \"labels\"))\n",
    "    os.remove(os.path.join(start, file))\n",
    "    os.remove(os.path.join(start, file).replace(\"images\", \"labels\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b88449",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Nuclei\\Nuclei\\train\\labels\"\n",
    "out = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Nuclei\\Nuclei\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(start):\n",
    "    preds = cv2.imread(os.path.join(start, file), 0)\n",
    "    preds[preds == 1] = 50\n",
    "    preds[preds == 2] = 100\n",
    "    preds[preds == 3] = 150\n",
    "    preds[preds == 4] = 200\n",
    "    preds[preds == 5] = 250\n",
    "    cv2.imwrite(os.path.join(out, file), preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3244b1a0",
   "metadata": {},
   "source": [
    "# 255 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b2d6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = r\"E:\\source\\repos\\Daten\\HER-N\\hubt\\dataset\\Cells\\valid\\labels\"\n",
    "import cv2\n",
    "import os\n",
    "for file in os.listdir(start):\n",
    "    img = cv2.imread(os.path.join(start, file), 0)\n",
    "    img[img > 0] = 1\n",
    "    cv2.imwrite(os.path.join(start, file), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0567e77b",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490756ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from uformer_pytorch import Uformer\n",
    "\n",
    "model = model = Uformer(\n",
    "                dim = 64,           # initial dimensions after input projection, which increases by 2x each stage\n",
    "                stages = 3,         # number of stages\n",
    "                num_blocks = 2,     # number of transformer blocks per stage\n",
    "                window_size = 16,   # set window size (along one side) for which to do the attention within\n",
    "                dim_head = 64,\n",
    "                heads = 4,\n",
    "                ff_mult = 2\n",
    "            ).cuda()\n",
    "\n",
    "x = torch.randn(2, 3, 128, 128).cuda()\n",
    "with torch.cuda.amp.autocast():\n",
    "    pred = model(x) # (1, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce132281",
   "metadata": {},
   "source": [
    "# Extract single instances for obj. detec + semant. seg -> bring into yolov5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080b1a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e39689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv.imread(r\"E:\\source\\repos\\Daten\\Converting\\Tumor Cells\\train\\labels\\PD-L1=2_0_37392-42804_85.png\", 0)\n",
    "\n",
    "# Creating kernel\n",
    "#kernel = np.ones((2, 2), np.uint8)\n",
    "  \n",
    "# Using cv2.erode() method \n",
    "#image = cv.erode(im, kernel) \n",
    "\n",
    "#image[image > 0] = 255\n",
    "#ret, thresh = cv.threshold(im, 127, 255, 0)\n",
    "#contours,hierachy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea0d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "im[im != 1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b1581",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(contours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e416ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.imwrite(\"xd.png\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46782004",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_without = 0\n",
    "count_with = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43df744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_without)\n",
    "print(count_with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba61ddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = {}\n",
    "start = r\"C:/Users/phili/Downloads/Telegram Desktop/MegNeueDaten/dataset/valid/labels\"\n",
    "all_files = os.listdir(start)\n",
    "nc = 2\n",
    "for img in os.listdir(start):\n",
    "    for i in range(1, nc):\n",
    "        im = cv.imread(os.path.join(start, img), 0)\n",
    "        im[im > 0] = 1.0\n",
    "        kernel = np.ones((2, 2), np.uint8)\n",
    "        #im = cv.erode(im, kernel)\n",
    "        #im = cv.dilate(im, kernel)\n",
    "        tmp = im.copy()\n",
    "        tmp[tmp != i] = 0\n",
    "        tmp[tmp == i] = 255\n",
    "        #if img == \"05__1_3130-9263_11.png\":\n",
    "        #    cv2.imwrite(\"xd.png\", tmp)\n",
    "        contours,hierachy = cv.findContours(tmp, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "        blank = np.zeros_like(tmp)\n",
    "        for cnt, cont in enumerate(contours):\n",
    "            xmin,ymin,width,height = cv.boundingRect(cont)\n",
    "            if width <= 3 or height <= 3:\n",
    "                continue\n",
    "            #cv.rectangle(im,(x,y),(x+width,y+height),(255),1)\n",
    "            #cv2.imwrite(\"xd.png\", im)\n",
    "            image_width = im.shape[0]\n",
    "            xcenter, ycenter = xmin + width/2, ymin + height/2\n",
    "            xcenter, ycenter, width, height = xcenter/image_width, ycenter/image_width, width/image_width, height/image_width\n",
    "            if not img in files:\n",
    "                files[img] = [(str(i - 1),str(xcenter), str(ycenter), str(width), str(height))]\n",
    "            else:\n",
    "\n",
    "                files[img] += [(str(i - 1),str(xcenter), str(ycenter), str(width), str(height))]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a93a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "base = cv.imread(r\"F:\\source\\repos\\Daten\\FinalHer2512\\train\\images\\05__1_4192-7336_15.png\")\n",
    "\n",
    "for rec in files['05__1_4192-7336_15.png']:\n",
    "    xcenter,ycenter,width,height = [int(float(xx) * base.shape[0]) for xx in rec[1:]]\n",
    "    x = int(xcenter - width/2)\n",
    "    y = int(ycenter - height/2)\n",
    "    va = int(rec[0] * 50)\n",
    "    print(rec)\n",
    "    print(\"\\nXD\")\n",
    "    cv.circle(base, (xcenter,ycenter), 1, (255,255,255), -1)\n",
    "    #cv.rectangle(base,(x,y),(x+width,y+height),(50,va * 50,va * 50),1)\n",
    "cv.imwrite(\"xd4.png\", base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c82d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv.imread(os.path.join(r\"E:\\source\\repos\\Daten\\Cells\\train\\images\", '05__1_3115_10030.png'), 0)\n",
    "kernel = np.ones((2, 2), np.uint8)\n",
    "image = cv.erode(im, kernel) \n",
    "contours,hierachy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "blank = np.zeros_like(im)\n",
    "for cnt, cont in enumerate(contours):\n",
    "    x,y,width,height = cv.boundingRect(cont)\n",
    "    cv.rectangle(im,(x,y),(x+width,y+height),(255),1)\n",
    "cv2.imwrite(\"xd.png\", im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51cb372",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_anno = [item for item in all_files if item not in list(files.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d35ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e9b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"labels\")\n",
    "for cnt, (key, val) in enumerate(files.items()):\n",
    "    with open(\"labels/\" + key.replace(\".png\", \".txt\"), \"w\") as handle:\n",
    "        for va in val:\n",
    "            handle.write(\" \".join(list(va))+ \"\\n\") \n",
    "            \n",
    "for name in no_anno:\n",
    "    with open(\"labels/\" + name.replace(\".png\", \".txt\"), \"w\") as handle:\n",
    "        handle.write(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891f647a",
   "metadata": {},
   "source": [
    "# Export for instance segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50876230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abacfd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"labels\")\n",
    "os.mkdir(\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86410f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_rec(img, rec):\n",
    "    x,y,w,h = rec\n",
    "    return img[y:y+h, x:x+w]\n",
    "def add_rec(orig, img, rec):\n",
    "    x,y,w,h = rec\n",
    "    #r,g,b = [random.randint(20, 255) for i in range(3)]\n",
    "    img[img > 0] = random.randint(20,255)\n",
    "    tmp = np.expand_dims(img.astype(np.uint8), axis=-1)\n",
    "    #tmp[np.all(tmp == (255, 255, 255), axis=-1)] = (b,g,r)\n",
    "    orig[y:y+h, x:x+w] += tmp\n",
    "    return orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e98f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pad = A.Compose([\n",
    "    A.PadIfNeeded(64,64, value = 0, border_mode = 0),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c44bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "labels = r\"E:\\source\\repos\\Daten\\Cells\\train\\labels\"\n",
    "images = r\"E:\\source\\repos\\Daten\\Cells\\train\\images\"\n",
    "to_labels = r\"E:\\source\\repos\\VisualDL\\custom_experiments\\labels\"\n",
    "to_images = r\"E:\\source\\repos\\VisualDL\\custom_experiments\\images\"\n",
    "all_files = os.listdir(labels)\n",
    "adding = 0\n",
    "for img in os.listdir(labels):\n",
    "    im = cv.imread(os.path.join(labels, img), 0)\n",
    "    original = cv.imread(os.path.join(images, img))\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    image = cv.erode(im, kernel) \n",
    "    contours,hierachy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    for cnt, cont in enumerate(contours):\n",
    "        blank = np.zeros_like(im)\n",
    "        xmin,ymin,width,height = cv.boundingRect(cont)\n",
    "        xmin -= 5\n",
    "        ymin -= 5\n",
    "        width +=5\n",
    "        height += 5\n",
    "        xmin = min(0, xmin)\n",
    "        ymin = min(0, ymin)\n",
    "        cv.drawContours(blank, [cont], -1, 1, -1)\n",
    "        final = cut_rec(blank, (xmin, ymin , width , height ))\n",
    "        orig_final = cut_rec(original, (xmin, ymin , width , height ))\n",
    "        trans = pad(image = orig_final, mask = final)\n",
    "        final = trans[\"mask\"]\n",
    "        orig_final = trans[\"image\"]\n",
    "        cv.imwrite(os.path.join(to_labels, img.replace(\".png\", f\"{cnt}.png\")), final)\n",
    "        cv.imwrite(os.path.join(to_images, img.replace(\".png\", f\"{cnt}.png\")), orig_final)\n",
    "        #cv.rectangle(im,(x,y),(x+width,y+height),(255),1)\n",
    "        \n",
    "        #cv2.imwrite(\"xd.png\", im)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b4cab0",
   "metadata": {},
   "source": [
    "# instance segmentation inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc80695",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.UnetPlusPlus(encoder_name = \"resnext50_32x4d\", classes = 2, in_channels = 3)\n",
    "model.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\resnext50_32x4d, UnetPlusPlus.pt\"))\n",
    "model.eval()\n",
    "model=model.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9078c0",
   "metadata": {},
   "source": [
    "## simple predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed2ae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    #A.PadIfNeeded(64,64, value = 0, border_mode = 0),\n",
    "    A.Resize(width=64, height=64),\n",
    "])\n",
    "test = r\"E:\\source\\repos\\VisualDL\\custom_experiments\\valid\\images\"\n",
    "for cnt, im in enumerate(os.listdir(test)):\n",
    "    image = io.imread(os.path.join(test, im))\n",
    "    image = transform(image = image)[\"image\"]\n",
    "    image = image/255.\n",
    "    s = torch.unsqueeze(torch.tensor(image, dtype = torch.float).permute(2, 0, 1), 0).cuda()\n",
    "    with torch.cuda.amp.autocast():\n",
    "        preds = model(s)\n",
    "    preds = torch.argmax(preds, 1)\n",
    "    preds *= 255\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    io.imsave(f\"{im}.png\", preds[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf9c6b5",
   "metadata": {},
   "source": [
    "# use rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8452363",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.PadIfNeeded(64,64, value = 0, border_mode = 0),\n",
    "    A.Resize(width=64, height=64),\n",
    "])\n",
    "\n",
    "files = {}\n",
    "labels = r\"E:\\source\\repos\\Daten\\Cells\\valid\\labels\"\n",
    "images = r\"E:\\source\\repos\\Daten\\Cells\\valid\\images\"\n",
    "to = r\"E:\\source\\repos\\VisualDL\\custom_experiments\\test\"\n",
    "all_files = os.listdir(labels)\n",
    "adding = 0\n",
    "for ii, img in enumerate(os.listdir(labels)):\n",
    "    im = cv.imread(os.path.join(labels, img), 0)\n",
    "    print(img)\n",
    "    original = cv.imread(os.path.join(images, img))\n",
    "    blank = np.zeros_like(original)\n",
    "    original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    image = cv.erode(im, kernel) \n",
    "    contours,hierachy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "    for cnt, cont in enumerate(contours):\n",
    "        \n",
    "        xmin,ymin,width,height = cv.boundingRect(cont)\n",
    "        orig_final = cut_rec(original, (xmin, ymin , width , height ))\n",
    "        tt = orig_final.copy()\n",
    "        orig_final = transform(image = orig_final)[\"image\"]\n",
    "        image = orig_final/255.\n",
    "        s = torch.unsqueeze(torch.tensor(image, dtype = torch.float).permute(2, 0, 1), 0).cuda()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            preds = model(s)\n",
    "        preds = torch.argmax(preds, 1)\n",
    "        preds *= 255\n",
    "        preds = preds.detach().cpu().numpy()[0]\n",
    "\n",
    "        preds = cut_rec(preds, (32 - int(width/2), 32 - int(height/2), width, height))\n",
    "        #contours,hierachy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "        blank = add_rec(blank, preds, (xmin,ymin,width,height))\n",
    "        cv2.imwrite(os.path.join(to, f\"{img}{cnt}.png\"), preds)\n",
    "        cv2.imwrite(os.path.join(to, f\"{img}{cnt}orig.png\"), tt)\n",
    "    cv2.imwrite(os.path.join(to, f\"{img}{cnt}xdddd.png\"), blank)\n",
    "    if ii == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658f0779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a3aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "skimage.skimage.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de6e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = U2NET(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aa5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_in = torch.randn(1, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0201e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, einsum\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from custom import ViT, AxialImageTransformer, AxialAttention\n",
    "# helpers\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def divisible_by(val, divisor):\n",
    "    return (val % divisor) == 0\n",
    "\n",
    "def unfold_output_size(image_size, kernel_size, stride, padding):\n",
    "    return int(((image_size - kernel_size + (2 * padding)) / stride) + 1)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = heads * dim_head\n",
    "        self.heads =  heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, d, h = *x.shape, self.heads\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
    "\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "        attn = sim.softmax(dim = -1)\n",
    "\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "# main class\n",
    "\n",
    "class TNT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        image_size,\n",
    "        patch_dim,\n",
    "        pixel_dim,\n",
    "        patch_size,\n",
    "        pixel_size,\n",
    "        depth,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        ff_dropout = 0.,\n",
    "        attn_dropout = 0.,\n",
    "        channels = 3,\n",
    "        unfold_args = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert divisible_by(image_size, patch_size), 'image size must be divisible by patch size'\n",
    "        assert divisible_by(patch_size, pixel_size), 'patch size must be divisible by pixel size for now'\n",
    "\n",
    "        num_patch_tokens = (image_size // patch_size) ** 2\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patch_tokens = nn.Parameter(torch.randn(num_patch_tokens + 1, patch_dim))\n",
    "\n",
    "        unfold_args = default(unfold_args, (pixel_size, pixel_size, 0))\n",
    "        unfold_args = (*unfold_args, 0) if len(unfold_args) == 2 else unfold_args\n",
    "        kernel_size, stride, padding = unfold_args\n",
    "\n",
    "        pixel_width = unfold_output_size(patch_size, kernel_size, stride, padding)\n",
    "        num_pixels = pixel_width ** 2\n",
    "\n",
    "        self.to_pixel_tokens = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> (b h w) c p1 p2', p1 = patch_size, p2 = patch_size),\n",
    "            nn.Unfold(kernel_size = kernel_size, stride = stride, padding = padding),\n",
    "            Rearrange('... c n -> ... n c'),\n",
    "            nn.Linear(channels * kernel_size ** 2, pixel_dim)\n",
    "        )\n",
    "\n",
    "        self.patch_pos_emb = nn.Parameter(torch.randn(num_patch_tokens + 1, patch_dim))\n",
    "        self.pixel_pos_emb = nn.Parameter(torch.randn(num_pixels, pixel_dim))\n",
    "\n",
    "        layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "\n",
    "            pixel_to_patch = nn.Sequential(\n",
    "                nn.LayerNorm(pixel_dim),\n",
    "                Rearrange('... n d -> ... (n d)'),\n",
    "                nn.Linear(pixel_dim * num_pixels, patch_dim),\n",
    "            )\n",
    "\n",
    "            layers.append(nn.ModuleList([\n",
    "                PreNorm(pixel_dim, Attention(dim = pixel_dim, heads = heads, dim_head = dim_head, dropout = attn_dropout)),\n",
    "                PreNorm(pixel_dim, FeedForward(dim = pixel_dim, dropout = ff_dropout)),\n",
    "                pixel_to_patch,\n",
    "                PreNorm(patch_dim, Attention(dim = patch_dim, heads = heads, dim_head = dim_head, dropout = attn_dropout)),\n",
    "                PreNorm(patch_dim, FeedForward(dim = patch_dim, dropout = ff_dropout)),\n",
    "            ]))\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, _, h, w, patch_size, image_size = *x.shape, self.patch_size, self.image_size\n",
    "        assert divisible_by(h, patch_size) and divisible_by(w, patch_size), f'height {h} and width {w} of input must be divisible by the patch size'\n",
    "\n",
    "        num_patches_h = h // patch_size\n",
    "        num_patches_w = w // patch_size\n",
    "        n = num_patches_w * num_patches_h\n",
    "\n",
    "        pixels = self.to_pixel_tokens(x)\n",
    "        patches = repeat(self.patch_tokens[:(n + 1)], 'n d -> b n d', b = b)\n",
    "\n",
    "        patches += rearrange(self.patch_pos_emb[:(n + 1)], 'n d -> () n d')\n",
    "        pixels += rearrange(self.pixel_pos_emb, 'n d -> () n d')\n",
    "\n",
    "        for pixel_attn, pixel_ff, pixel_to_patch_residual, patch_attn, patch_ff in self.layers:\n",
    "\n",
    "            pixels = pixel_attn(pixels) + pixels\n",
    "            pixels = pixel_ff(pixels) + pixels\n",
    "\n",
    "            patches_residual = pixel_to_patch_residual(pixels)\n",
    "\n",
    "            patches_residual = rearrange(patches_residual, '(b h w) d -> b (h w) d', h = num_patches_h, w = num_patches_w)\n",
    "            patches_residual = F.pad(patches_residual, (0, 0, 1, 0), value = 0) # cls token gets residual of 0\n",
    "            patches = patches + patches_residual\n",
    "\n",
    "            patches = patch_attn(patches) + patches\n",
    "            patches = patch_ff(patches) + patches\n",
    "        hidden_states = patches[:,1:,:]\n",
    "        B, n_patch, hidden = hidden_states.size()  # reshape from (B, n_patch, hidden) to (B, h, w, hidden)\n",
    "        h, w = int(np.sqrt(n_patch)), int(np.sqrt(n_patch))\n",
    "        x = hidden_states.permute(0, 2, 1)\n",
    "        x = x.contiguous().view(B, hidden, h, w)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Conv2dReLU(nn.Sequential):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            padding=0,\n",
    "            stride=1,\n",
    "            use_batchnorm=True,\n",
    "    ):\n",
    "\n",
    "        if use_batchnorm == \"inplace\" and InPlaceABN is None:\n",
    "            raise RuntimeError(\n",
    "                \"In order to use `use_batchnorm='inplace'` inplace_abn package must be installed. \"\n",
    "                + \"To install see: https://github.com/mapillary/inplace_abn\"\n",
    "            )\n",
    "\n",
    "        conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=not (use_batchnorm),\n",
    "        )\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        if use_batchnorm == \"inplace\":\n",
    "            bn = InPlaceABN(out_channels, activation=\"leaky_relu\", activation_param=0.0)\n",
    "            relu = nn.Identity()\n",
    "\n",
    "        elif use_batchnorm and use_batchnorm != \"inplace\":\n",
    "            bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        else:\n",
    "            bn = nn.Identity()\n",
    "\n",
    "        super(Conv2dReLU, self).__init__(conv, bn, relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1990a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "image_size = 128\n",
    "init_dim = 16\n",
    "conv = Conv2dReLU(3, init_dim, 1).cuda()\n",
    "inst = AxialImageTransformer(dim = init_dim,depth = 2,axial_pos_emb_shape = (image_size,image_size)).cuda()\n",
    "\n",
    "first = TNT(image_size = image_size, patch_size = 2, pixel_dim = 16, pixel_size = 1, patch_dim = init_dim * 2, depth = 2, heads = 4, channels = init_dim).cuda()\n",
    "second = TNT(image_size = image_size//2, patch_size = 2, pixel_dim = 16, pixel_size = 1, patch_dim = init_dim * 4, depth = 4, heads = 6, channels = init_dim * 2).cuda()\n",
    "third = TNT(image_size = image_size//4, patch_size = 2, pixel_dim = 16, pixel_size = 1, patch_dim = init_dim * 8, depth = 6, heads = 8, channels = init_dim * 4).cuda()\n",
    "fourth = TNT(image_size = image_size//8, patch_size = 2, pixel_dim = 16, pixel_size = 1, patch_dim = init_dim * 16, depth = 8, heads = 12, channels = init_dim * 8).cuda()\n",
    "cup = ViT(\n",
    "            image_size = 8,\n",
    "            patch_size = 1,\n",
    "            dim = init_dim * 32,\n",
    "            depth = 6,\n",
    "            heads = 12,\n",
    "            mlp_dim = 2048,\n",
    "            dropout = 0.1,\n",
    "            emb_dropout = 0.1,\n",
    "            channels = init_dim * 16\n",
    "        ).cuda()\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                in_channels,\n",
    "                skip_channels,\n",
    "                out_channels,\n",
    "                depth,\n",
    "                size,\n",
    "                heads= 2):\n",
    "        super().__init__()   \n",
    "        self.ax = AxialImageTransformer(dim = in_channels + skip_channels,heads= heads,depth = depth,axial_pos_emb_shape = (size,size)).cuda()\n",
    "        self.out = Conv2dReLU(in_channels + skip_channels, out_channels, 1)\n",
    "        \n",
    "    def forward(self, x, skip = None):\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        return self.out(self.ax(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2090bb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = 2\n",
    "up1 = DecoderBlock(init_dim*32, init_dim*8, init_dim*8, depth = 2, heads = 2,size = image_size//8).cuda()\n",
    "up2 = DecoderBlock(init_dim*8, init_dim*4, init_dim*4, depth = 2, heads = 2,size = image_size//4).cuda()\n",
    "up3 = DecoderBlock(init_dim*4, init_dim*2, init_dim*2, depth = 2, heads = 2,size = image_size//2).cuda()\n",
    "up4 = DecoderBlock(init_dim*2, init_dim, 2, depth = 2, heads = 2,size = image_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abe12ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=AxialAttention(32,dim_index =1).cuda()\n",
    "a= AxialImageTransformer(dim = 3072, depth = 1,axial_pos_emb_shape = (128,128), heads=1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ce9eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_in = torch.randn(2, 3072, 128, 128).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ddc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "a(dummy_in).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c640cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inst(conv(dummy_in)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b26bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(hidden_states):\n",
    "    B, n_patch, hidden = hidden_states.size()  # reshape from (B, n_patch, hidden) to (B, h, w, hidden)\n",
    "    h, w = int(np.sqrt(n_patch)), int(np.sqrt(n_patch))\n",
    "    x = hidden_states.permute(0, 2, 1)\n",
    "    x = x.contiguous().view(B, hidden, h, w)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a3629",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.cuda.amp.autocast():\n",
    "    out1 = inst(conv(dummy_in)) #128\n",
    "with torch.cuda.amp.autocast():\n",
    "    out2 = first(out1) #64\n",
    "with torch.cuda.amp.autocast():\n",
    "    out3 = second(out2) #32\n",
    "with torch.cuda.amp.autocast():\n",
    "    out4 = third(out3) #16\n",
    "with torch.cuda.amp.autocast():\n",
    "    out5 = fourth(out4) #8\n",
    "with torch.cuda.amp.autocast():\n",
    "     out6 = cup(out5) #8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca05747f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "up4(up3(up2(up1(out6, out4), out3), out2), out1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2127a1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hidden_states = out\n",
    "B, n_patch, hidden = hidden_states.size()  # reshape from (B, n_patch, hidden) to (B, h, w, hidden)\n",
    "h, w = int(np.sqrt(n_patch)), int(np.sqrt(n_patch))\n",
    "x = hidden_states.permute(0, 2, 1)\n",
    "x = x.contiguous().view(B, hidden, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ea3048",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e79fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "2**5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e0f671",
   "metadata": {},
   "source": [
    "# HRNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d30d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "from torch.nn.parallel._functions import ReduceAddCoalesced, Broadcast\n",
    "\n",
    "class SyncMaster(object):\n",
    "    \"\"\"An abstract `SyncMaster` object.\n",
    "    - During the replication, as the data parallel will trigger an callback of each module, all slave devices should\n",
    "    call `register(id)` and obtain an `SlavePipe` to communicate with the master.\n",
    "    - During the forward pass, master device invokes `run_master`, all messages from slave devices will be collected,\n",
    "    and passed to a registered callback.\n",
    "    - After receiving the messages, the master device should gather the information and determine to message passed\n",
    "    back to each slave devices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, master_callback):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            master_callback: a callback to be invoked after having collected messages from slave devices.\n",
    "        \"\"\"\n",
    "        self._master_callback = master_callback\n",
    "        self._queue = queue.Queue()\n",
    "        self._registry = collections.OrderedDict()\n",
    "        self._activated = False\n",
    "\n",
    "    def register_slave(self, identifier):\n",
    "        \"\"\"\n",
    "        Register an slave device.\n",
    "        Args:\n",
    "            identifier: an identifier, usually is the device id.\n",
    "        Returns: a `SlavePipe` object which can be used to communicate with the master device.\n",
    "        \"\"\"\n",
    "        if self._activated:\n",
    "            assert self._queue.empty(), 'Queue is not clean before next initialization.'\n",
    "            self._activated = False\n",
    "            self._registry.clear()\n",
    "        future = FutureResult()\n",
    "        self._registry[identifier] = _MasterRegistry(future)\n",
    "        return SlavePipe(identifier, self._queue, future)\n",
    "\n",
    "class _SynchronizedBatchNorm(_BatchNorm):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.001, affine=True):\n",
    "        super(_SynchronizedBatchNorm, self).__init__(num_features, eps=eps, momentum=momentum, affine=affine)\n",
    "\n",
    "        self._sync_master = SyncMaster(self._data_parallel_master)\n",
    "\n",
    "        self._is_parallel = False\n",
    "        self._parallel_id = None\n",
    "        self._slave_pipe = None\n",
    "\n",
    "        # customed batch norm statistics\n",
    "        self._moving_average_fraction = 1. - momentum\n",
    "        self.register_buffer('_tmp_running_mean', torch.zeros(self.num_features))\n",
    "        self.register_buffer('_tmp_running_var', torch.ones(self.num_features))\n",
    "        self.register_buffer('_running_iter', torch.ones(1))\n",
    "        self._tmp_running_mean = self.running_mean.clone() * self._running_iter\n",
    "        self._tmp_running_var = self.running_var.clone() * self._running_iter\n",
    "\n",
    "    def forward(self, input):\n",
    "        # If it is not parallel computation or is in evaluation mode, use PyTorch's implementation.\n",
    "        if not (self._is_parallel and self.training):\n",
    "            return F.batch_norm(\n",
    "                input, self.running_mean, self.running_var, self.weight, self.bias,\n",
    "                self.training, self.momentum, self.eps)\n",
    "\n",
    "        # Resize the input to (B, C, -1).\n",
    "        input_shape = input.size()\n",
    "        input = input.view(input.size(0), self.num_features, -1)\n",
    "\n",
    "        # Compute the sum and square-sum.\n",
    "        sum_size = input.size(0) * input.size(2)\n",
    "        input_sum = _sum_ft(input)\n",
    "        input_ssum = _sum_ft(input ** 2)\n",
    "\n",
    "        # Reduce-and-broadcast the statistics.\n",
    "        if self._parallel_id == 0:\n",
    "            mean, inv_std = self._sync_master.run_master(_ChildMessage(input_sum, input_ssum, sum_size))\n",
    "        else:\n",
    "            mean, inv_std = self._slave_pipe.run_slave(_ChildMessage(input_sum, input_ssum, sum_size))\n",
    "\n",
    "        # Compute the output.\n",
    "        if self.affine:\n",
    "            # MJY:: Fuse the multiplication for speed.\n",
    "            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std * self.weight) + _unsqueeze_ft(self.bias)\n",
    "        else:\n",
    "            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std)\n",
    "\n",
    "        # Reshape it.\n",
    "        return output.view(input_shape)\n",
    "\n",
    "    def __data_parallel_replicate__(self, ctx, copy_id):\n",
    "        self._is_parallel = True\n",
    "        self._parallel_id = copy_id\n",
    "\n",
    "        # parallel_id == 0 means master device.\n",
    "        if self._parallel_id == 0:\n",
    "            ctx.sync_master = self._sync_master\n",
    "        else:\n",
    "            self._slave_pipe = ctx.sync_master.register_slave(copy_id)\n",
    "\n",
    "    def _data_parallel_master(self, intermediates):\n",
    "        \"\"\"Reduce the sum and square-sum, compute the statistics, and broadcast it.\"\"\"\n",
    "        intermediates = sorted(intermediates, key=lambda i: i[1].sum.get_device())\n",
    "\n",
    "        to_reduce = [i[1][:2] for i in intermediates]\n",
    "        to_reduce = [j for i in to_reduce for j in i]  # flatten\n",
    "        target_gpus = [i[1].sum.get_device() for i in intermediates]\n",
    "\n",
    "        sum_size = sum([i[1].sum_size for i in intermediates])\n",
    "        sum_, ssum = ReduceAddCoalesced.apply(target_gpus[0], 2, *to_reduce)\n",
    "\n",
    "        mean, inv_std = self._compute_mean_std(sum_, ssum, sum_size)\n",
    "\n",
    "        broadcasted = Broadcast.apply(target_gpus, mean, inv_std)\n",
    "\n",
    "        outputs = []\n",
    "        for i, rec in enumerate(intermediates):\n",
    "            outputs.append((rec[0], _MasterMessage(*broadcasted[i*2:i*2+2])))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _add_weighted(self, dest, delta, alpha=1, beta=1, bias=0):\n",
    "        \"\"\"return *dest* by `dest := dest*alpha + delta*beta + bias`\"\"\"\n",
    "        return dest * alpha + delta * beta + bias\n",
    "\n",
    "    def _compute_mean_std(self, sum_, ssum, size):\n",
    "        \"\"\"Compute the mean and standard-deviation with sum and square-sum. This method\n",
    "        also maintains the moving average on the master device.\"\"\"\n",
    "        assert size > 1, 'BatchNorm computes unbiased standard-deviation, which requires size > 1.'\n",
    "        mean = sum_ / size\n",
    "        sumvar = ssum - sum_ * mean\n",
    "        unbias_var = sumvar / (size - 1)\n",
    "        bias_var = sumvar / size\n",
    "\n",
    "        self._tmp_running_mean = self._add_weighted(self._tmp_running_mean, mean.data, alpha=self._moving_average_fraction)\n",
    "        self._tmp_running_var = self._add_weighted(self._tmp_running_var, unbias_var.data, alpha=self._moving_average_fraction)\n",
    "        self._running_iter = self._add_weighted(self._running_iter, 1, alpha=self._moving_average_fraction)\n",
    "\n",
    "        self.running_mean = self._tmp_running_mean / self._running_iter\n",
    "        self.running_var = self._tmp_running_var / self._running_iter\n",
    "\n",
    "        return mean, bias_var.clamp(self.eps) ** -0.5\n",
    "    \n",
    "class SynchronizedBatchNorm2d(_SynchronizedBatchNorm):\n",
    "    r\"\"\"Applies Batch Normalization over a 4d input that is seen as a mini-batch\n",
    "    of 3d inputs\n",
    "    .. math::\n",
    "        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
    "    This module differs from the built-in PyTorch BatchNorm2d as the mean and\n",
    "    standard-deviation are reduced across all devices during training.\n",
    "    For example, when one uses `nn.DataParallel` to wrap the network during\n",
    "    training, PyTorch's implementation normalize the tensor on each device using\n",
    "    the statistics only on that device, which accelerated the computation and\n",
    "    is also easy to implement, but the statistics might be inaccurate.\n",
    "    Instead, in this synchronized version, the statistics will be computed\n",
    "    over all training samples distributed on multiple devices.\n",
    "    \n",
    "    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n",
    "    as the built-in PyTorch implementation.\n",
    "    The mean and standard-deviation are calculated per-dimension over\n",
    "    the mini-batches and gamma and beta are learnable parameter vectors\n",
    "    of size C (where C is the input size).\n",
    "    During training, this layer keeps a running estimate of its computed mean\n",
    "    and variance. The running sum is kept with a default momentum of 0.1.\n",
    "    During evaluation, this running mean/variance is used for normalization.\n",
    "    Because the BatchNorm is done over the `C` dimension, computing statistics\n",
    "    on `(N, H, W)` slices, it's common terminology to call this Spatial BatchNorm\n",
    "    Args:\n",
    "        num_features: num_features from an expected input of\n",
    "            size batch_size x num_features x height x width\n",
    "        eps: a value added to the denominator for numerical stability.\n",
    "            Default: 1e-5\n",
    "        momentum: the value used for the running_mean and running_var\n",
    "            computation. Default: 0.1\n",
    "        affine: a boolean value that when set to ``True``, gives the layer learnable\n",
    "            affine parameters. Default: ``True``\n",
    "    Shape:\n",
    "        - Input: :math:`(N, C, H, W)`\n",
    "        - Output: :math:`(N, C, H, W)` (same shape as input)\n",
    "    Examples:\n",
    "        >>> # With Learnable Parameters\n",
    "        >>> m = SynchronizedBatchNorm2d(100)\n",
    "        >>> # Without Learnable Parameters\n",
    "        >>> m = SynchronizedBatchNorm2d(100, affine=False)\n",
    "        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45))\n",
    "        >>> output = m(input)\n",
    "    \"\"\"\n",
    "\n",
    "    def _check_input_dim(self, input):\n",
    "        if input.dim() != 4:\n",
    "            raise ValueError('expected 4D input (got {}D input)'\n",
    "                             .format(input.dim()))\n",
    "        super(SynchronizedBatchNorm2d, self)._check_input_dim(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5289bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This HRNet implementation is modified from the following repository:\n",
    "https://github.com/HRNet/HRNet-Semantic-Segmentation\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "BatchNorm2d = SynchronizedBatchNorm2d\n",
    "BN_MOMENTUM = 0.1\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "__all__ = ['hrnetv2']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'hrnetv2': 'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/hrnetv2_w48-imagenet.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.bn3 = BatchNorm2d(planes * self.expansion,\n",
    "                               momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class HighResolutionModule(nn.Module):\n",
    "    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\n",
    "                 num_channels, fuse_method, multi_scale_output=True):\n",
    "        super(HighResolutionModule, self).__init__()\n",
    "        self._check_branches(\n",
    "            num_branches, blocks, num_blocks, num_inchannels, num_channels)\n",
    "\n",
    "        self.num_inchannels = num_inchannels\n",
    "        self.fuse_method = fuse_method\n",
    "        self.num_branches = num_branches\n",
    "\n",
    "        self.multi_scale_output = multi_scale_output\n",
    "\n",
    "        self.branches = self._make_branches(\n",
    "            num_branches, blocks, num_blocks, num_channels)\n",
    "        self.fuse_layers = self._make_fuse_layers()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def _check_branches(self, num_branches, blocks, num_blocks,\n",
    "                        num_inchannels, num_channels):\n",
    "        if num_branches != len(num_blocks):\n",
    "            error_msg = 'NUM_BRANCHES({}) <> NUM_BLOCKS({})'.format(\n",
    "                num_branches, len(num_blocks))\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "        if num_branches != len(num_channels):\n",
    "            error_msg = 'NUM_BRANCHES({}) <> NUM_CHANNELS({})'.format(\n",
    "                num_branches, len(num_channels))\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "        if num_branches != len(num_inchannels):\n",
    "            error_msg = 'NUM_BRANCHES({}) <> NUM_INCHANNELS({})'.format(\n",
    "                num_branches, len(num_inchannels))\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\n",
    "                         stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or \\\n",
    "           self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.num_inchannels[branch_index],\n",
    "                          num_channels[branch_index] * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                BatchNorm2d(num_channels[branch_index] * block.expansion,\n",
    "                            momentum=BN_MOMENTUM),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.num_inchannels[branch_index],\n",
    "                            num_channels[branch_index], stride, downsample))\n",
    "        self.num_inchannels[branch_index] = \\\n",
    "            num_channels[branch_index] * block.expansion\n",
    "        for i in range(1, num_blocks[branch_index]):\n",
    "            layers.append(block(self.num_inchannels[branch_index],\n",
    "                                num_channels[branch_index]))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n",
    "        branches = []\n",
    "\n",
    "        for i in range(num_branches):\n",
    "            branches.append(\n",
    "                self._make_one_branch(i, block, num_blocks, num_channels))\n",
    "\n",
    "        return nn.ModuleList(branches)\n",
    "\n",
    "    def _make_fuse_layers(self):\n",
    "        if self.num_branches == 1:\n",
    "            return None\n",
    "\n",
    "        num_branches = self.num_branches\n",
    "        num_inchannels = self.num_inchannels\n",
    "        fuse_layers = []\n",
    "        for i in range(num_branches if self.multi_scale_output else 1):\n",
    "            fuse_layer = []\n",
    "            for j in range(num_branches):\n",
    "                if j > i:\n",
    "                    fuse_layer.append(nn.Sequential(\n",
    "                        nn.Conv2d(num_inchannels[j],\n",
    "                                  num_inchannels[i],\n",
    "                                  1,\n",
    "                                  1,\n",
    "                                  0,\n",
    "                                  bias=False),\n",
    "                        BatchNorm2d(num_inchannels[i], momentum=BN_MOMENTUM)))\n",
    "                elif j == i:\n",
    "                    fuse_layer.append(None)\n",
    "                else:\n",
    "                    conv3x3s = []\n",
    "                    for k in range(i-j):\n",
    "                        if k == i - j - 1:\n",
    "                            num_outchannels_conv3x3 = num_inchannels[i]\n",
    "                            conv3x3s.append(nn.Sequential(\n",
    "                                nn.Conv2d(num_inchannels[j],\n",
    "                                          num_outchannels_conv3x3,\n",
    "                                          3, 2, 1, bias=False),\n",
    "                                BatchNorm2d(num_outchannels_conv3x3,\n",
    "                                            momentum=BN_MOMENTUM)))\n",
    "                        else:\n",
    "                            num_outchannels_conv3x3 = num_inchannels[j]\n",
    "                            conv3x3s.append(nn.Sequential(\n",
    "                                nn.Conv2d(num_inchannels[j],\n",
    "                                          num_outchannels_conv3x3,\n",
    "                                          3, 2, 1, bias=False),\n",
    "                                BatchNorm2d(num_outchannels_conv3x3,\n",
    "                                            momentum=BN_MOMENTUM),\n",
    "                                nn.ReLU(inplace=True)))\n",
    "                    fuse_layer.append(nn.Sequential(*conv3x3s))\n",
    "            fuse_layers.append(nn.ModuleList(fuse_layer))\n",
    "\n",
    "        return nn.ModuleList(fuse_layers)\n",
    "\n",
    "    def get_num_inchannels(self):\n",
    "        return self.num_inchannels\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.num_branches == 1:\n",
    "            return [self.branches[0](x[0])]\n",
    "\n",
    "        for i in range(self.num_branches):\n",
    "            x[i] = self.branches[i](x[i])\n",
    "\n",
    "        x_fuse = []\n",
    "        for i in range(len(self.fuse_layers)):\n",
    "            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n",
    "            for j in range(1, self.num_branches):\n",
    "                if i == j:\n",
    "                    y = y + x[j]\n",
    "                elif j > i:\n",
    "                    width_output = x[i].shape[-1]\n",
    "                    height_output = x[i].shape[-2]\n",
    "                    y = y + F.interpolate(\n",
    "                        self.fuse_layers[i][j](x[j]),\n",
    "                        size=(height_output, width_output),\n",
    "                        mode='bilinear',\n",
    "                        align_corners=False)\n",
    "                else:\n",
    "                    y = y + self.fuse_layers[i][j](x[j])\n",
    "            x_fuse.append(self.relu(y))\n",
    "\n",
    "        return x_fuse\n",
    "\n",
    "\n",
    "blocks_dict = {\n",
    "    'BASIC': BasicBlock,\n",
    "    'BOTTLENECK': Bottleneck\n",
    "}\n",
    "class Conv2dReLU(nn.Sequential):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            padding=0,\n",
    "            stride=1,\n",
    "            use_batchnorm=True,\n",
    "    ):\n",
    "\n",
    "        if use_batchnorm == \"inplace\" and InPlaceABN is None:\n",
    "            raise RuntimeError(\n",
    "                \"In order to use `use_batchnorm='inplace'` inplace_abn package must be installed. \"\n",
    "                + \"To install see: https://github.com/mapillary/inplace_abn\"\n",
    "            )\n",
    "\n",
    "        conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=not (use_batchnorm),\n",
    "        )\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        if use_batchnorm == \"inplace\":\n",
    "            bn = InPlaceABN(out_channels, activation=\"leaky_relu\", activation_param=0.0)\n",
    "            relu = nn.Identity()\n",
    "\n",
    "        elif use_batchnorm and use_batchnorm != \"inplace\":\n",
    "            bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        else:\n",
    "            bn = nn.Identity()\n",
    "\n",
    "        super(Conv2dReLU, self).__init__(conv, bn, relu)\n",
    "        \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            use_batchnorm=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2dReLU(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        self.conv2 = Conv2dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x        \n",
    "relu_inplace = True\n",
    "class HRNetV2(nn.Module):\n",
    "    def __init__(self, n_class, **kwargs):\n",
    "        super(HRNetV2, self).__init__()\n",
    "        extra = {\n",
    "            'STAGE1': {'NUM_MODULES': 1, 'NUM_BRANCHES': 1, 'BLOCK': 'BOTTLENECK', 'NUM_BLOCKS': (4), 'NUM_CHANNELS': (64), 'FUSE_METHOD': 'SUM'},\n",
    "            'STAGE2': {'NUM_MODULES': 1, 'NUM_BRANCHES': 2, 'BLOCK': 'BASIC', 'NUM_BLOCKS': (4, 4), 'NUM_CHANNELS': (48, 96), 'FUSE_METHOD': 'SUM'},\n",
    "            'STAGE3': {'NUM_MODULES': 4, 'NUM_BRANCHES': 3, 'BLOCK': 'BASIC', 'NUM_BLOCKS': (4, 4, 4), 'NUM_CHANNELS': (48, 96, 192), 'FUSE_METHOD': 'SUM'},\n",
    "            'STAGE4': {'NUM_MODULES': 3, 'NUM_BRANCHES': 4, 'BLOCK': 'BASIC', 'NUM_BLOCKS': (4, 4, 4, 4), 'NUM_CHANNELS': (48, 96, 192, 384), 'FUSE_METHOD': 'SUM'},\n",
    "            'FINAL_CONV_KERNEL': 1\n",
    "            }\n",
    "        ALIGN_CORNERS = False\n",
    "        relu_inplace = True\n",
    "        # stem net\n",
    "        # stem net\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn2 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        \n",
    "        self.stage1_cfg = extra['STAGE1']\n",
    "        num_channels = self.stage1_cfg['NUM_CHANNELS']\n",
    "        block = blocks_dict[self.stage1_cfg['BLOCK']]\n",
    "        num_blocks = self.stage1_cfg['NUM_BLOCKS']\n",
    "        self.layer1 = self._make_layer(block, 64, num_channels, num_blocks)\n",
    "        stage1_out_channel = block.expansion*num_channels\n",
    "        \n",
    "        \n",
    "        self.stage2_cfg = extra['STAGE2']\n",
    "        num_channels = self.stage2_cfg['NUM_CHANNELS']\n",
    "        block = blocks_dict[self.stage2_cfg['BLOCK']]\n",
    "        num_channels = [\n",
    "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
    "        self.transition1 = self._make_transition_layer(\n",
    "            [stage1_out_channel], num_channels)\n",
    "        self.stage2, pre_stage_channels = self._make_stage(\n",
    "            self.stage2_cfg, num_channels)\n",
    "\n",
    "        self.stage3_cfg = extra['STAGE3']\n",
    "        num_channels = self.stage3_cfg['NUM_CHANNELS']\n",
    "        block = blocks_dict[self.stage3_cfg['BLOCK']]\n",
    "        num_channels = [\n",
    "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
    "        self.transition2 = self._make_transition_layer(\n",
    "            pre_stage_channels, num_channels)\n",
    "        self.stage3, pre_stage_channels = self._make_stage(\n",
    "            self.stage3_cfg, num_channels)\n",
    "\n",
    "        self.stage4_cfg = extra['STAGE4']\n",
    "        num_channels = self.stage4_cfg['NUM_CHANNELS']\n",
    "        block = blocks_dict[self.stage4_cfg['BLOCK']]\n",
    "        num_channels = [\n",
    "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
    "        self.transition3 = self._make_transition_layer(\n",
    "            pre_stage_channels, num_channels)\n",
    "        self.stage4, pre_stage_channels = self._make_stage(\n",
    "            self.stage4_cfg, num_channels, multi_scale_output=True)\n",
    "        \n",
    "        last_inp_channels = np.int(np.sum(pre_stage_channels))\n",
    "        self.up1 = DecoderBlock(720, 512)\n",
    "        self.up2 = DecoderBlock(512, n_class)\n",
    "        \n",
    "        self.last_layer = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=last_inp_channels,\n",
    "                out_channels=last_inp_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0),\n",
    "            BatchNorm2d(last_inp_channels, momentum=BN_MOMENTUM),\n",
    "            nn.ReLU(inplace=relu_inplace),\n",
    "            nn.Conv2d(\n",
    "                in_channels=last_inp_channels,\n",
    "                out_channels=n_class,\n",
    "                kernel_size=extra['FINAL_CONV_KERNEL'],\n",
    "                stride=1,\n",
    "                padding=1 if extra['FINAL_CONV_KERNEL'] == 3 else 0)\n",
    "        )\n",
    "        \n",
    "    def _make_transition_layer(\n",
    "            self, num_channels_pre_layer, num_channels_cur_layer):\n",
    "        num_branches_cur = len(num_channels_cur_layer)\n",
    "        num_branches_pre = len(num_channels_pre_layer)\n",
    "\n",
    "        transition_layers = []\n",
    "        for i in range(num_branches_cur):\n",
    "            if i < num_branches_pre:\n",
    "                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n",
    "                    transition_layers.append(nn.Sequential(\n",
    "                        nn.Conv2d(num_channels_pre_layer[i],\n",
    "                                  num_channels_cur_layer[i],\n",
    "                                  3,\n",
    "                                  1,\n",
    "                                  1,\n",
    "                                  bias=False),\n",
    "                        BatchNorm2d(\n",
    "                            num_channels_cur_layer[i], momentum=BN_MOMENTUM),\n",
    "                        nn.ReLU(inplace=relu_inplace)))\n",
    "                else:\n",
    "                    transition_layers.append(None)\n",
    "            else:\n",
    "                conv3x3s = []\n",
    "                for j in range(i+1-num_branches_pre):\n",
    "                    inchannels = num_channels_pre_layer[-1]\n",
    "                    outchannels = num_channels_cur_layer[i] \\\n",
    "                        if j == i-num_branches_pre else inchannels\n",
    "                    conv3x3s.append(nn.Sequential(\n",
    "                        nn.Conv2d(\n",
    "                            inchannels, outchannels, 3, 2, 1, bias=False),\n",
    "                        BatchNorm2d(outchannels, momentum=BN_MOMENTUM),\n",
    "                        nn.ReLU(inplace=relu_inplace)))\n",
    "                transition_layers.append(nn.Sequential(*conv3x3s))\n",
    "\n",
    "        return nn.ModuleList(transition_layers)\n",
    "\n",
    "    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(inplanes, planes, stride, downsample))\n",
    "        inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_stage(self, layer_config, num_inchannels,\n",
    "                    multi_scale_output=True):\n",
    "        num_modules = layer_config['NUM_MODULES']\n",
    "        num_branches = layer_config['NUM_BRANCHES']\n",
    "        num_blocks = layer_config['NUM_BLOCKS']\n",
    "        num_channels = layer_config['NUM_CHANNELS']\n",
    "        block = blocks_dict[layer_config['BLOCK']]\n",
    "        fuse_method = layer_config['FUSE_METHOD']\n",
    "\n",
    "        modules = []\n",
    "        for i in range(num_modules):\n",
    "            # multi_scale_output is only used last module\n",
    "            if not multi_scale_output and i == num_modules - 1:\n",
    "                reset_multi_scale_output = False\n",
    "            else:\n",
    "                reset_multi_scale_output = True\n",
    "            modules.append(\n",
    "                HighResolutionModule(num_branches,\n",
    "                                      block,\n",
    "                                      num_blocks,\n",
    "                                      num_inchannels,\n",
    "                                      num_channels,\n",
    "                                      fuse_method,\n",
    "                                      reset_multi_scale_output)\n",
    "            )\n",
    "            num_inchannels = modules[-1].get_num_inchannels()\n",
    "\n",
    "        return nn.Sequential(*modules), num_inchannels\n",
    "\n",
    "    def forward(self, x, return_feature_maps=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "\n",
    "        x_list = []\n",
    "        for i in range(self.stage2_cfg['NUM_BRANCHES']):\n",
    "            if self.transition1[i] is not None:\n",
    "                x_list.append(self.transition1[i](x))\n",
    "            else:\n",
    "                x_list.append(x)\n",
    "        y_list = self.stage2(x_list)\n",
    "\n",
    "        x_list = []\n",
    "        for i in range(self.stage3_cfg['NUM_BRANCHES']):\n",
    "            if self.transition2[i] is not None:\n",
    "                if i < self.stage2_cfg['NUM_BRANCHES']:\n",
    "                    x_list.append(self.transition2[i](y_list[i]))\n",
    "                else:\n",
    "                    x_list.append(self.transition2[i](y_list[-1]))\n",
    "            else:\n",
    "                x_list.append(y_list[i])\n",
    "        y_list = self.stage3(x_list)\n",
    "\n",
    "        x_list = []\n",
    "        for i in range(self.stage4_cfg['NUM_BRANCHES']):\n",
    "            if self.transition3[i] is not None:\n",
    "                if i < self.stage3_cfg['NUM_BRANCHES']:\n",
    "                    x_list.append(self.transition3[i](y_list[i]))\n",
    "                else:\n",
    "                    x_list.append(self.transition3[i](y_list[-1]))\n",
    "            else:\n",
    "                x_list.append(y_list[i])\n",
    "        x = self.stage4(x_list)\n",
    "\n",
    "        # Upsampling\n",
    "        x0_h, x0_w = x[0].size(2), x[0].size(3)\n",
    "        x1 = F.interpolate(x[1], size=(x0_h, x0_w), mode='bilinear', align_corners=False)\n",
    "        x2 = F.interpolate(x[2], size=(x0_h, x0_w), mode='bilinear', align_corners=False)\n",
    "        x3 = F.interpolate(x[3], size=(x0_h, x0_w), mode='bilinear', align_corners=False)\n",
    "\n",
    "        x = torch.cat([x[0], x1, x2, x3], 1)\n",
    "\n",
    "        #x = self.up2(self.up1(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def hrnetv2(pretrained=False, **kwargs):\n",
    "    model = HRNetV2(n_class=2, **kwargs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e93e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "import collections\n",
    "import threading\n",
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "#model = hrnetv2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13df8596",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = eval(f'smp.create_model(arch=\"{\"FullAxialUnet\"}\", encoder_name=\"{\"resnet18\"}\", encoder_weights=\"imagenet\", in_channels={3}, classes = {2}, image_size = {256})').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48af99b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_in = torch.randn(2, 3, 256, 256).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8703cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(dummy_in).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1539c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77bcacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "li = [{'Id': 1, 'ParentId': 0, 'ClassificationSubtype': False, 'Label': 'Base ROI', 'Color': 'rgba(0, 0, 0, 0.0)', 'Tools': [{'Name': 'iam_base_roi', 'Parameters': {'invert_result': False, 'min_intensity': 28.2, 'smoothing': 23.55, 'minsize': 5, 'paramLabels': {'invert_result': 'Invert Base ROI', 'min_intensity': 'Minimum Intensity (%)', 'smoothing': 'Smoothing', 'minsize': 'Minimum Size (%)'}}}], 'SelectedToolName': None, 'Dynamic': False, 'HasChild': False, 'IsSubtype': False, 'SubtypeLevel': 0, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': [], 'DefaultSelected': False}, {'Id': 2, 'ParentId': 0, 'ClassificationSubtype': False, 'Label': 'Tumor Cells', 'Color': '#e6194b', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': True, 'IsSubtype': False, 'SubtypeLevel': 0, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 4, 'ParentId': 2, 'ClassificationSubtype': True, 'Label': 'Positive', 'Color': '#ffe119', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 1, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 3, 'ParentId': 2, 'ClassificationSubtype': True, 'Label': 'Negative', 'Color': '#3cb44b', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 1, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 5, 'ParentId': 0, 'ClassificationSubtype': False, 'Label': 'Immun Cells', 'Color': '#4363d8', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': True, 'IsSubtype': False, 'SubtypeLevel': 0, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 7, 'ParentId': 5, 'ClassificationSubtype': True, 'Label': 'Lymphozyten', 'Color': '#911eb4', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': True, 'IsSubtype': True, 'SubtypeLevel': 1, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 11, 'ParentId': 7, 'ClassificationSubtype': True, 'Label': 'Lymphozyten +', 'Color': '#fabebe', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 2, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 12, 'ParentId': 7, 'ClassificationSubtype': True, 'Label': 'Lymphozyten -', 'Color': '#008080', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 2, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 6, 'ParentId': 5, 'ClassificationSubtype': True, 'Label': 'Granulozyten', 'Color': '#f58231', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': True, 'IsSubtype': True, 'SubtypeLevel': 1,'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 14, 'ParentId': 6, 'ClassificationSubtype': True, 'Label': 'Granulozyten +', 'Color': '#9a6324', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 2, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 13, 'ParentId': 6, 'ClassificationSubtype': True, 'Label': 'Granulozyten -', 'Color': '#e6beff', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 2, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 10, 'ParentId': 5, 'ClassificationSubtype': True, 'Label': 'Dendritische Zellen', 'Color': '#bcf60c', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 1, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 9, 'ParentId': 5, 'ClassificationSubtype': True, 'Label': 'Makrophagen', 'Color': '#f032e6', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 1, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 8, 'ParentId': 5, 'ClassificationSubtype': True, 'Label': 'Plasmazellen', 'Color': '#46f0f0', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': True, 'SubtypeLevel': 1, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}, {'Id': 15, 'ParentId': 0, 'ClassificationSubtype': False, 'Label': 'Grid', 'Color': '#00000000', 'Tools': [], 'SelectedToolName': None, 'Dynamic': True, 'HasChild': False, 'IsSubtype': False, 'SubtypeLevel': 0, 'CriticalClass': 0, 'CriticalPercentage': 0, 'ClassFrequencies': {'Class_0': 0, 'Class_1': 0, 'Class_2': 0, 'Class_3': 0, 'Class_4': 0, 'Class_5': 0, 'Class_6': 0}, 'AvgClassFrequency': 0, 'ToolParams': None, 'DefaultSelected': False}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373d0a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in list(filter(lambda x: x['HasChild'], li)):\n",
    "    if layer['ParentId'] == idd:\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e1a149",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt, layer in enumerate(list(filter(lambda x: not x['HasChild'] and x['ParentId'] == selectedId, li))):\n",
    "    print(layer['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b0429",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedId"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7395f6d",
   "metadata": {},
   "source": [
    "# Paul Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf2f0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "fi = cv2.imread(r\"E:\\source\\repos\\VisualDL\\custom_experiments\\2.png\")\n",
    "se = cv2.imread(r\"E:\\source\\repos\\VisualDL\\custom_experiments\\3.png\")\n",
    "res = cv2.absdiff(fi, se)\n",
    "res = res.astype(np.uint8)\n",
    "#--- find percentage difference based on number of pixels that are not zero ---\n",
    "percentage = (np.count_nonzero(res) * 100)/ res.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343a8acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01da989",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = cv2.subtract(cv2.imread(r\"E:\\source\\repos\\VisualDL\\custom_experiments\\2.png\"), cv2.imread(r\"E:\\source\\repos\\VisualDL\\custom_experiments\\3.png\"))\n",
    "difference = cv2.cvtColor(difference, cv2.COLOR_BGR2GRAY)\n",
    "difference[difference>0]=255\n",
    "print(1 - (np.count_nonzero(difference) / (1920*1080)))\n",
    "\n",
    "cv2.imwrite(\"Xd.png\", difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a3aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(im1, im2):\n",
    "    res = cv2.absdiff(im1, im2)\n",
    "    res = res.astype(np.uint8)\n",
    "    return (np.count_nonzero(res) * 100)/ res.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbc1df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video = cv2.VideoCapture(r\"C:\\Users\\phili\\Downloads\\ALM scam.mp4\")\n",
    "frames_per_second = int(input_video.get(cv2.CAP_PROP_FPS))\n",
    "frame_width = int(input_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(input_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6ecd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcd687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "last_image = None\n",
    "threshold = 0.9991\n",
    "# Loop through all the frames in the video\n",
    "\n",
    "while 1:\n",
    "    # Read the video to retrieve individual frames. 'frame' will reference the inidivdual frames read from the video.\n",
    "    ret, frame = input_video.read()\n",
    "    if not ret:\n",
    "        print('Processed all frames')\n",
    "        break\n",
    "    if last_image is None:\n",
    "        last_image = frame\n",
    "        continue\n",
    "    \n",
    "    di = diff(frame, last_image)\n",
    "    last_image = frame\n",
    "    if di > threshold:\n",
    "        count += 1\n",
    "        cv2.imwrite(f\"{count}.png\", frame)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab14dc32",
   "metadata": {},
   "source": [
    "# Convert MakeSenseAi Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c798da6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "image_dir = r\"C:\\Users\\phili\\Videos\\Call of Duty  Modern Warfare 2019\\trainnig\"\n",
    "csv_file = r\"C:\\Users\\phili\\Videos\\Call of Duty  Modern Warfare 2019\\labels_my-project-name_2021-07-31-07-17-36.csv\"\n",
    "label_dir = \"labels\"\n",
    "with open(csv_file, \"r\", encoding = \"utf-8\") as handle:\n",
    "    lines = handle.readlines()\n",
    "files = {}\n",
    "for line in lines:\n",
    "    x_min, y_min, width, height = [int(x) for x in line.split(\",\")[1:5]]\n",
    "    image_width = int(line.split(\",\")[-2])\n",
    "    image_height = int(line.split(\",\")[-1])\n",
    "    name = line.split(\",\")[-3]\n",
    "    \n",
    "    middle_x = (x_min + width / 2) / image_width\n",
    "    middle_y = (y_min + height / 2) / image_height\n",
    "    width /= image_width\n",
    "    height /= image_height\n",
    "    if not name in files:\n",
    "        files[name] = [f\"0 {middle_x} {middle_y} {width} {height}\\n\"]\n",
    "    else:\n",
    "        files[name].append(f\"0 {middle_x} {middle_y} {width} {height}\\n\")\n",
    "all_images = os.listdir(image_dir)   \n",
    "no_annotations = list(set(all_images) - set(files.keys()))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bdc6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in files.items():\n",
    "    with open(os.path.join(label_dir, key).replace(\".png\", \".txt\"), \"w\") as handle:\n",
    "        for it in val:\n",
    "            handle.write(it)\n",
    "for file in no_annotations:\n",
    "    with open(os.path.join(label_dir, file).replace(\".png\", \".txt\"), \"w\") as handle:\n",
    "        handle.write(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103986cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = r\"F:\\source\\repos\\Daten\\HER-N\\Pdl1Combined\\Tumor Cells 512\\train\\labels\"\n",
    "co = set()\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "for file in os.listdir(start):\n",
    "    im = cv2.imread(os.path.join(start, file), 0)\n",
    "    un = np.unique(im)\n",
    "    for u in un:\n",
    "        co.add(u)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f080b253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "np.count_nonzero(cv2.imread(r\"F:\\source\\repos\\VisualDL\\custom_experiments\\bayer\\valid\\images\\1_ 1_07__1s_0r_412x_824y_.png\", 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a54a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.load(r\"../maskrcnn.pt\") #load your model  \n",
    "#t is a simple python dictionary\n",
    "t['custom_data'] = {\"structure_indices\" : [15], \"image_size\": 1024, \"object_based\" : False, \"new_key\": \"irgendwas\"} #Change the custom_data dictionary to whatever you want and add keys if you want\n",
    "torch.save(t, \"changed_keys_model.pt\") #save the new model in file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26736ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56eb57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.load(r\"../maskrcnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00556bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "t['custom_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32024869",
   "metadata": {},
   "outputs": [],
   "source": [
    "t['custom_data'] = {\"structure_indices\" : [15], \"image_size\": 1024, \"object_based\" : False, \"new_key\": \"irgendwas\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db69afd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t['custom_data']['structure_indices'] = [4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b6a0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(t, \"changed_keys_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a37ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "t['custom_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f57412",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.load(r\"001.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baa5282",
   "metadata": {},
   "outputs": [],
   "source": [
    "t['custom_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d953a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7421ceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Test\\Cells128\\train\\labels\"\n",
    "for im in os.listdir(start):\n",
    "    img = io.imread(os.path.join(start, im), as_gray = True).astype(np.float32)\n",
    "    print(np.unique(img))\n",
    "\n",
    "    cv2.imwrite(os.path.join(start, im), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6a5f63",
   "metadata": {},
   "source": [
    "# Single class per contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a497d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45b8e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06bff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(r\"F:\\source\\repos\\Daten\\Nuclei\\Nuclei\\Nuclei\\val\\images\\06_PAS_1_8911_5853.png\", 0)\n",
    "orig = img.copy()\n",
    "img[img > 0] = 255\n",
    "\n",
    "contours, hierarchy = cv2.findContours(image=img, mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE)\n",
    "for i, cnt in enumerate(contours):        \n",
    "    area = cv2.contourArea(cnt)         \n",
    "    #if area > threshold_area:\n",
    "    \n",
    "    mask = np.zeros_like(orig)\n",
    "    cv2.drawContours(mask, [cnt], -1, 255, -1)\n",
    "    cv2.imwrite(f\"dsa{i}dsa.png\",mask)\n",
    "\n",
    "    pts = np.where(mask == 255)\n",
    "    vals = np.unique(orig[pts[0], pts[1]], return_counts = True)\n",
    "  \n",
    "    orig[pts[0], pts[1]] = vals[0][np.argmax(vals[1])]\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d862714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"orig.png\", orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd6a94f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b90313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualdl import vdl\n",
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.cvtColor(cv2.imread(r\"F:\\source\\repos\\Daten\\Nuclei\\Nuclei\\Nuclei\\val\\images\\06_PAS_1_8911_5853.png\"), cv2.COLOR_BGR2RGB)\n",
    "img = cv2.resize(img, (512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2098ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = vdl.get_inference_model(r\"C:\\Users\\phili\\Documents\\002.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f80d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pred.predict([img], True, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e4ccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"preds1.png\", preds[0]*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dd804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddaf80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6b02d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25350e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337e61db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path=r\"F:\\source\\repos\\hsayolo\\runs\\train\\exp5\\weights\\best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b0c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from visualdl import vdl\n",
    "imgs = [cv2.imread(r\"F:\\source\\repos\\Daten\\ObjectDetection\\Her1\\train\\images\\PD-L1=2_0_41328-42312_75.png\")[..., ::-1]]\n",
    "model = vdl.get_inference_model(r\"F:\\source\\repos\\hsayolo\\runs\\train\\exp5\\weights\\best.pt\", type=\"od\")\n",
    "print(model.predict(imgs))\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8e5065",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(imgs, size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe245c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds.xyxy[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d600392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(weightpath, imgs, confidence = 0.45):\n",
    "    model = torch.hub.load('ultralytics/yolov5', 'custom', path=weightpath, force_reload=True)\n",
    "    size = imgs[0].shape[0]\n",
    "    model.conf = confidence\n",
    "    preds = model(imgs, size=size)\n",
    "    finals = []\n",
    "    for cnt, img in enumerate(imgs):\n",
    "        tmp = []\n",
    "        boxes = preds.xyxy[cnt]\n",
    "        for box in boxes:\n",
    "            middlex = int(box[0] + (box[2] - box[0]) / 2)\n",
    "            middley = int(box[1] + (box[3] - box[1]) / 2)\n",
    "            data = list(box.detach().cpu().numpy())\n",
    "            data.append((middlex, middley))\n",
    "            tmp.append(tuple(data))\n",
    "        finals.append(tmp)\n",
    "    return finals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83841932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "imgs = [cv2.imread(r\"F:\\source\\repos\\Daten\\ObjectDetection\\Her1\\train\\images\\PD-L1=2_0_41328-42312_75.png\")[..., ::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edaa91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualdl import vdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1031513",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = vdl.get_inference_model(r\"F:\\source\\repos\\hsayolo\\runs\\train\\exp5\\weights\\001.pt\", type=\"od\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5569eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vdl.get_inference_model(r\"F:\\source\\repos\\VisualDL\\tu-resnet18, UnetPlusPlus.pt\", type=\"segmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34538d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.predict(imgs, confidence = 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8caceb5",
   "metadata": {},
   "source": [
    "# Create valid folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec111f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from shutil import copyfile\n",
    "from random import shuffle\n",
    "#copyfile(src, dst)\n",
    "folder = r\"F:\\source\\repos\\Daten\\knochenpartikel\\knochenpartikel\"\n",
    "to = r\"F:\\source\\repos\\Daten\\knochenpartikel\\knochenpartikel\\valid\"\n",
    "all_files = os.listdir(os.path.join(folder, \"images\"))\n",
    "shuffle(all_files)\n",
    "le = int(len(all_files) * 0.1)\n",
    "all_images = all_images[:le]\n",
    "for file in all_images:\n",
    "    image = os.path.join(folder, \"images\")\n",
    "    label = os.path.join(folder, \"labels\")\n",
    "    image = os.path.join(image, file)\n",
    "    label = os.path.join(label, file)\n",
    "    #print(os.path.join(os.path.join(to, \"images\"), image))\n",
    "    #print(os.path.join(os.path.join(to, \"images\"), file))\n",
    "    copyfile(image, os.path.join(os.path.join(to, \"images\"), file))\n",
    "    copyfile(label, os.path.join(os.path.join(to, \"labels\"), file))\n",
    "    os.remove(image)\n",
    "    os.remove(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b73211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5705c249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad687010",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load(r\"F:\\source\\repos\\VisualDL\\tu-resnest50d, UnetPlusPlus.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ec7eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"validation_metrics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8610c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dog:\n",
    "    def __init__(self):\n",
    "        s = \"s\"\n",
    "        self.a = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1557a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Dog()\n",
    "for i in range(5):\n",
    "    a.a.append(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5221a357",
   "metadata": {},
   "outputs": [],
   "source": [
    "del a.a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afa6881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2799df",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np.delete(np.array(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dbbbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21afa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = cv2.imread(r\"F:\\source\\repos\\VisualDL\\0a.png\",0)\n",
    "a = cv2.distanceTransform(a, cv2.DIST_L2, 5)\n",
    "a = cv2.normalize(a, a, 0, 1.0, cv2.NORM_MINMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161200cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd.png\", a * 255.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bb00ba",
   "metadata": {},
   "source": [
    "# Create distance map for each object 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459b329",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells her2 128\\train\\labels\\05__1_3120-9167_15.png\", 0) *255.\n",
    "to = np.zeros_like(img, dtype=np.float32)\n",
    "img = img.astype(np.uint8)\n",
    "orig = img.copy()\n",
    "img[img > 0] = 255\n",
    "contours, hierarchy = cv2.findContours(image=img, mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE)\n",
    "for i, cnt in enumerate(contours):        \n",
    "    area = cv2.contourArea(cnt) \n",
    "    mask = np.zeros_like(orig)\n",
    "    cv2.drawContours(mask, [cnt], -1, 255, -1)\n",
    "    dist = cv2.distanceTransform(mask, cv2.DIST_L2, 5)\n",
    "    ab = cv2.normalize(dist, dist, 0, 1.0, cv2.NORM_MINMAX)\n",
    "    ab[ab < 0.7] = 0\n",
    "    pts = np.where(ab > 0)\n",
    "    to[pts[0], pts[1]] = ab[pts[0], pts[1]]\n",
    "to = to*255.\n",
    "cv2.imwrite(\"xd.png\", to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3030dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4390240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file = r\"../visualdl/dependencies/yolov5/data/coco128.yaml\"\n",
    "with open(yaml_file, \"r\",  encoding = \"utf-8\") as handle:\n",
    "    a = yaml.load(handle, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cf1d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a['train'] = r\"F:\\source\\repos\\Daten\\ObjectDetection\\tumor\\train\"\n",
    "a['val'] = r\"F:\\source\\repos\\Daten\\ObjectDetection\\tumor\\valid\"\n",
    "a['names'] = [\"xd\", \"bc\", \"bd\", \"ab\"]\n",
    "a['nc'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62754097",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"../visualdl/dependencies/yolov5/data/hsa.yaml\", 'w') as yaml_file:\n",
    "    yaml.dump(a, yaml_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93be3a12",
   "metadata": {},
   "source": [
    "# Test parse argmeunts from code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843c4635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('img_size', type=int, default = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f770cf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430dc33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abc(opt):\n",
    "    print(opt.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82767518",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc(parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63660a47",
   "metadata": {},
   "source": [
    "# Bring mask into yolo format on the fly\n",
    " The starting point for this are the labels that are used to extract the contours and finally the bounding boxes of those.\n",
    " After that they only have to be put in the right format for yolo and saved in an appropiate folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8fd0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe40cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "train_folder = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells her2 256\\train\"\n",
    "valid_folder = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells her2 256\\valid\"\n",
    "test_folder = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells her2 256\\valid\"\n",
    "def create_files(folder, nc = 1, single_class = False):\n",
    "    start = os.path.join(folder, \"labels\")\n",
    "    all_files = os.listdir(start)\n",
    "    if single_class:\n",
    "        nc = 1\n",
    "    for img in os.listdir(start):\n",
    "        for i in range(0, nc):\n",
    "            im = cv2.imread(os.path.join(start, img), 0)\n",
    "            #kernel = np.ones((2, 2), np.uint8)\n",
    "            #im = cv.erode(im, kernel)\n",
    "            #im = cv.dilate(im, kernel)\n",
    "            tmp = im.copy()\n",
    "            if not single_class:\n",
    "                tmp[tmp != (i+1)] = 0\n",
    "                tmp[tmp == (i+1)] = 255\n",
    "            else:\n",
    "                tmp[tmp > 0] = 255\n",
    "            contours,hierachy = cv2.findContours(tmp, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            blank = np.zeros_like(tmp)\n",
    "            for cnt, cont in enumerate(contours):\n",
    "                xmin,ymin,width,height = cv2.boundingRect(cont)\n",
    "                #cv.rectangle(im,(x,y),(x+width,y+height),(255),1)\n",
    "                #cv2.imwrite(\"xd.png\", im)\n",
    "                image_width = im.shape[0]\n",
    "                xcenter, ycenter = xmin + width/2, ymin + height/2\n",
    "                xcenter, ycenter, width, height = xcenter/image_width, ycenter/image_width, width/image_width, height/image_width\n",
    "                if not img in files:\n",
    "                    files[img] = [(str(i),str(xcenter), str(ycenter), str(width), str(height))]\n",
    "                else:\n",
    "                    files[img] += [(str(i),str(xcenter), str(ycenter), str(width), str(height))]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9df1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_files(train_folder, 4 , False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06740a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"labels\")\n",
    "for cnt, (key, val) in enumerate(files.items()):\n",
    "    with open(\"labels/\" + key.replace(\".png\", \".txt\"), \"w\") as handle:\n",
    "        for va in val:\n",
    "            handle.write(\" \".join(list(va))+ \"\\n\") \n",
    "            \n",
    "for name in no_anno:\n",
    "    with open(\"labels/\" + name.replace(\".png\", \".txt\"), \"w\") as handle:\n",
    "        handle.write(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6994bb9d",
   "metadata": {},
   "source": [
    "# Test ob inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ba4f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualdl import vdl \n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage import morphology\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.segmentation import watershed\n",
    "from torch import nn\n",
    "import torch\n",
    "import os\n",
    "from skimage.feature import peak_local_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a403afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inf = vdl.get_inference_model(r\"F:\\source\\repos\\VisualDL\\runs\\exp110\\weights\\best.pt\",type = \"od\")\n",
    "#inf = vdl.get_inference_model(r\"F:\\source\\repos\\VisualDL\\maskrcnn.pt\",type = \"instance\")\n",
    "inf2 = vdl.get_inference_model(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\001 (3).pt\")\n",
    "#inf3 = vdl.get_inference_model(r\"C:\\Users\\phili\\Documents\\001.pt\",type = \"segmentation_od\", watershed_od = r\"F:\\source\\repos\\VisualDL\\runs\\exp14\\weights\\001.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce71b1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89dc924",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf2.state['validation_metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44cf4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea648cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(pretrained=True)\n",
    "target_layers = [model.layer4[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ce1342",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc5cab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf2.model.encoder.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0020d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualdl.utils.model_utils import visualize\n",
    "import torch\n",
    "import cv2\n",
    "from visualdl import vdl\n",
    "from pytorch_grad_cam import GradCAM, AblationCAM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57006cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = torch.load(r\"F:\\source\\repos\\VisualDL\\maskrcnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdcd3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b9d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf2 = vdl.get_inference_model(r\"F:\\source\\repos\\VisualDL\\tu-resnest50d, UnetPlusPlus.pt\")\n",
    "inf3 = vdl.get_inference_model(r\"F:\\source\\repos\\VisualDL\\maskrcnn.pt\", type = \"instance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ce55d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf3.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa287cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = cv2.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\NecroseTumor\\dataset\\valid\\images\\07__1s_0r_624x_156y_.png\")[:,:,::-1]\n",
    "rgb = cv2.resize(rgb, (128,128))\n",
    "xd = torch.tensor(rgb, dtype = torch.float).permute(2, 0, 1)\n",
    "#vis = visualize(inf2.model, inf2.model.encoder.model['layer4'][-2], xd)\n",
    "maps = inf2.predict([rgb])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb319f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = cv2.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset (12)\\dataset\\valid\\images\\08__1s_0r_1416x_2124y_.png\", 0) * 255.\n",
    "#08__1s_0r_1416x_2124y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103b7775",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd.png\", rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1d30c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd.png\", maps * 50.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31ee84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cam = GradCAM(model=inf2.model, target_layer=inf2.model.encoder.model['layer4'][-1], use_cuda=False)\n",
    "#im = cam(input_tensor=xd.unsqueeze(0), target_category = 0)\n",
    "cam = GradCAM(model=inf2.model, target_layer=inf2.model.decoder.blocks.x_0_4, use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad9983",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = cv2.applyColorMap(np.uint8(255 * cam(input_tensor=xd.unsqueeze(0), target_category = None)[0,:]), cv2.COLORMAP_JET)\n",
    "#heatmap = np.float32(heatmap) / 255\n",
    "# cam = heatmap + xd.permute(1,2,0).numpy()\n",
    "# cam = cam / np.max(cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a08aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = heatmap + cv2.cvtColor(maps.astype(np.uint8), cv2.COLOR_GRAY2RGB)\n",
    "u = u / np.max(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe443ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = cv2.addWeighted(heatmap, 0.5, cv2.cvtColor(maps.astype(np.uint8), cv2.COLOR_GRAY2RGB) * 50, 0.7, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a36c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd5.png\", u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f371c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(maps, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20282bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = cv2.imread(r\"D:\\Hsa\\temp\\projects\\168714ca-2337-44cb-b384-36215f416198\\dataset\\valid\\images\\05__1s_0r_2304x_1920y_.png\")[:,:,::-1]\n",
    "rgb = cv2.resize(rgb, (512,512))\n",
    "maps = inf2.predict([rgb])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c70f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd6.png\", maps * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbd2a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e1a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = cv2.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\imgs\\imgs\\3_img.png\")[:,:,::-1]\n",
    "rgb = cv2.resize(rgb, (512,512))\n",
    "maps = inf2.predict([rgb])[0][0]\n",
    "box_prediction = inf.predict([rgb], confidence = 0.5)[0]\n",
    "\n",
    "label_map = np.zeros_like(maps).astype(np.int32)\n",
    "p = 1\n",
    "for b in box_prediction:\n",
    "    cv2.circle\n",
    "    label_map = cv2.circle(label_map, b[-1], 1, 255, -1)\n",
    "    p += 1\n",
    "\n",
    "cv2.imwrite(\"xd2.png\", label_map + cv2.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\imgs\\imgs\\3_img.png\", 0))\n",
    "# print(map.shape)\n",
    "#rgb_mask = cv2.cvtColor(maps.astype(np.float32) * 10., cv2.COLOR_GRAY2RGB).astype(np.uint8)\n",
    "#cv2.imwrite(\"xd.png\", rgb_mask + cv2.cvtColor(label_map.astype(np.float32), cv2.COLOR_GRAY2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = cv2.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset_14810all\\dataset\\train\\images\\05__1s_0r_0x_256y_.png\")[:,:,::-1]\n",
    "rgb = cv2.resize(rgb, (512,512))\n",
    "maps = inf.predict([rgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce599c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd1.png\", maps * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e4595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, sure_fg = cv2.threshold(maps,0.7*maps.max(),255,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d39ad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd1.png\", sure_fg * 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d61d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_mask = cv2.cvtColor(maps.astype(np.uint8), cv2.COLOR_GRAY2RGB)\n",
    "markers = cv2.watershed(rgb_mask, label_map)\n",
    "empty = np.zeros_like(markers).astype(np.uint8)\n",
    "empty[markers == -1] = 255\n",
    "kernel = np.ones((2, 2), np.uint8)\n",
    "labels = cv2.dilate(empty, kernel)\n",
    "maps[labels == 255] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a30627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_greate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928262ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6aeb34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30436e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd1.png\", maps * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2561d715",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(markers, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833090b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb00d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, markers = cv2.connectedComponents(label_map)\n",
    "markers += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772d6949",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "labels = cv2.watershed(rgb_mask, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f085fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "maps[labels == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cf0a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd2.png\", np.float32(maps) * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6d3d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((2, 2), np.uint8)\n",
    "labels = cv2.erode(np.uint8(labels), kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406408ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096d5c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c77ca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = cv2.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\DatasetViktorFixed\\dataset\\train\\labels\\05__1_0256_1536.png\") * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a00e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd.png\", cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d006602",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a015f4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = rgb.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dbe809",
   "metadata": {},
   "outputs": [],
   "source": [
    "for box in test:\n",
    "    cv2.circle(im, box[-1], 1, (255,255,255), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d97a699",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd4.png\", im[:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bfa357",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = cv2.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\datasetClassified\\dataset\\valid\\images\\4.png\")[:,:,::-1]\n",
    "#rgb = cv2.resize(rgb, (512,512))\n",
    "#test = inf3.predict([rgb], confidence = 0.65, fill_holes = False)\n",
    "maps = inf2.predict([rgb])\n",
    "#cv2.imwrite(\"xd.png\", np.float32(test[0]) * 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36035827",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd1.png\", np.float32(maps[0][0]) * 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42265956",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\imgs\\imgs\"\n",
    "folder = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset_14810all\\dataset\\valid\\images\"\n",
    "#folder = r\"D:\\Hsa\\temp\\projects\\713a5ef7-8eb6-4d47-8e63-d70889ea91a2\\dataset\\train\\images\"\n",
    "#folder = r\"D:\\Hsa\\temp\\projects\\acf3ea56-507f-4578-9aa2-5f712a1d24e8\\dataset\\train\\images\"\n",
    "#folder = r\"D:\\Hsa\\temp\\projects\\ef91a20f-f5fb-4fda-b316-fee78913fcb0\\dataset\\train\\images\"\n",
    "#folder = r\"F:\\source\\repos\\Daten\\Nuclei\\Nuclei\\Nuclei\\val\\images\"\n",
    "with_box = False\n",
    "for im in os.listdir(folder):\n",
    "    #image = cv2.imread(os.path.join(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset_14810\\dataset\\train\\labels\", im)) * 255\n",
    "    #cv2.imwrite(os.path.join(\"bayer\", im), image)\n",
    "    rgb = cv2.imread(os.path.join(folder, im))[:,:,::-1]\n",
    "    rgb = cv2.resize(rgb, (512,512))\n",
    "    maps = inf2.predict([rgb])[0][0]\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    #coords = peak_local_max(np.float32(maps[1][0]) * 255., footprint=np.ones((3, 3)), labels=np.int32(maps[0][0]) * 255)\n",
    "    #mask = np.zeros(np.float32(maps[1][0]).shape, dtype=bool)\n",
    "    #mask[tuple(coords.T)] = True\n",
    "    #markers, _ = ndi.label(mask)\n",
    "    #markers[markers > 0] = 255\n",
    "    maps_copy = maps.copy()\n",
    "    if with_box:\n",
    "        box_prediction = inf.predict([rgb], confidence = 0.45)[0]\n",
    "        label_map = np.zeros_like(maps).astype(np.int32)\n",
    "        p = 1\n",
    "        for b in box_prediction:\n",
    "            label_map = cv2.circle(label_map, b[-1], 1, p, -1)\n",
    "            p += 1\n",
    "        rgb_mask = cv2.cvtColor(maps.astype(np.uint8), cv2.COLOR_GRAY2RGB)\n",
    "        markers = cv2.watershed(rgb_mask, label_map)\n",
    "        empty = np.zeros_like(markers).astype(np.uint8)\n",
    "        empty[markers == -1] = 255\n",
    "        kernel = np.ones((2, 2), np.uint8)\n",
    "        labels = cv2.dilate(empty, kernel)\n",
    "        maps[labels == 255] = 0\n",
    "\n",
    "    \n",
    "    #cv2.imwrite(os.path.join(\"bayer\", f\"dis{im}\"), np.float32(markers) * 255.)\n",
    "    cv2.imwrite(os.path.join(\"bayer\", f\"seg_{im}\"), np.float32(maps_copy) * 255.)\n",
    "    if with_box:\n",
    "        cv2.imwrite(os.path.join(\"bayer\", f\"segbox_{im}\"), np.float32(maps) * 255)\n",
    "    #cv2.imwrite(os.path.join(\"bayer\", f\"dis{im}\"), np.float32(maps[1][0]) * 255.)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a879bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = cv2.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\imgs\\imgs\\3_img.png\")[:,:,::-1]\n",
    "rgb = cv2.resize(rgb, (512,512))\n",
    "maps = inf2.predict([rgb])\n",
    "kernel = np.ones((2, 2), np.uint8)\n",
    "cv2.imwrite(\"xd2.png\", np.float32(maps[0][0]) * 255)\n",
    "cv2.imwrite(\"xd4.png\",  cv2.erode(np.float32(maps[0][0]) * 255, kernel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acbba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd1.png\", np.float32(maps[1][0]) * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faca650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage as ndi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71e94d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = maps[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7befff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = np.unique(predictions)\n",
    "all_classes = np.zeros_like(predictions)\n",
    "for val in unique_values:\n",
    "    if val == 0:\n",
    "        continue\n",
    "    print(val)\n",
    "    tmp = np.zeros_like(predictions)\n",
    "    tmp[predictions==val] = 1\n",
    "    tmp = np.uint8(ndi.binary_fill_holes(tmp))\n",
    "    tmp = tmp.astype(np.int32)\n",
    "    all_classes[tmp == 1] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ca173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd.png\", all_classes * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cbdf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.float32(maps[1][0]) * 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67615a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[a < 175] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0981504",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd1.png\", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c4c424",
   "metadata": {},
   "outputs": [],
   "source": [
    "maps[1][0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38da6e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd.png\", np.float32(maps[1][0]) * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e58b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = cv2.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells her2 256\\valid\\images\\05__1_5352-10677_170.png\")#[:,:,::-1]\n",
    "orig = rgb.copy()\n",
    "box_image = np.zeros_like(rgb)\n",
    "box_image = cv2.cvtColor(box_image, cv2.COLOR_BGR2GRAY)\n",
    "boxes = inf.predict([rgb[:,:,::-1]], confidence = 0.45)[0]\n",
    "maps = inf2.predict([rgb[:,:,::-1]])\n",
    "dist = maps[1][0]\n",
    "maps = maps[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fb1807",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca82a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "maps[maps == 1] = 50\n",
    "maps[maps == 2] = 100\n",
    "maps[maps == 3] = 150\n",
    "maps[maps == 4] = 200\n",
    "cv2.imwrite(\"xd2.png\", maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53cc54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = np.int32(np.zeros_like(maps))\n",
    "p = 1\n",
    "used = []\n",
    "for box in boxes:\n",
    "    tmp = np.random.randint(10, 255)\n",
    "    while tmp in used:\n",
    "        tmp = np.random.randint(10, 255)\n",
    "    used.append(tmp)\n",
    "    label_map = cv2.circle(label_map, box[-1], 1, tmp, -1)\n",
    "    p += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c249826",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d689bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapss = np.uint8(ndi.binary_fill_holes(maps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2f4733",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(mapss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dc74af",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = ndi.distance_transform_edt(maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aebda49",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = watershed(-distance, label_map, mask=maps, watershed_line = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b401d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "watershed_lines = np.zeros(shape=np.shape(labels))\n",
    "#watershed_lines(labels==0)=1 # ws lines are labeled as 0 in markers\n",
    "#watershed_lines_thick = dilation(bright_pixel, square(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03bfdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "watershed_lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126bf61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdd = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f3b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdd = cv2.add(np.uint8(labels), np.uint8(label_map * 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6f80ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((2, 2), np.uint8)\n",
    "import cv2\n",
    "# Using cv2.erode() method \n",
    "#xdd = cv2.erode(xdd, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ab04f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, len(np.unique(maps)) + 1):\n",
    "    maps[maps == i] = i +15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1246079",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd1.png\", orig)\n",
    "cv2.imwrite(\"xd3.png\", xdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885e362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9f8127",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdd = np.uint8(xdd)\n",
    "maps = np.uint8(maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf5c9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdd[xdd > 0] = maps[xdd>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb5beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd.png\", xdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69184ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute(arr):\n",
    "    import numpy as np\n",
    "    from scipy.ndimage import label, binary_dilation\n",
    "    from collections import Counter\n",
    "    imputed_array = np.copy(arr)\n",
    "\n",
    "    mask = np.isnan(arr)\n",
    "    labels, count = label(mask)\n",
    "    for idx in range(1, count + 1):\n",
    "        hole = labels == idx\n",
    "        surrounding_values = arr[binary_dilation(hole) & ~hole]\n",
    "        most_frequent = Counter(surrounding_values).most_common(1)[0][0]\n",
    "        imputed_array[hole] = most_frequent\n",
    "\n",
    "    return imputed_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147efa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdd = impute(xdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35dcb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdd[xdd == 1] = 25\n",
    "xdd[xdd == 2] = 75\n",
    "xdd[xdd == 3] = 125\n",
    "xdd[xdd == 4] = 150\n",
    "xdd[xdd == 5] = 250\n",
    "cv2.imwrite(\"xd2.png\", xdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f94ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(xdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cce3a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "from skimage.segmentation import watershed\n",
    "from skimage.feature import peak_local_max\n",
    "\n",
    "\n",
    "# Generate an initial image with two overlapping circles\n",
    "x, y = np.indices((80, 80))\n",
    "x1, y1, x2, y2 = 28, 28, 44, 52\n",
    "r1, r2 = 16, 20\n",
    "mask_circle1 = (x - x1)**2 + (y - y1)**2 < r1**2\n",
    "mask_circle2 = (x - x2)**2 + (y - y2)**2 < r2**2\n",
    "image = np.logical_or(mask_circle1, mask_circle2)\n",
    "\n",
    "# Now we want to separate the two objects in image\n",
    "# Generate the markers as local maxima of the distance to the background\n",
    "distance = ndi.distance_transform_edt(image)\n",
    "coords = peak_local_max(distance, footprint=np.ones((3, 3)), labels=image)\n",
    "mask = np.zeros(distance.shape, dtype=bool)\n",
    "mask[tuple(coords.T)] = True\n",
    "markers, _ = ndi.label(mask)\n",
    "labels = watershed(-distance, markers, mask=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e984ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22dfc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fe9c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ce88d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea22dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705871d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = np.uint8(maps[0] * 255)\n",
    "#kernel = np.ones((2,2),np.uint8)\n",
    "#sure_bg = cv2.dilate(maps,kernel,iterations=3)\n",
    "sure_bg = maps\n",
    "\n",
    "sure_fg = np.uint8(maps)\n",
    "unknown = cv2.subtract(sure_bg,sure_fg)\n",
    "ret, markers = cv2.connectedComponents(sure_fg)\n",
    "markers = box_image\n",
    "# Add one to all labels so that sure background is not 0, but 1\n",
    "markers = markers+1\n",
    "markers[unknown==255] = 0\n",
    "markers = cv2.watershed(orig,markers)\n",
    "orig[markers == -1] = [255,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6c9234",
   "metadata": {},
   "source": [
    "# watershedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a9ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "from skimage.segmentation import watershed\n",
    "from skimage.feature import peak_local_max\n",
    "\n",
    "\n",
    "# Generate an initial image with two overlapping circles\n",
    "x, y = np.indices((80, 80))\n",
    "x1, y1, x2, y2 = 28, 28, 44, 52\n",
    "r1, r2 = 16, 20\n",
    "mask_circle1 = (x - x1)**2 + (y - y1)**2 < r1**2\n",
    "mask_circle2 = (x - x2)**2 + (y - y2)**2 < r2**2\n",
    "image = np.logical_or(mask_circle1, mask_circle2)\n",
    "\n",
    "# Now we want to separate the two objects in image\n",
    "# Generate the markers as local maxima of the distance to the background\n",
    "distance = ndi.distance_transform_edt(image)\n",
    "coords = peak_local_max(distance, footprint=np.ones((3, 3)), labels=image)\n",
    "mask = np.zeros(distance.shape, dtype=bool)\n",
    "mask[tuple(coords.T)] = True\n",
    "markers, _ = ndi.label(mask)\n",
    "labels = watershed(-distance, markers, mask=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325e6694",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(9, 3), sharex=True, sharey=True)\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(image, cmap=plt.cm.gray)\n",
    "ax[0].set_title('Overlapping objects')\n",
    "ax[1].imshow(-distance, cmap=plt.cm.gray)\n",
    "ax[1].set_title('Distances')\n",
    "ax[2].imshow(labels, cmap=plt.cm.nipy_spectral)\n",
    "ax[2].set_title('Separated objects')\n",
    "\n",
    "for a in ax:\n",
    "    a.set_axis_off()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3b49f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(distance, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50e6c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9905c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualdl import vdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b8f09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdl.train(r\"F:\\source\\repos\\VisualDL\\visualdl\\trainer\\detection\\detection.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4c70da",
   "metadata": {},
   "source": [
    "# Roche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978f505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a6cada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c79a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "count_zero = 0\n",
    "count_one = 0\n",
    "start = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\ROCHE\\dataset\\train\\labels\"\n",
    "all_files = os.listdir(start)\n",
    "nc = 2\n",
    "cl = 0\n",
    "for img in os.listdir(start):\n",
    "    for i in range(1, nc):\n",
    "        im = cv.imread(os.path.join(start, img), 0)\n",
    "        co = im.copy()\n",
    "        im[im > 0] = 1.0\n",
    "        kernel = np.ones((2, 2), np.uint8)\n",
    "        #im = cv.erode(im, kernel)\n",
    "        #im = cv.dilate(im, kernel)\n",
    "        tmp = im.copy()\n",
    "        #tmp[tmp != i] = 0\n",
    "        #tmp[tmp == i] = 255\n",
    "        tmp[tmp > 0] = 255\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        tmp = cv2.dilate(tmp, kernel)\n",
    "        #cv2.imwrite(\"xd.png\", tmp)\n",
    "        \n",
    "        \n",
    "        #if img == \"05__1_3130-9263_11.png\":\n",
    "        #    cv2.imwrite(\"xd.png\", tmp)\n",
    "        contours,hierachy = cv.findContours(tmp, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        for cnt, cont in enumerate(contours):\n",
    "            blank = np.zeros_like(tmp)\n",
    "            xmin,ymin,width,height = cv.boundingRect(cont)\n",
    "            if width <= 3 or height <= 3:\n",
    "                continue\n",
    "            cv2.drawContours(blank, [cont], -1, 255, -1)\n",
    "            #cv.rectangle(im,(x,y),(x+width,y+height),(255),1)\n",
    "            #cv2.imwrite(\"xd.png\", im)\n",
    "            image_height = im.shape[0]\n",
    "            image_width = im.shape[1]\n",
    "            pts = np.where(blank > 0)\n",
    "            \n",
    "            if len(np.unique(co[pts[0], pts[1]])) == 2:\n",
    "                cl = 0\n",
    "                count_zero += 1\n",
    "            else:\n",
    "                cl = 1\n",
    "                count_one += 1\n",
    "                \n",
    "\n",
    "            xcenter, ycenter = xmin + width/2, ymin + height/2\n",
    "            xcenter, ycenter, width, height = xcenter/image_width, ycenter/image_height, width/image_width, height/image_height\n",
    "            if not img in files:\n",
    "                files[img] = [(str(cl),str(xcenter), str(ycenter), str(width), str(height))]\n",
    "            else:\n",
    "\n",
    "                files[img] += [(str(cl),str(xcenter), str(ycenter), str(width), str(height))]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec2884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in files.items():\n",
    "    base = cv.imread(os.path.join(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\ROCHE\\dataset\\train/images\", key))\n",
    "    label = cv.imread(os.path.join(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\ROCHE\\dataset\\train/labels\", key))\n",
    "    for cnt, rec in enumerate(val):\n",
    "        xcenter,ycenter,width,height = [float(xx) for xx in rec[1:]]\n",
    "        xcenter *= base.shape[1]\n",
    "        width *= base.shape[1]\n",
    "        ycenter *= base.shape[0]\n",
    "        height *= base.shape[0]\n",
    "        xcenter = int(xcenter)\n",
    "        ycenter = int(ycenter)\n",
    "        width = int(width)\n",
    "        height = int(height)\n",
    "        x = int(xcenter - width/2)\n",
    "        y = int(ycenter - height/2)\n",
    "        va = int((int(rec[0])+1) * 100)\n",
    "        #cv.circle(base, (xcenter,ycenter), 1, (255,255,255), -1)\n",
    "        cv.rectangle(base,(int(x),int(y)),(int(x+width),int(y+height)),(va,va,va),1)\n",
    "        xd1 = base[y:y+height, x:x+width]\n",
    "        xd2 = label[y:y+height, x:x+width]\n",
    "        cv.imwrite(os.path.join(r\"F:\\source\\repos\\VisualDL\\custom_experiments\\CutTrainingData\\images\", f\"{cnt}{key}.png\"),xd1)\n",
    "        cv.imwrite(os.path.join(r\"F:\\source\\repos\\VisualDL\\custom_experiments\\CutTrainingData\\labels\", f\"{cnt}{key}.png\"),xd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f5c1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "base = cv.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\ROCHE\\dataset\\valid/images\\A065 - 20211005_111945_0.png\")\n",
    "\n",
    "for cnt, rec in enumerate(files['A065 - 20211005_111945_0.png']):\n",
    "    xcenter,ycenter,width,height = [float(xx) for xx in rec[1:]]\n",
    "    xcenter *= base.shape[1]\n",
    "    width *= base.shape[1]\n",
    "    ycenter *= base.shape[0]\n",
    "    height *= base.shape[0]\n",
    "    xcenter = int(xcenter)\n",
    "    ycenter = int(ycenter)\n",
    "    width = int(width)\n",
    "    height = int(height)\n",
    "\n",
    "    x = int(xcenter - width/2)\n",
    "    y = int(ycenter - height/2)\n",
    "    va = int((int(rec[0])+1) * 100)\n",
    "\n",
    "    #cv.circle(base, (xcenter,ycenter), 1, (255,255,255), -1)\n",
    "    cv.rectangle(base,(int(x),int(y)),(int(x+width),int(y+height)),(va,va,va),1)\n",
    "    xd = base[y:y+height, x:x+width]\n",
    "    cv.imwrite(f\"{cnt}.png\", xd)\n",
    "label = cv.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\ROCHE\\dataset\\valid\\labels\\A065 - 20211005_111945_0.png\")\n",
    "cv2.imwrite(\"xd5.png\", label * 100)\n",
    "cv.imwrite(\"xd4.png\", base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64393df",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"labels\")\n",
    "for cnt, (key, val) in enumerate(files.items()):\n",
    "    with open(\"labels/\" + key.replace(\".png\", \".txt\"), \"w\") as handle:\n",
    "        for va in val:\n",
    "            handle.write(\" \".join(list(va))+ \"\\n\") \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82042cd1",
   "metadata": {},
   "source": [
    "# od inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5985d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualdl import vdl\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef031c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf = vdl.get_inference_model(r\"F:\\source\\repos\\VisualDL\\runs\\exp62\\weights\\best.pt\",type = \"od\")\n",
    "inf2 = vdl.get_inference_model(r\"F:\\source\\repos\\VisualDL\\tu-resnest50d, UnetPlusPlus.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a223f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_folder = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\ROCHE\\dataset\\valid/images\"\n",
    "for c, im in enumerate(os.listdir(val_folder)):   \n",
    "    label = cv2.imread(os.path.join(val_folder.replace(\"images\", \"labels\"), im))\n",
    "    rgb = cv2.imread(os.path.join(val_folder, im))[:,:,::-1]\n",
    "    rgb = cv2.resize(rgb, (512,512))\n",
    "    label = cv2.resize(label, (512,512))\n",
    "    test = inf.predict([rgb], confidence = 0.35)[0]\n",
    "    maps = inf2.predict([rgb])[0][0]\n",
    "    to_remove = []\n",
    "    for box in test:\n",
    "        for b in test:\n",
    "            if box != b:\n",
    "                cnt = 0\n",
    "                for i in range(4):\n",
    "                    if abs(box[i] - b[i]) <= 5:\n",
    "                        cnt += 1\n",
    "                if cnt == 4:\n",
    "                    if box[-3] < b[-3]:\n",
    "                        to_remove.append(b)\n",
    "                    else:\n",
    "                        to_remove.append(box)\n",
    "    to_remove = list(set(to_remove))\n",
    "    #for val in to_remove:\n",
    "    #    test.remove(val)\n",
    "    for box in test:\n",
    "        print(box)\n",
    "        if box[-2] == 1.0:\n",
    "            val = (255,0,0)\n",
    "        else:\n",
    "            val = (0,255,0)\n",
    "\n",
    "    cv2.imwrite(f\"label{im}.png\", label * 150)\n",
    "    cv2.imwrite(f\"seg{im}.png\", maps * 50)\n",
    "    cv2.imwrite(f\"{im}.png\", rgb[:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c91fe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.rectangle(base,(int(x),int(y)),(int(x+width),int(y+height)),(va,va,va),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c5598a",
   "metadata": {},
   "source": [
    "# Roche inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d1697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506fb8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a6d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualdl import vdl \n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage import morphology\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.segmentation import watershed\n",
    "from torch import nn\n",
    "import torch\n",
    "import os\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "inf2 = vdl.get_inference_model(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\ROCHE\\001.pt\")\n",
    "val_folder = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\ROCHE\\dataset\\valid/images\"\n",
    "for c, im in enumerate(os.listdir(val_folder)):   \n",
    "    img = cv2.imread(os.path.join(val_folder, im))[:,:,::-1]\n",
    "    \n",
    "    img = cv2.resize(img, (512,512))\n",
    "    orig = img.copy()\n",
    "    maps = inf2.predict([img])[0][0]\n",
    "    \n",
    "    liquid = (maps == 1).astype('uint8')\n",
    "    bubble = (maps == 2).astype('uint8')\n",
    "    \n",
    "    contours_liquid, hierarchy = cv2.findContours(\n",
    "        liquid, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    contours_bubble, hierarchy = cv2.findContours(\n",
    "        bubble, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    overlay = img.copy()\n",
    "    cv2.fillPoly(overlay, contours_liquid, (255, 0, 0))\n",
    "    cv2.fillPoly(overlay, contours_bubble, (0, 0, 255))\n",
    "    cv2.addWeighted(overlay, 0.4, img, 1 - 0.4,\n",
    "\t\t0, img)\n",
    "    fig = plt.figure(figsize=(15, 12))\n",
    "    rows = 1\n",
    "    columns = 3\n",
    "    fig.add_subplot(rows, columns, 1)\n",
    "  \n",
    "    # showing image\n",
    "    plt.imshow(orig)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Original\")\n",
    "    \n",
    "    fig.add_subplot(rows, columns, 2)\n",
    "  \n",
    "    # showing image\n",
    "    plt.imshow(maps)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Prediction\")\n",
    "    \n",
    "    fig.add_subplot(rows, columns, 3)\n",
    "  \n",
    "    # showing image\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Overlayed\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc632fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93c4bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd.png\", cv2.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\datasetClassified\\dataset\\train\\labels\\05__1_256_0.png\") * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c07747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\WirklichNeu\\dataset\\valid\\images\"\n",
    "\n",
    "for cnt, im in enumerate(os.listdir(folder)):\n",
    "    os.rename(os.path.join(folder, im), os.path.join(folder, f\"big{cnt}.png\"))\n",
    "    #cv2.imwrite(os.path.join(\"255\", im), cv2.imread(os.path.join(folder, im)) * 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e7f416",
   "metadata": {},
   "source": [
    "# Test marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e42fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.morphology import skeletonize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7071002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_map_fixed(mask):\n",
    "    #mask[mask > 0] = 255 #set all classes to the same value\n",
    "    distances = []\n",
    "    for cnt2, img in enumerate(mask):\n",
    "        to = np.zeros_like(img, dtype=np.float32)\n",
    "        contours, hierarchy = cv2.findContours(image=img, mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE)\n",
    "        for i, cnt in enumerate(contours):\n",
    "            mask2 = np.zeros_like(img)\n",
    "            cv2.drawContours(mask2, [cnt], -1, 1, -1)\n",
    "            skeleton = skeletonize(mask2)\n",
    "            dist = cv2.distanceTransform(mask2, cv2.DIST_L2, 5)\n",
    "            ab = cv2.normalize(dist, dist, 0, 1.0, cv2.NORM_MINMAX)\n",
    "            pts = np.where(skeleton > 0)\n",
    "            to[pts[0], pts[1]] = ab[pts[0], pts[1]]\n",
    "        distances.append(to)\n",
    "        \n",
    "    a = np.stack(np.array(distances), axis = 0)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ee3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_input = cv2.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset_14810all\\dataset\\valid\\labels\\05__1s_0r_0x_768y_.png\", 0).astype(np.uint8) * 255\n",
    "marker_input = marker_input * 255\n",
    "im = cv2.imread(r\"F:\\source\\repos\\VisualDL\\custom_experiments\\bayer\\seg_05__1s_0r_0x_768y_.png\", 0).astype(np.uint8) * 255\n",
    "#im = im * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1754be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((2, 2), 'uint8')\n",
    "bb = cv2.dilate(im, kernel, iterations=3)\n",
    "sure_fg = bb.copy()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a41fc76",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15076d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = get_distance_map_fixed([marker_input])[0]\n",
    "tt[tt < 0.4] = 0\n",
    "tt[tt > 0] = 255\n",
    "ret, markers = cv2.connectedComponents(tt.astype(np.uint8))\n",
    "#markers += 1\n",
    "#markers = tt.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b79b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_mask = cv2.cvtColor(sure_fg.astype(np.uint8) * 255, cv2.COLOR_GRAY2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bb3031",
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = cv2.watershed(rgb_mask, markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556ed77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = np.zeros_like(markers).astype(np.uint8)\n",
    "empty[markers == -1] = 255\n",
    "kernel = np.ones((2, 2), np.uint8)\n",
    "labels = cv2.dilate(empty, kernel)\n",
    "sure_fg[empty == 255] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45690fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"orig.png\", im * 255.)\n",
    "cv2.imwrite(\"marker_input.png\", tt * 255)\n",
    "cv2.imwrite(\"segmentation.png\", sure_fg * 255)\n",
    "cv2.imwrite(\"original_image.png\", marker_input * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ab0630",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(markers, return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e25bf27",
   "metadata": {},
   "source": [
    "# Caranet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ef3883",
   "metadata": {},
   "outputs": [],
   "source": [
    "class caranet(nn.Module):\n",
    "    def __init__(self, channel=32):\n",
    "        super().__init__()\n",
    "        \n",
    "         # ---- ResNet Backbone ----\n",
    "        self.resnet = res2net101_v1b_26w_4s(pretrained=True)\n",
    "\n",
    "        # Receptive Field Block\n",
    "        self.rfb2_1 = Conv(512, 32,3,1,padding=1,bn_acti=True)\n",
    "        self.rfb3_1 = Conv(1024, 32,3,1,padding=1,bn_acti=True)\n",
    "        self.rfb4_1 = Conv(2048, 32,3,1,padding=1,bn_acti=True)\n",
    "\n",
    "        # Partial Decoder\n",
    "        self.agg1 = aggregation(channel)\n",
    "        \n",
    "        self.CFP_1 = CFPModule(32, d = 8)\n",
    "        self.CFP_2 = CFPModule(32, d = 8)\n",
    "        self.CFP_3 = CFPModule(32, d = 8)\n",
    "        ###### dilation rate 4, 62.8\n",
    "\n",
    "\n",
    "\n",
    "        self.ra1_conv1 = Conv(32,32,3,1,padding=1,bn_acti=True)\n",
    "        self.ra1_conv2 = Conv(32,32,3,1,padding=1,bn_acti=True)\n",
    "        self.ra1_conv3 = Conv(32,1,3,1,padding=1,bn_acti=True)\n",
    "        \n",
    "        self.ra2_conv1 = Conv(32,32,3,1,padding=1,bn_acti=True)\n",
    "        self.ra2_conv2 = Conv(32,32,3,1,padding=1,bn_acti=True)\n",
    "        self.ra2_conv3 = Conv(32,1,3,1,padding=1,bn_acti=True)\n",
    "        \n",
    "        self.ra3_conv1 = Conv(32,32,3,1,padding=1,bn_acti=True)\n",
    "        self.ra3_conv2 = Conv(32,32,3,1,padding=1,bn_acti=True)\n",
    "        self.ra3_conv3 = Conv(32,1,3,1,padding=1,bn_acti=True)\n",
    "        \n",
    "        self.aa_kernel_1 = AA_kernel(32,32)\n",
    "        self.aa_kernel_2 = AA_kernel(32,32)\n",
    "        self.aa_kernel_3 = AA_kernel(32,32)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)      # bs, 64, 88, 88\n",
    "        \n",
    "        # ----------- low-level features -------------\n",
    "        \n",
    "        x1 = self.resnet.layer1(x)      # bs, 256, 88, 88\n",
    "        x2 = self.resnet.layer2(x1)     # bs, 512, 44, 44\n",
    "        \n",
    "        x3 = self.resnet.layer3(x2)     # bs, 1024, 22, 22\n",
    "        x4 = self.resnet.layer4(x3)     # bs, 2048, 11, 11\n",
    "        \n",
    "        x2_rfb = self.rfb2_1(x2) # 512 - 32\n",
    "        x3_rfb = self.rfb3_1(x3) # 1024 - 32\n",
    "        x4_rfb = self.rfb4_1(x4) # 2048 - 32\n",
    "        \n",
    "        decoder_1 = self.agg1(x4_rfb, x3_rfb, x2_rfb) # 1,44,44\n",
    "        lateral_map_1 = F.interpolate(decoder_1, scale_factor=8, mode='bilinear')\n",
    "        \n",
    "        # ------------------- atten-one -----------------------\n",
    "        decoder_2 = F.interpolate(decoder_1, scale_factor=0.25, mode='bilinear')\n",
    "        cfp_out_1 = self.CFP_3(x4_rfb) # 32 - 32\n",
    "        decoder_2_ra = -1*(torch.sigmoid(decoder_2)) + 1\n",
    "        aa_atten_3 = self.aa_kernel_3(cfp_out_1)\n",
    "        aa_atten_3_o = decoder_2_ra.expand(-1, 32, -1, -1).mul(aa_atten_3)\n",
    "        \n",
    "        ra_3 = self.ra3_conv1(aa_atten_3_o) # 32 - 32\n",
    "        ra_3 = self.ra3_conv2(ra_3) # 32 - 32\n",
    "        ra_3 = self.ra3_conv3(ra_3) # 32 - 1\n",
    "        \n",
    "        x_3 = ra_3 + decoder_2\n",
    "        lateral_map_2 = F.interpolate(x_3,scale_factor=32,mode='bilinear')\n",
    "        \n",
    "        # ------------------- atten-two -----------------------      \n",
    "        decoder_3 = F.interpolate(x_3, scale_factor=2, mode='bilinear')\n",
    "        cfp_out_2 = self.CFP_3(x3_rfb) # 32 - 32\n",
    "        decoder_3_ra = -1*(torch.sigmoid(decoder_3)) + 1\n",
    "        aa_atten_2 = self.aa_kernel_2(cfp_out_2)\n",
    "        aa_atten_2_o = decoder_3_ra.expand(-1, 32, -1, -1).mul(aa_atten_2)\n",
    "        \n",
    "        ra_2 = self.ra2_conv1(aa_atten_2_o) # 32 - 32\n",
    "        ra_2 = self.ra2_conv2(ra_2) # 32 - 32\n",
    "        ra_2 = self.ra2_conv3(ra_2) # 32 - 1\n",
    "        \n",
    "        x_2 = ra_2 + decoder_3\n",
    "        lateral_map_3 = F.interpolate(x_2,scale_factor=16,mode='bilinear')        \n",
    "        \n",
    "        # ------------------- atten-three -----------------------\n",
    "        decoder_4 = F.interpolate(x_2, scale_factor=2, mode='bilinear')\n",
    "        cfp_out_3 = self.CFP_1(x2_rfb) # 32 - 32\n",
    "        decoder_4_ra = -1*(torch.sigmoid(decoder_4)) + 1\n",
    "        aa_atten_1 = self.aa_kernel_1(cfp_out_3)\n",
    "        aa_atten_1_o = decoder_4_ra.expand(-1, 32, -1, -1).mul(aa_atten_1)\n",
    "        \n",
    "        ra_1 = self.ra1_conv1(aa_atten_1_o) # 32 - 32\n",
    "        ra_1 = self.ra1_conv2(ra_1) # 32 - 32\n",
    "        ra_1 = self.ra1_conv3(ra_1) # 32 - 1\n",
    "        \n",
    "        x_1 = ra_1 + decoder_4\n",
    "        lateral_map_5 = F.interpolate(x_1,scale_factor=8,mode='bilinear') \n",
    "        \n",
    "\n",
    "\n",
    "        return lateral_map_5,lateral_map_3,lateral_map_2,lateral_map_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054fadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "start = r\"C:/Users/phili/Downloads/Telegram Desktop/MegNeueDaten/dataset/valid/labels\"\n",
    "all_files = os.listdir(start)\n",
    "nc = 2\n",
    "for img in os.listdir(start):\n",
    "    for i in range(1, nc):\n",
    "        im = cv.imread(os.path.join(start, img), 0)\n",
    "        im[im > 0] = 1.0\n",
    "        kernel = np.ones((2, 2), np.uint8)\n",
    "        #im = cv.erode(im, kernel)\n",
    "        #im = cv.dilate(im, kernel)\n",
    "        tmp = im.copy()\n",
    "        tmp[tmp != i] = 0\n",
    "        tmp[tmp == i] = 255\n",
    "        #if img == \"05__1_3130-9263_11.png\":\n",
    "        #    cv2.imwrite(\"xd.png\", tmp)\n",
    "        contours,hierachy = cv.findContours(tmp, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "        blank = np.zeros_like(tmp)\n",
    "        for cnt, cont in enumerate(contours):\n",
    "            xmin,ymin,width,height = cv.boundingRect(cont)\n",
    "            if width <= 3 or height <= 3:\n",
    "                continue\n",
    "            #cv.rectangle(im,(x,y),(x+width,y+height),(255),1)\n",
    "            #cv2.imwrite(\"xd.png\", im)\n",
    "            image_width = im.shape[0]\n",
    "            xcenter, ycenter = xmin + width/2, ymin + height/2\n",
    "            xcenter, ycenter, width, height = xcenter/image_width, ycenter/image_width, width/image_width, height/image_width\n",
    "            if not img in files:\n",
    "                files[img] = [(str(i - 1),str(xcenter), str(ycenter), str(width), str(height))]\n",
    "            else:\n",
    "\n",
    "                files[img] += [(str(i - 1),str(xcenter), str(ycenter), str(width), str(height))]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4e53f3",
   "metadata": {},
   "source": [
    "# Create maskrcnn dataset from\n",
    "- https://www.kaggle.com/rluethy/sartorius-torch-mask-r-cnn\n",
    "- https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239f26e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d628e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = io.imread(r\"D:\\Hsa\\temp\\projects\\168714ca-2337-44cb-b384-36215f416198\\dataset\\train\\labels\\05__1s_0r_0x_4532y_.png\", as_gray = True)\n",
    "tmp = label.copy()\n",
    "tmp[tmp > 0] = 1\n",
    "contours,hierachy = cv2.findContours(tmp, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "boxes = []\n",
    "labels = []\n",
    "masks = [np.zeros_like(label)] * len(contours)\n",
    "#image_id = torch.tensor([idx])\n",
    "area = []\n",
    "is_crowd = [0] * len(contours)\n",
    "for cnt, cont in enumerate(contours):\n",
    "    rect = cv2.boundingRect(cont)\n",
    "    xmin, ymin, xmax, ymax = rect[0], rect[1], rect[0] + rect[2], rect[1] + rect[3]\n",
    "    rect = (xmin, ymin, xmax, ymax)\n",
    "    mask2 = np.zeros_like(tmp)\n",
    "    cv2.drawContours(mask2, [cont], -1, 255, -1)\n",
    "    pts = np.where(mask2 > 0)\n",
    "    _cls = label[pts[0], pts[1]]\n",
    "    cls = np.unique(_cls)[np.argmax(np.unique(_cls, return_counts = True)[1])]\n",
    "    is_crowd = 0\n",
    "    area = (rect[3] - rect[1]) * (rect[2] - rect[0])\n",
    "    mask2[pts[0], pts[1]] = label[pts[0], pts[1]]\n",
    "    boxes.append(rect)\n",
    "    labels.append(cls)\n",
    "    masks[cnt] = mask2\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ced2dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd.png\", masks[32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54737b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84506c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "folder = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\ddddddddddddd\\dataset\\train\\labels\"\n",
    "out_folder = \"bayer2\"\n",
    "for im in os.listdir(folder):\n",
    "    image_255 = cv2.imread(os.path.join(folder, im), 0) * 255.\n",
    "    cv2.imwrite(os.path.join(out_folder, im), image_255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eabba6b",
   "metadata": {},
   "source": [
    "# maskrcnn inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624ee96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "import cv2\n",
    "from torchvision.ops.misc import Conv2d\n",
    "import random\n",
    "from visualdl import vdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e5b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False, box_detections_per_img = 800)\n",
    "# #model.roi_heads.mask_predictor.mask_fcn_logits = Conv2d(256, 5, 1)\n",
    "# model.load_state_dict(torch.load(r\"F:\\source\\repos\\VisualDL\\Version3/maskrcnn.pt\")['model_state_dict'])\n",
    "# model.eval()\n",
    "\n",
    "model = vdl.get_inference_model(r\"F:\\source\\repos\\VisualDL\\Version3/maskrcnn.pt\").model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52dc93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = io.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\AbsolutFinal\\dataset\\valid\\images\\06__1s_2r_224x_448y_.png\").astype(np.float32)\n",
    "img = img / 255.\n",
    "inp = torch.tensor(img, dtype=torch.float).permute(2,0,1).unsqueeze(0)\n",
    "with torch.cuda.amp.autocast():\n",
    "    out = model(inp)  #ab hier\n",
    "th = 0.5\n",
    "vals = 50\n",
    "final = np.zeros((img.shape[1],img.shape[1]))\n",
    "for cnt, (mask, sc) in enumerate(zip(out[0]['masks'], out[0]['scores'])):\n",
    "    if sc.item() > 0.2:\n",
    "        mas = mask.detach().numpy()\n",
    "        mas[mas < th] = 0\n",
    "        mas[mas>=th] = 255\n",
    "        if np.count_nonzero(final[mas[0] == 255]) <= 100:\n",
    "            final[mas[0] == 255] = random.randint(50, 255) \n",
    "        vals += 1\n",
    "cv2.imwrite(\"xd.png\", final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf6a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box confidence mit bergeben lassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffb5798",
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d286a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610c983e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = io.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\256neu\\dataset\\valid\\images\\07__1s_0r_448x_224y_.png\").astype(np.float32)\n",
    "img = img / 255.\n",
    "inp = torch.tensor(img, dtype=torch.float).permute(2,0,1).unsqueeze(0)\n",
    "out = model(inp)  #ab hier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8904de60",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(out[0].values())[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0ce851",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed9002",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = len(list(filter(lambda x:x >= threshold, list(out[0].values())[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c5c8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(out[0].values())[0].cpu().detach().numpy()[0:u]\n",
    "list(out[0].values())[1].cpu().detach().numpy()[0:u]\n",
    "list(out[0].values())[0].cpu().detach().numpy()[0:u]\n",
    "list(out[0].values())[0].cpu().detach().numpy()[0:u]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4615753",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [list(out[0].values())[i].cpu().detach().numpy()[0:u] for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b69feba",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b2604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3735e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4838af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_preds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f3e7d8",
   "metadata": {},
   "source": [
    "# Split into tiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe74dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0da529",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = r\"C:/Users/phili/Downloads/Telegram Desktop/NeueZellen/dataset/train/images\"\n",
    "to = r\"F:\\source\\repos\\VisualDL\\custom_experiments\\bayer\\train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9afb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nRows = 2\n",
    "mCols = 2\n",
    "for cnt, im in enumerate(os.listdir(folder)):\n",
    "    label_img = cv2.imread(os.path.join(folder.replace(\"images\", \"labels\"), im), 0)\n",
    "    real_img = cv2.imread(os.path.join(folder, im))\n",
    "    sizeX = real_img.shape[1]\n",
    "    sizeY = real_img.shape[0]\n",
    "    for i in range(0,nRows):\n",
    "        for j in range(0, mCols):\n",
    "            roi_label = label_img[int(i*sizeY/nRows):int(i*sizeY/nRows + sizeY/nRows) ,int(j*sizeX/mCols):int(j*sizeX/mCols + sizeX/mCols)]\n",
    "            roi_image = real_img[int(i*sizeY/nRows):int(i*sizeY/nRows + sizeY/nRows) ,int(j*sizeX/mCols):int(j*sizeX/mCols + sizeX/mCols)]\n",
    "            cv2.imwrite(os.path.join(os.path.join(to, \"images\"), f\"{i}_ {j}_{im}\"), roi_image)\n",
    "            cv2.imwrite(os.path.join(os.path.join(to, \"labels\"), f\"{i}_ {j}_{im}\"), roi_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa3342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6697e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd.png\", cv2.resize(cv2.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\256neu\\dataset\\valid\\labels\\06__1s_3r_1344x_1120y_.png\") * 255, (512,512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618af54c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
