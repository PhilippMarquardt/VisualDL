{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa983342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import timm\n",
    "import albumentations as A\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "from skimage import io\n",
    "from custom import U2NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea980cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "timm.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d33b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.Resize(width=128, height=128),\n",
    "    A.RandomRotate90(p = 1),\n",
    "    A.Transpose(p=1),\n",
    "    A.RandomBrightness(p=1),\n",
    "    A.RandomContrast(p=1),\n",
    "    A.RandomShadow(p=1),\n",
    "    A.RGBShift(p=1),\n",
    "    A.RandomContrast(p=1),\n",
    "])\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(width=128, height=128),\n",
    "    A.GridDistortion(p=1),\n",
    "    A.OpticalDistortion(p=1),\n",
    "    A.ElasticTransform(p=1)\n",
    "\n",
    "])\n",
    "\n",
    "image =  io.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\CellsFinal\\Cells\\right side\\cells\\train\\images\\05__1_5291_9286.png\")\n",
    "mask = io.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\CellsFinal\\Cells\\right side\\cells\\train\\labels\\05__1_5291_9286.png\", as_gray = True)\n",
    "io.imsave(\"orig.png\", image)\n",
    "io.imsave(\"origmask.png\", mask)\n",
    "trans = transform(image = image, mask = mask)\n",
    "image = trans[\"image\"]\n",
    "mask = trans[\"mask\"]\n",
    "io.imsave(\"image.png\", image)\n",
    "io.imsave(\"mask.png\", mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10abfd40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d8b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0195a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(encoder_name = \"timm-resnest50d\", classes = 2, in_channels = 3)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0a49e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\timm-resnest50d, Unet.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a295f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells\\Cells\\test\\images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db05a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt, im in enumerate(os.listdir(image_dir)):\n",
    "    im = io.imread(os.path.join(image_dir, im))\n",
    "    im = im/255.\n",
    "    im = transform(image = im)[\"image\"]\n",
    "    im = torch.tensor(im, dtype = torch.float).permute(2, 0, 1).unsqueeze(0)\n",
    "    print(im.shape)\n",
    "    pred = model(im)\n",
    "    pred = torch.argmax(pred, 1)[0] * 255.\n",
    "    cv2.imwrite(str(cnt) + \".png\", pred.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390b4a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = cv2.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells1\\Cells\\train\\labels\\05__1_4685_10225.png\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c9ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = ii * 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a1d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"test.png\", ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187c5777",
   "metadata": {},
   "source": [
    "# Replace 2D with 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b75530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e72369",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, modules in model.named_modules():\n",
    "    for module in modules:\n",
    "        print(module)\n",
    "    if(isinstance(module, nn.Conv2d)):\n",
    "        kernel_size = module.kernel_size[0]\n",
    "        stride = module.stride[0]\n",
    "        padding = module.padding[0]\n",
    "        weight = module.weight.unsqueeze(2) / kernel_size\n",
    "        weight = torch.cat([weight for _ in range(0, kernel_size)], dim=2)\n",
    "        bias = module.bias\n",
    "\n",
    "        if(bias is None):\n",
    "            print(modules)\n",
    "            print(modules[name])\n",
    "            modules[name] = nn.Conv3d(in_channels=module.weight.shape[1], out_channels=module.weight.shape[0],\n",
    "                               kernel_size=kernel_size, padding=padding, stride=stride, bias=False)\n",
    "        else:\n",
    "            modules[name] = nn.Conv3d(in_channels=module.weight.shape[1], out_channels=module.weight.shape[0],\n",
    "                               kernel_size=kernel_size, padding=padding, stride=stride, bias=True)\n",
    "            modules[name].bias = bias\n",
    "\n",
    "            modules[name].weight.data = weight\n",
    "\n",
    "    elif(isinstance(module, nn.BatchNorm2d)):\n",
    "        weight = module.weight\n",
    "        bias = module.bias\n",
    "        modules[name] = nn.BatchNorm3d(weight.shape[0])\n",
    "        modules[name].weight = weight\n",
    "        modules[name].bias = bias\n",
    "\n",
    "for name in modules:\n",
    "    parent_module = model\n",
    "    objs = name.split(\".\")\n",
    "    if len(objs) == 1:\n",
    "        model.__setattr__(name, modules[name])\n",
    "        continue\n",
    "\n",
    "    for obj in objs[:-1]:\n",
    "        parent_module = parent_module.__getattr__(obj)\n",
    "\n",
    "    parent_module.__setattr__(objs[-1], modules[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609929d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_modules(module):\n",
    "    for a in module:\n",
    "        get_all_modules(a.children())\n",
    "        if isinstance(a, nn.Conv2d):\n",
    "            print(a)\n",
    "            #a = nn.Conv3d(3,32,3)\n",
    "            \n",
    "        elif isinstance(a, nn.BatchNorm2d):\n",
    "            print(a)\n",
    "\n",
    "            \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2654f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_modules(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5713df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in model.encoder.named_modules():\n",
    "    print(a)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc30e742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from uformer_pytorch import Uformer\n",
    "\n",
    "model = Uformer(\n",
    "    dim = 16,           # initial dimensions after input projection, which increases by 2x each stage\n",
    "    stages = 4,         # number of stages\n",
    "    num_blocks = 2,     # number of transformer blocks per stage\n",
    "    window_size = 16,   # set window size (along one side) for which to do the attention within\n",
    "    dim_head = 64,\n",
    "    heads = 1,\n",
    "    ff_mult = 4\n",
    ")\n",
    "model.cuda()\n",
    "x = torch.randn(1, 3, 512, 512).cuda()\n",
    "pred = model(x) # (1, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb84406",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dffd467",
   "metadata": {},
   "source": [
    "# Segmentation inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f1c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uformer_pytorch import Uformer\n",
    "model = U2NET(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d91f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.create_model(arch = \"Unet\", encoder_name = \"resnext50_32x4d\", classes = 2, in_channels = 3)\n",
    "model.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\resnext50_32x4d, Unet.pt\"))\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d196607",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4547c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.Resize(width=128, height=128),\n",
    "])\n",
    "test = r\"E:\\source\\repos\\Daten\\Cells\\valid\\images\"\n",
    "for cnt, im in enumerate(os.listdir(test)):\n",
    "    image = io.imread(os.path.join(test, im))\n",
    "    image = transform(image = image)[\"image\"]\n",
    "    image = image/255.\n",
    "    s = torch.unsqueeze(torch.tensor(image, dtype = torch.float).permute(2, 0, 1), 0).cuda()\n",
    "    with torch.cuda.amp.autocast():\n",
    "        preds = model(s)\n",
    "    preds = torch.argmax(preds, 1)\n",
    "    preds *= 255\n",
    "    #preds[preds == 1] = 50\n",
    "    #preds[preds == 2] = 100\n",
    "    #preds[preds == 3] = 150\n",
    "    #preds[preds == 4] = 200\n",
    "    #preds[preds == 5] = 250\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    io.imsave(f\"{im}.png\", preds[0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79278e31",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d4df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells\\train\\labels\"\n",
    "ab = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells\\train\\bs\"\n",
    "for cnt, im in enumerate(os.listdir(test)):\n",
    "    img = cv2.imread(os.path.join(test, im)) * 255.\n",
    "    kernel = np.ones((3, 3), 'uint8')\n",
    "    dilate_img = cv2.dilate(img, kernel, iterations=1)\n",
    "    img1_bg = dilate_img - img\n",
    "    img1 = img1_bg[:,:,0]\n",
    "    clipped = np.clip(img1, 1, 6) # weight edges by factor (e.g. 6)\n",
    "    print(np.min(clipped))\n",
    "    cv2.imwrite(os.path.join(ab, im),clipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78dc66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch.losses import DiceLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af932e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DiceLoss(reduce = \"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8963cd",
   "metadata": {},
   "source": [
    "# Classification inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf46979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from skimage import io\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f6ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = timm.create_model(\"resnext50_32x4d\", pretrained=True, num_classes = 5).cuda()\n",
    "#second = timm.create_model(\"resnext50d_32x4d\", pretrained=True, num_classes = 5).cuda()\n",
    "first.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\resnext50_32x4d.pt\"))\n",
    "#second.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\resnext50d_32x4d.pt\"))\n",
    "first.eval()\n",
    "#second.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a827dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = timm.create_model(\"resnext50_32x4d\", pretrained=True, num_classes = 5).cuda()\n",
    "second = timm.create_model(\"efficientnet_b4\", pretrained=True, num_classes = 5).cuda()\n",
    "first.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\resnext50_32x4d.pt\"))\n",
    "#second.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\efficientnet_b4.pt\"))\n",
    "first.eval()\n",
    "#second.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30bc34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "transform = A.Compose([\n",
    "    A.Resize(width=512, height=512),\n",
    "])\n",
    "counter = 0\n",
    "counterxd = 0\n",
    "names = [\"NA\", \"TRG0\", \"TRG1\", \"TRG2\", \"TRG3\"]\n",
    "for name in names:\n",
    "    os.mkdir(name)\n",
    "values = dict()\n",
    "for cnt, name in enumerate(names):\n",
    "    values[name] = []\n",
    "    base = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset_valid/\" + name\n",
    "    for im in os.listdir(base):\n",
    "        image = io.imread(os.path.join(base, im)).astype(np.float32)\n",
    "        image = transform(image = image)[\"image\"]\n",
    "        image = image/255.\n",
    "        s = torch.unsqueeze(torch.tensor(image, dtype = torch.float).permute(2, 0, 1), 0).cuda()\n",
    "        preds =  first(s) \n",
    "        preds = torch.argmax(preds, 1)\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        values[name].append(preds[0])\n",
    "        counterxd += 1\n",
    "        io.imsave(f\"{names[preds[0]]}/{im}.png\", image)\n",
    "        if cnt == preds:\n",
    "            counter += 1\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076f2189",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter/counterxd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de14543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'NA': [4, 0, 0, 2, 0, 0],\n",
    " 'TRG0': [1, 4],\n",
    " 'TRG1': [2, 2, 2],\n",
    " 'TRG2': [3, 4, 3],\n",
    " 'TRG3': [4, 4, 4, 4, 4, 4, 4, 0, 4, 2, 4, 4, 4, 4, 4, 4, 2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d2f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset_valid\\TRG2\\E93 L X20_0_1183_3925.png\"\n",
    "base = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset_valid\\TRG2\"\n",
    "\n",
    "\n",
    "image = io.imread(path)\n",
    "image = image/255.\n",
    "s = torch.unsqueeze(torch.tensor(image, dtype = torch.float).permute(2, 0, 1), 0).cuda()\n",
    "preds = first(s) + second(s)\n",
    "preds = torch.argmax(preds, 1)\n",
    "preds = preds.detach().cpu().numpy()\n",
    "print(preds)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0872d3",
   "metadata": {},
   "source": [
    "# Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab262af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "start = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\test_data\\test_data\"\n",
    "\n",
    "for im in os.listdir(os.path.join(start, \"images\")):\n",
    "    img = cv2.imread(os.path.join(os.path.join(start, \"images\"), im), 0)\n",
    "    mask = cv2.imread(os.path.join(os.path.join(start, \"masks\"), im), 0)\n",
    "    mask[img == 0] = 0\n",
    "    mask[mask == 0] = 0\n",
    "    mask[mask == 29] = 1\n",
    "    mask[mask == 105] = 2\n",
    "    mask[mask == 117] = 3\n",
    "    mask[mask ==189] = 4\n",
    "    mask[mask == 225] = 5\n",
    "    mask[mask > 5] = 1\n",
    "    print(os.path.join(os.path.join(start, \"labels\"), im))\n",
    "    cv2.imwrite(os.path.join(os.path.join(start, \"labels\"), im), mask)\n",
    "    #img[img > 0] = 1\n",
    "    #cv2.imwrite(os.path.join(start, im), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebdae52",
   "metadata": {},
   "source": [
    "# Trian test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2884ed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = r\"E:\\source\\repos\\Daten\\PLA\\train\\images\"\n",
    "to = r\"E:\\source\\repos\\Daten\\PLA\\val\\images\"\n",
    "import random\n",
    "from shutil import copyfile\n",
    "import os\n",
    "split = os.listdir(start)\n",
    "random.shuffle(split)\n",
    "\n",
    "test = split[0:35]\n",
    "train = split[35:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c565f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in test:\n",
    "    copyfile(os.path.join(start, file), os.path.join(to, file))\n",
    "    copyfile(os.path.join(start, file).replace(\"images\", \"labels\"), os.path.join(to, file).replace(\"images\", \"labels\"))\n",
    "    os.remove(os.path.join(start, file))\n",
    "    os.remove(os.path.join(start, file).replace(\"images\", \"labels\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b88449",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Nuclei\\Nuclei\\train\\labels\"\n",
    "out = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Nuclei\\Nuclei\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(start):\n",
    "    preds = cv2.imread(os.path.join(start, file), 0)\n",
    "    preds[preds == 1] = 50\n",
    "    preds[preds == 2] = 100\n",
    "    preds[preds == 3] = 150\n",
    "    preds[preds == 4] = 200\n",
    "    preds[preds == 5] = 250\n",
    "    cv2.imwrite(os.path.join(out, file), preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3244b1a0",
   "metadata": {},
   "source": [
    "# 255 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b2d6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = r\"E:\\source\\repos\\Daten\\HER-N\\hubt\\dataset\\Cells\\valid\\labels\"\n",
    "import cv2\n",
    "import os\n",
    "for file in os.listdir(start):\n",
    "    img = cv2.imread(os.path.join(start, file), 0)\n",
    "    img[img > 0] = 1\n",
    "    cv2.imwrite(os.path.join(start, file), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0567e77b",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490756ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from uformer_pytorch import Uformer\n",
    "\n",
    "model = model = Uformer(\n",
    "                dim = 64,           # initial dimensions after input projection, which increases by 2x each stage\n",
    "                stages = 3,         # number of stages\n",
    "                num_blocks = 2,     # number of transformer blocks per stage\n",
    "                window_size = 16,   # set window size (along one side) for which to do the attention within\n",
    "                dim_head = 64,\n",
    "                heads = 4,\n",
    "                ff_mult = 2\n",
    "            ).cuda()\n",
    "\n",
    "x = torch.randn(2, 3, 128, 128).cuda()\n",
    "with torch.cuda.amp.autocast():\n",
    "    pred = model(x) # (1, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce132281",
   "metadata": {},
   "source": [
    "# Extract single instances for obj. detec + semant. seg -> bring into yolov5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080b1a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e39689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv.imread(r\"E:\\source\\repos\\Daten\\Cells\\train\\labels\\05__1_3115_9058.png\", 0)\n",
    "\n",
    "# Creating kernel\n",
    "kernel = np.ones((2, 2), np.uint8)\n",
    "  \n",
    "# Using cv2.erode() method \n",
    "image = cv.erode(im, kernel) \n",
    "\n",
    "#image[image > 0] = 255\n",
    "#ret, thresh = cv.threshold(im, 127, 255, 0)\n",
    "contours,hierachy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b1581",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(contours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e416ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.imwrite(\"xd.png\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba61ddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = {}\n",
    "start = r\"E:\\source\\repos\\Daten\\HER-N\\hubt\\dataset\\Cells\\train\\labels\"\n",
    "all_files = os.listdir(start)\n",
    "for img in os.listdir(start):\n",
    "    print(img)\n",
    "    im = cv.imread(os.path.join(start, img), 0)\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    image = cv.erode(im, kernel) \n",
    "    contours,hierachy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "    blank = np.zeros_like(im)\n",
    "    for cnt, cont in enumerate(contours):\n",
    "        xmin,ymin,width,height = cv.boundingRect(cont)\n",
    "        print(width)\n",
    "        #cv.rectangle(im,(x,y),(x+width,y+height),(255),1)\n",
    "        #cv2.imwrite(\"xd.png\", im)\n",
    "        image_width = 256\n",
    "        xcenter, ycenter = xmin + width/2, ymin + height/2\n",
    "        xcenter, ycenter, width, height = xcenter/image_width, ycenter/image_width, width/image_width, height/image_width\n",
    "        if not img in files:\n",
    "            files[img] = [(\"0\",str(xcenter), str(ycenter), str(width), str(height))]\n",
    "        else:\n",
    "\n",
    "            files[img] += [(\"0\",str(xcenter), str(ycenter), str(width), str(height))]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a3dac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a93a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "base = cv2.imread(os.path.join(r\"E:\\source\\repos\\Daten\\Cells\\train\\labels\", '05__1_3115_10030.png'))\n",
    "\n",
    "for rec in files['05__1_3115_10030.png']:\n",
    "    xcenter,ycenter,width,height = [int(float(xx) * 128) for xx in rec[1:]]\n",
    "    x = int(xcenter - width/2)\n",
    "    y = int(ycenter - height/2)\n",
    "    cv.rectangle(base,(x,y),(x+width,y+height),(255,0,0),1)\n",
    "cv2.imwrite(\"xd2.png\", base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c82d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv.imread(os.path.join(r\"E:\\source\\repos\\Daten\\Cells\\train\\labels\", '05__1_3115_10030.png'), 0)\n",
    "kernel = np.ones((2, 2), np.uint8)\n",
    "image = cv.erode(im, kernel) \n",
    "contours,hierachy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "blank = np.zeros_like(im)\n",
    "for cnt, cont in enumerate(contours):\n",
    "    x,y,width,height = cv.boundingRect(cont)\n",
    "    cv.rectangle(im,(x,y),(x+width,y+height),(255),1)\n",
    "cv2.imwrite(\"xd.png\", im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51cb372",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_anno = [item for item in all_files if item not in list(files.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e9b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"labels\")\n",
    "for cnt, (key, val) in enumerate(files.items()):\n",
    "    with open(\"labels/\" + key.replace(\".png\", \".txt\"), \"w\") as handle:\n",
    "        for va in val:\n",
    "            handle.write(\" \".join(list(va))+ \"\\n\") \n",
    "            \n",
    "for name in no_anno:\n",
    "    with open(\"labels/\" + name.replace(\".png\", \".txt\"), \"w\") as handle:\n",
    "        handle.write(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891f647a",
   "metadata": {},
   "source": [
    "# Export for instance segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50876230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abacfd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"labels\")\n",
    "os.mkdir(\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86410f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_rec(img, rec):\n",
    "    x,y,w,h = rec\n",
    "    return img[y:y+h, x:x+w]\n",
    "def add_rec(orig, img, rec):\n",
    "    x,y,w,h = rec\n",
    "    #r,g,b = [random.randint(20, 255) for i in range(3)]\n",
    "    img[img > 0] = random.randint(20,255)\n",
    "    tmp = np.expand_dims(img.astype(np.uint8), axis=-1)\n",
    "    #tmp[np.all(tmp == (255, 255, 255), axis=-1)] = (b,g,r)\n",
    "    orig[y:y+h, x:x+w] += tmp\n",
    "    return orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e98f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pad = A.Compose([\n",
    "    A.PadIfNeeded(64,64, value = 0, border_mode = 0),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c44bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "labels = r\"E:\\source\\repos\\Daten\\Cells\\train\\labels\"\n",
    "images = r\"E:\\source\\repos\\Daten\\Cells\\train\\images\"\n",
    "to_labels = r\"E:\\source\\repos\\VisualDL\\custom_experiments\\labels\"\n",
    "to_images = r\"E:\\source\\repos\\VisualDL\\custom_experiments\\images\"\n",
    "all_files = os.listdir(labels)\n",
    "adding = 0\n",
    "for img in os.listdir(labels):\n",
    "    im = cv.imread(os.path.join(labels, img), 0)\n",
    "    original = cv.imread(os.path.join(images, img))\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    image = cv.erode(im, kernel) \n",
    "    contours,hierachy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    for cnt, cont in enumerate(contours):\n",
    "        blank = np.zeros_like(im)\n",
    "        xmin,ymin,width,height = cv.boundingRect(cont)\n",
    "        xmin -= 5\n",
    "        ymin -= 5\n",
    "        width +=5\n",
    "        height += 5\n",
    "        xmin = min(0, xmin)\n",
    "        ymin = min(0, ymin)\n",
    "        cv.drawContours(blank, [cont], -1, 1, -1)\n",
    "        final = cut_rec(blank, (xmin, ymin , width , height ))\n",
    "        orig_final = cut_rec(original, (xmin, ymin , width , height ))\n",
    "        trans = pad(image = orig_final, mask = final)\n",
    "        final = trans[\"mask\"]\n",
    "        orig_final = trans[\"image\"]\n",
    "        cv.imwrite(os.path.join(to_labels, img.replace(\".png\", f\"{cnt}.png\")), final)\n",
    "        cv.imwrite(os.path.join(to_images, img.replace(\".png\", f\"{cnt}.png\")), orig_final)\n",
    "        #cv.rectangle(im,(x,y),(x+width,y+height),(255),1)\n",
    "        \n",
    "        #cv2.imwrite(\"xd.png\", im)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b4cab0",
   "metadata": {},
   "source": [
    "# instance segmentation inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc80695",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.UnetPlusPlus(encoder_name = \"resnext50_32x4d\", classes = 2, in_channels = 3)\n",
    "model.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\resnext50_32x4d, UnetPlusPlus.pt\"))\n",
    "model.eval()\n",
    "model=model.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9078c0",
   "metadata": {},
   "source": [
    "## simple predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed2ae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    #A.PadIfNeeded(64,64, value = 0, border_mode = 0),\n",
    "    A.Resize(width=64, height=64),\n",
    "])\n",
    "test = r\"E:\\source\\repos\\VisualDL\\custom_experiments\\valid\\images\"\n",
    "for cnt, im in enumerate(os.listdir(test)):\n",
    "    image = io.imread(os.path.join(test, im))\n",
    "    image = transform(image = image)[\"image\"]\n",
    "    image = image/255.\n",
    "    s = torch.unsqueeze(torch.tensor(image, dtype = torch.float).permute(2, 0, 1), 0).cuda()\n",
    "    with torch.cuda.amp.autocast():\n",
    "        preds = model(s)\n",
    "    preds = torch.argmax(preds, 1)\n",
    "    preds *= 255\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    io.imsave(f\"{im}.png\", preds[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf9c6b5",
   "metadata": {},
   "source": [
    "# use rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8452363",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.PadIfNeeded(64,64, value = 0, border_mode = 0),\n",
    "    A.Resize(width=64, height=64),\n",
    "])\n",
    "\n",
    "files = {}\n",
    "labels = r\"E:\\source\\repos\\Daten\\Cells\\valid\\labels\"\n",
    "images = r\"E:\\source\\repos\\Daten\\Cells\\valid\\images\"\n",
    "to = r\"E:\\source\\repos\\VisualDL\\custom_experiments\\test\"\n",
    "all_files = os.listdir(labels)\n",
    "adding = 0\n",
    "for ii, img in enumerate(os.listdir(labels)):\n",
    "    im = cv.imread(os.path.join(labels, img), 0)\n",
    "    print(img)\n",
    "    original = cv.imread(os.path.join(images, img))\n",
    "    blank = np.zeros_like(original)\n",
    "    original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    image = cv.erode(im, kernel) \n",
    "    contours,hierachy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "    for cnt, cont in enumerate(contours):\n",
    "        \n",
    "        xmin,ymin,width,height = cv.boundingRect(cont)\n",
    "        orig_final = cut_rec(original, (xmin, ymin , width , height ))\n",
    "        tt = orig_final.copy()\n",
    "        orig_final = transform(image = orig_final)[\"image\"]\n",
    "        image = orig_final/255.\n",
    "        s = torch.unsqueeze(torch.tensor(image, dtype = torch.float).permute(2, 0, 1), 0).cuda()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            preds = model(s)\n",
    "        preds = torch.argmax(preds, 1)\n",
    "        preds *= 255\n",
    "        preds = preds.detach().cpu().numpy()[0]\n",
    "\n",
    "        preds = cut_rec(preds, (32 - int(width/2), 32 - int(height/2), width, height))\n",
    "        #contours,hierachy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "        blank = add_rec(blank, preds, (xmin,ymin,width,height))\n",
    "        cv2.imwrite(os.path.join(to, f\"{img}{cnt}.png\"), preds)\n",
    "        cv2.imwrite(os.path.join(to, f\"{img}{cnt}orig.png\"), tt)\n",
    "    cv2.imwrite(os.path.join(to, f\"{img}{cnt}xdddd.png\"), blank)\n",
    "    if ii == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658f0779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a3aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "skimage.skimage.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de6e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = U2NET(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aa5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_in = torch.randn(1, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0201e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, einsum\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from custom import ViT, AxialImageTransformer, AxialAttention\n",
    "# helpers\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def divisible_by(val, divisor):\n",
    "    return (val % divisor) == 0\n",
    "\n",
    "def unfold_output_size(image_size, kernel_size, stride, padding):\n",
    "    return int(((image_size - kernel_size + (2 * padding)) / stride) + 1)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = heads * dim_head\n",
    "        self.heads =  heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, d, h = *x.shape, self.heads\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
    "\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "        attn = sim.softmax(dim = -1)\n",
    "\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "# main class\n",
    "\n",
    "class TNT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        image_size,\n",
    "        patch_dim,\n",
    "        pixel_dim,\n",
    "        patch_size,\n",
    "        pixel_size,\n",
    "        depth,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        ff_dropout = 0.,\n",
    "        attn_dropout = 0.,\n",
    "        channels = 3,\n",
    "        unfold_args = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert divisible_by(image_size, patch_size), 'image size must be divisible by patch size'\n",
    "        assert divisible_by(patch_size, pixel_size), 'patch size must be divisible by pixel size for now'\n",
    "\n",
    "        num_patch_tokens = (image_size // patch_size) ** 2\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patch_tokens = nn.Parameter(torch.randn(num_patch_tokens + 1, patch_dim))\n",
    "\n",
    "        unfold_args = default(unfold_args, (pixel_size, pixel_size, 0))\n",
    "        unfold_args = (*unfold_args, 0) if len(unfold_args) == 2 else unfold_args\n",
    "        kernel_size, stride, padding = unfold_args\n",
    "\n",
    "        pixel_width = unfold_output_size(patch_size, kernel_size, stride, padding)\n",
    "        num_pixels = pixel_width ** 2\n",
    "\n",
    "        self.to_pixel_tokens = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> (b h w) c p1 p2', p1 = patch_size, p2 = patch_size),\n",
    "            nn.Unfold(kernel_size = kernel_size, stride = stride, padding = padding),\n",
    "            Rearrange('... c n -> ... n c'),\n",
    "            nn.Linear(channels * kernel_size ** 2, pixel_dim)\n",
    "        )\n",
    "\n",
    "        self.patch_pos_emb = nn.Parameter(torch.randn(num_patch_tokens + 1, patch_dim))\n",
    "        self.pixel_pos_emb = nn.Parameter(torch.randn(num_pixels, pixel_dim))\n",
    "\n",
    "        layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "\n",
    "            pixel_to_patch = nn.Sequential(\n",
    "                nn.LayerNorm(pixel_dim),\n",
    "                Rearrange('... n d -> ... (n d)'),\n",
    "                nn.Linear(pixel_dim * num_pixels, patch_dim),\n",
    "            )\n",
    "\n",
    "            layers.append(nn.ModuleList([\n",
    "                PreNorm(pixel_dim, Attention(dim = pixel_dim, heads = heads, dim_head = dim_head, dropout = attn_dropout)),\n",
    "                PreNorm(pixel_dim, FeedForward(dim = pixel_dim, dropout = ff_dropout)),\n",
    "                pixel_to_patch,\n",
    "                PreNorm(patch_dim, Attention(dim = patch_dim, heads = heads, dim_head = dim_head, dropout = attn_dropout)),\n",
    "                PreNorm(patch_dim, FeedForward(dim = patch_dim, dropout = ff_dropout)),\n",
    "            ]))\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, _, h, w, patch_size, image_size = *x.shape, self.patch_size, self.image_size\n",
    "        assert divisible_by(h, patch_size) and divisible_by(w, patch_size), f'height {h} and width {w} of input must be divisible by the patch size'\n",
    "\n",
    "        num_patches_h = h // patch_size\n",
    "        num_patches_w = w // patch_size\n",
    "        n = num_patches_w * num_patches_h\n",
    "\n",
    "        pixels = self.to_pixel_tokens(x)\n",
    "        patches = repeat(self.patch_tokens[:(n + 1)], 'n d -> b n d', b = b)\n",
    "\n",
    "        patches += rearrange(self.patch_pos_emb[:(n + 1)], 'n d -> () n d')\n",
    "        pixels += rearrange(self.pixel_pos_emb, 'n d -> () n d')\n",
    "\n",
    "        for pixel_attn, pixel_ff, pixel_to_patch_residual, patch_attn, patch_ff in self.layers:\n",
    "\n",
    "            pixels = pixel_attn(pixels) + pixels\n",
    "            pixels = pixel_ff(pixels) + pixels\n",
    "\n",
    "            patches_residual = pixel_to_patch_residual(pixels)\n",
    "\n",
    "            patches_residual = rearrange(patches_residual, '(b h w) d -> b (h w) d', h = num_patches_h, w = num_patches_w)\n",
    "            patches_residual = F.pad(patches_residual, (0, 0, 1, 0), value = 0) # cls token gets residual of 0\n",
    "            patches = patches + patches_residual\n",
    "\n",
    "            patches = patch_attn(patches) + patches\n",
    "            patches = patch_ff(patches) + patches\n",
    "        hidden_states = patches[:,1:,:]\n",
    "        B, n_patch, hidden = hidden_states.size()  # reshape from (B, n_patch, hidden) to (B, h, w, hidden)\n",
    "        h, w = int(np.sqrt(n_patch)), int(np.sqrt(n_patch))\n",
    "        x = hidden_states.permute(0, 2, 1)\n",
    "        x = x.contiguous().view(B, hidden, h, w)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Conv2dReLU(nn.Sequential):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            padding=0,\n",
    "            stride=1,\n",
    "            use_batchnorm=True,\n",
    "    ):\n",
    "\n",
    "        if use_batchnorm == \"inplace\" and InPlaceABN is None:\n",
    "            raise RuntimeError(\n",
    "                \"In order to use `use_batchnorm='inplace'` inplace_abn package must be installed. \"\n",
    "                + \"To install see: https://github.com/mapillary/inplace_abn\"\n",
    "            )\n",
    "\n",
    "        conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=not (use_batchnorm),\n",
    "        )\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        if use_batchnorm == \"inplace\":\n",
    "            bn = InPlaceABN(out_channels, activation=\"leaky_relu\", activation_param=0.0)\n",
    "            relu = nn.Identity()\n",
    "\n",
    "        elif use_batchnorm and use_batchnorm != \"inplace\":\n",
    "            bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        else:\n",
    "            bn = nn.Identity()\n",
    "\n",
    "        super(Conv2dReLU, self).__init__(conv, bn, relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1990a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "image_size = 128\n",
    "init_dim = 16\n",
    "conv = Conv2dReLU(3, init_dim, 1).cuda()\n",
    "inst = AxialImageTransformer(dim = init_dim,depth = 2,axial_pos_emb_shape = (image_size,image_size)).cuda()\n",
    "\n",
    "first = TNT(image_size = image_size, patch_size = 2, pixel_dim = 16, pixel_size = 1, patch_dim = init_dim * 2, depth = 2, heads = 4, channels = init_dim).cuda()\n",
    "second = TNT(image_size = image_size//2, patch_size = 2, pixel_dim = 16, pixel_size = 1, patch_dim = init_dim * 4, depth = 4, heads = 6, channels = init_dim * 2).cuda()\n",
    "third = TNT(image_size = image_size//4, patch_size = 2, pixel_dim = 16, pixel_size = 1, patch_dim = init_dim * 8, depth = 6, heads = 8, channels = init_dim * 4).cuda()\n",
    "fourth = TNT(image_size = image_size//8, patch_size = 2, pixel_dim = 16, pixel_size = 1, patch_dim = init_dim * 16, depth = 8, heads = 12, channels = init_dim * 8).cuda()\n",
    "cup = ViT(\n",
    "            image_size = 8,\n",
    "            patch_size = 1,\n",
    "            dim = init_dim * 32,\n",
    "            depth = 6,\n",
    "            heads = 12,\n",
    "            mlp_dim = 2048,\n",
    "            dropout = 0.1,\n",
    "            emb_dropout = 0.1,\n",
    "            channels = init_dim * 16\n",
    "        ).cuda()\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                in_channels,\n",
    "                skip_channels,\n",
    "                out_channels,\n",
    "                depth,\n",
    "                size,\n",
    "                heads= 2):\n",
    "        super().__init__()   \n",
    "        self.ax = AxialImageTransformer(dim = in_channels + skip_channels,heads= heads,depth = depth,axial_pos_emb_shape = (size,size)).cuda()\n",
    "        self.out = Conv2dReLU(in_channels + skip_channels, out_channels, 1)\n",
    "        \n",
    "    def forward(self, x, skip = None):\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        return self.out(self.ax(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306ae103",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = 2\n",
    "up1 = DecoderBlock(init_dim*32, init_dim*8, init_dim*8, depth = 2, heads = 2,size = image_size//8).cuda()\n",
    "up2 = DecoderBlock(init_dim*8, init_dim*4, init_dim*4, depth = 2, heads = 2,size = image_size//4).cuda()\n",
    "up3 = DecoderBlock(init_dim*4, init_dim*2, init_dim*2, depth = 2, heads = 2,size = image_size//2).cuda()\n",
    "up4 = DecoderBlock(init_dim*2, init_dim, 2, depth = 2, heads = 2,size = image_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b45715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=AxialAttention(32,dim_index =1).cuda()\n",
    "a= AxialImageTransformer(dim = 3072, depth = 1,axial_pos_emb_shape = (128,128), heads=1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ce9eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_in = torch.randn(2, 3072, 128, 128).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79380be",
   "metadata": {},
   "outputs": [],
   "source": [
    "a(dummy_in).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3c4b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inst(conv(dummy_in)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe71973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(hidden_states):\n",
    "    B, n_patch, hidden = hidden_states.size()  # reshape from (B, n_patch, hidden) to (B, h, w, hidden)\n",
    "    h, w = int(np.sqrt(n_patch)), int(np.sqrt(n_patch))\n",
    "    x = hidden_states.permute(0, 2, 1)\n",
    "    x = x.contiguous().view(B, hidden, h, w)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a3629",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.cuda.amp.autocast():\n",
    "    out1 = inst(conv(dummy_in)) #128\n",
    "with torch.cuda.amp.autocast():\n",
    "    out2 = first(out1) #64\n",
    "with torch.cuda.amp.autocast():\n",
    "    out3 = second(out2) #32\n",
    "with torch.cuda.amp.autocast():\n",
    "    out4 = third(out3) #16\n",
    "with torch.cuda.amp.autocast():\n",
    "    out5 = fourth(out4) #8\n",
    "with torch.cuda.amp.autocast():\n",
    "     out6 = cup(out5) #8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6430ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "up4(up3(up2(up1(out6, out4), out3), out2), out1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2127a1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hidden_states = out\n",
    "B, n_patch, hidden = hidden_states.size()  # reshape from (B, n_patch, hidden) to (B, h, w, hidden)\n",
    "h, w = int(np.sqrt(n_patch)), int(np.sqrt(n_patch))\n",
    "x = hidden_states.permute(0, 2, 1)\n",
    "x = x.contiguous().view(B, hidden, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ea3048",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da933744",
   "metadata": {},
   "outputs": [],
   "source": [
    "2**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f674d27b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
