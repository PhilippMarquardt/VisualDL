{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa983342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from visualdl import vdl\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import timm\n",
    "import albumentations as A\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "from skimage import io\n",
    "\n",
    "# from custom import U2NET\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7174b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((2, 10, 3, 128, 128), dtype=torch.float)\n",
    "y = torch.tensor([1] * 200, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b22ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = []\n",
    "for point in x.swapaxes(0, 1):\n",
    "    # print(model(point)[0].shape)\n",
    "    # print(b(model(point)[0]).shape)\n",
    "    out = b(F.adaptive_avg_pool2d(model(point)[-1], (1, 1)))\n",
    "\n",
    "    print(out.shape)\n",
    "    inp.append(out)\n",
    "torch.stack(inp, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fe5367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac554692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf4b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\n",
    "    r\"D:\\Hsa\\temp\\custom_models\\verified_models\\Custom - Glomeruli\\instance_seg\\valentinglom.pt\"\n",
    ")\n",
    "model[\n",
    "    \"model\"\n",
    "] = \"torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False,                 box_detections_per_img = 800,                 min_size = 800,                 num_classes=2, pretrained_backbone=False)\"\n",
    "torch.save(model, \"glom.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55505c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"valentinglom.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca2bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65709cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812b4a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(inp, dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737a8a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.nn.Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dacf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "pool = F.adaptive_avg_pool2d(x, (1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b2f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "timm.list_models(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d365fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\"resnet50\", pretrained=False, features_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51b0856",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_info.channels()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0ed16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vdl.get_inference_model(\n",
    "    r\"D:\\Hsa\\temp\\custom_models\\verified_models\\Custom - Her2 Cells\\instance_seg\\003.pt\",\n",
    "    type=\"instance\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd32b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\n",
    "    r\"D:\\Hsa\\temp\\custom_models\\verified_models\\Custom - Her2 Cells\\instance_seg\\003.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ed455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8d42c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vdl.get_inference_model(r\"F:\\source\\repos\\VisualDL\\resnet18, UnetPlusPlus.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414a67b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(r\"C:\\Users\\phili\\Documents\\001.pt\").keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b029cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cos(np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df04cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "t_range = np.arange(0, 2 * np.pi, 0.05)\n",
    "for t in t_range:\n",
    "    # print((math.sin (t) * t, math.cos(t) * t))\n",
    "    plt.plot(np.sin(t) * t, np.cos(t) * t, markersize=1, marker=\"o\")\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.spines[\"top\"].set_color(\"none\")\n",
    "    ax.spines[\"bottom\"].set_position(\"zero\")\n",
    "    ax.spines[\"left\"].set_position(\"zero\")\n",
    "    ax.spines[\"right\"].set_color(\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea980cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "timm.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d33b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(width=128, height=128),\n",
    "        A.RandomRotate90(p=1),\n",
    "        A.Transpose(p=1),\n",
    "        A.RandomBrightness(p=1),\n",
    "        A.RandomContrast(p=1),\n",
    "        A.RandomShadow(p=1),\n",
    "        A.RGBShift(p=1),\n",
    "        A.RandomContrast(p=1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(width=128, height=128),\n",
    "        A.GridDistortion(p=1),\n",
    "        A.OpticalDistortion(p=1),\n",
    "        A.ElasticTransform(p=1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "image = io.imread(\n",
    "    r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\CellsFinal\\Cells\\right side\\cells\\train\\images\\05__1_5291_9286.png\"\n",
    ")\n",
    "mask = io.imread(\n",
    "    r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\CellsFinal\\Cells\\right side\\cells\\train\\labels\\05__1_5291_9286.png\",\n",
    "    as_gray=True,\n",
    ")\n",
    "io.imsave(\"orig.png\", image)\n",
    "io.imsave(\"origmask.png\", mask)\n",
    "trans = transform(image=image, mask=mask)\n",
    "image = trans[\"image\"]\n",
    "mask = trans[\"mask\"]\n",
    "io.imsave(\"image.png\", image)\n",
    "io.imsave(\"mask.png\", mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10abfd40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d8b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0195a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(encoder_name=\"timm-resnest50d\", classes=2, in_channels=3)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0a49e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\timm-resnest50d, Unet.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a295f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells\\Cells\\test\\images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db05a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt, im in enumerate(os.listdir(image_dir)):\n",
    "    im = io.imread(os.path.join(image_dir, im))\n",
    "    im = im / 255.0\n",
    "    im = transform(image=im)[\"image\"]\n",
    "    im = torch.tensor(im, dtype=torch.float).permute(2, 0, 1).unsqueeze(0)\n",
    "    print(im.shape)\n",
    "    pred = model(im)\n",
    "    pred = torch.argmax(pred, 1)[0] * 255.0\n",
    "    cv2.imwrite(str(cnt) + \".png\", pred.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390b4a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = cv2.imread(\n",
    "    r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells1\\Cells\\train\\labels\\05__1_4685_10225.png\",\n",
    "    0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c9ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = ii * 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a1d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"test.png\", ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187c5777",
   "metadata": {},
   "source": [
    "# Replace 2D with 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b75530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e72369",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, modules in model.named_modules():\n",
    "    for module in modules:\n",
    "        print(module)\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        kernel_size = module.kernel_size[0]\n",
    "        stride = module.stride[0]\n",
    "        padding = module.padding[0]\n",
    "        weight = module.weight.unsqueeze(2) / kernel_size\n",
    "        weight = torch.cat([weight for _ in range(0, kernel_size)], dim=2)\n",
    "        bias = module.bias\n",
    "\n",
    "        if bias is None:\n",
    "            print(modules)\n",
    "            print(modules[name])\n",
    "            modules[name] = nn.Conv3d(\n",
    "                in_channels=module.weight.shape[1],\n",
    "                out_channels=module.weight.shape[0],\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                stride=stride,\n",
    "                bias=False,\n",
    "            )\n",
    "        else:\n",
    "            modules[name] = nn.Conv3d(\n",
    "                in_channels=module.weight.shape[1],\n",
    "                out_channels=module.weight.shape[0],\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                stride=stride,\n",
    "                bias=True,\n",
    "            )\n",
    "            modules[name].bias = bias\n",
    "\n",
    "            modules[name].weight.data = weight\n",
    "\n",
    "    elif isinstance(module, nn.BatchNorm2d):\n",
    "        weight = module.weight\n",
    "        bias = module.bias\n",
    "        modules[name] = nn.BatchNorm3d(weight.shape[0])\n",
    "        modules[name].weight = weight\n",
    "        modules[name].bias = bias\n",
    "\n",
    "for name in modules:\n",
    "    parent_module = model\n",
    "    objs = name.split(\".\")\n",
    "    if len(objs) == 1:\n",
    "        model.__setattr__(name, modules[name])\n",
    "        continue\n",
    "\n",
    "    for obj in objs[:-1]:\n",
    "        parent_module = parent_module.__getattr__(obj)\n",
    "\n",
    "    parent_module.__setattr__(objs[-1], modules[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609929d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_modules(module):\n",
    "    for a in module:\n",
    "        get_all_modules(a.children())\n",
    "        if isinstance(a, nn.Conv2d):\n",
    "            print(a)\n",
    "            # a = nn.Conv3d(3,32,3)\n",
    "\n",
    "        elif isinstance(a, nn.BatchNorm2d):\n",
    "            print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2654f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_modules(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5713df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in model.encoder.named_modules():\n",
    "    print(a)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc30e742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from uformer_pytorch import Uformer\n",
    "\n",
    "model = Uformer(\n",
    "    dim=16,  # initial dimensions after input projection, which increases by 2x each stage\n",
    "    stages=4,  # number of stages\n",
    "    num_blocks=2,  # number of transformer blocks per stage\n",
    "    window_size=16,  # set window size (along one side) for which to do the attention within\n",
    "    dim_head=64,\n",
    "    heads=1,\n",
    "    ff_mult=4,\n",
    ")\n",
    "model.cuda()\n",
    "x = torch.randn(1, 3, 512, 512).cuda()\n",
    "pred = model(x)  # (1, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb84406",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dffd467",
   "metadata": {},
   "source": [
    "# Segmentation inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cc3c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from visualdl import vdl\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.transform import rescale, resize\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "from visualdl.models.doubleunet.doubleunet import DoubleUnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdl.predict(images, r\"E:\\source\\repos\\VisualDL\\tu-resnest50d, Unet.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f1c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uformer_pytorch import Uformer\n",
    "\n",
    "model = U2NET(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d91f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.create_model(\n",
    "    arch=\"TransUnet\",\n",
    "    encoder_name=\"tu-efficientnetv2_rw_m\",\n",
    "    classes=2,\n",
    "    in_channels=3,\n",
    "    image_size=512,\n",
    "    decoder_attention_type=None,\n",
    ")\n",
    "# model1 = smp.create_model(arch = \"DoubleUnet\", encoder_name = \"tu-efficientnetv2_rw_m\", classes = 2, in_channels = 3, image_size = 512,decoder_attention_type = None)\n",
    "model1 = DoubleUnet(encoder_name=\"tu-efficientnetv2_rw_m\", classes=2)\n",
    "# model = smp.create_model(arch = \"UnetPlusPlus\", encoder_name = \"resnet18\", classes = 6, in_channels = 3, image_size = 512, decoder_attention_type = \"scse\")\n",
    "model.load_state_dict(\n",
    "    torch.load(r\"F:\\source\\repos\\VisualDL\\CurrentBesterEfficientnet.pt\")[\n",
    "        \"model_state_dict\"\n",
    "    ]\n",
    ")\n",
    "model1.load_state_dict(\n",
    "    torch.load(r\"F:\\source\\repos\\VisualDL\\tu-efficientnetv2_rw_m, DoubleUnet.pt\")[\n",
    "        \"model_state_dict\"\n",
    "    ]\n",
    ")\n",
    "model.eval()\n",
    "model1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c3c501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d196607",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()\n",
    "model1 = model1.cuda()\n",
    "# second = second.cuda()\n",
    "# third = third.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67a97c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4547c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(width=512, height=512),\n",
    "    ]\n",
    ")\n",
    "# test = r\"F:\\source\\repos\\Daten\\HER-N\\Pdl1Combined\\Tumor Cells 512\\valid\\images\"\n",
    "test = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\job_instance_analysis\"\n",
    "test = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells her2 128\\valid\\images\"\n",
    "test = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\imgs\\imgs\"\n",
    "for cnt, im in enumerate(os.listdir(test)):\n",
    "    image = io.imread(os.path.join(test, im))\n",
    "    image = transform(image=image)[\"image\"]\n",
    "    image = image / 255.0\n",
    "    s = torch.unsqueeze(\n",
    "        torch.tensor(image, dtype=torch.float).permute(2, 0, 1), 0\n",
    "    ).cuda()\n",
    "    with torch.cuda.amp.autocast():\n",
    "        preds = model(s)\n",
    "        preds += model1(s)\n",
    "        class_preds = preds  # [:,0:-1]\n",
    "        # dist_map = m(preds[:,-1])\n",
    "        # dist_map[dist_map < 0.45] = 0\n",
    "        # preds += model1(s)\n",
    "    preds = torch.argmax(class_preds, 1)\n",
    "    # dist_map[preds == 0] = 0.0\n",
    "    # class_for_dist = preds.clone()\n",
    "    # class_for_dist[class_for_dist>0] = 1\n",
    "    # dist_map[class_for_dist == 0] = 0\n",
    "\n",
    "    preds *= 255\n",
    "    # preds[preds == 1] = 50\n",
    "    # preds[preds == 2] = 100\n",
    "    # preds[preds == 3] = 150\n",
    "    # preds[preds == 4] = 200\n",
    "    # preds[preds == 5] = 250\n",
    "    # dist_map = dist_map.detach().cpu().numpy()\n",
    "    # maps = dist_map[0]\n",
    "    # maps[maps>0] = 255\n",
    "    # maps = maps.astype(np.uint8)\n",
    "    # print(image.shape)\n",
    "    # print(maps.dtype)\n",
    "    # markers\t= cv2.watershed(preds[0].detach().cpu().numpy().astype(np.int), maps)\n",
    "    io.imsave(f\"{im}.png\", preds[0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7bef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = cv2.imread(\n",
    "    r\"F:\\source\\repos\\VisualDL\\custom_experiments\\05__1_3118-10375_0.png.png\", 0\n",
    ")\n",
    "b[b > 160] = 255\n",
    "b[b <= 160] = 0\n",
    "cv2.imwrite(\"xd.png\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f19f593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92423dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5127b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79278e31",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d4df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells\\train\\labels\"\n",
    "ab = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells\\train\\bs\"\n",
    "for cnt, im in enumerate(os.listdir(test)):\n",
    "    img = cv2.imread(os.path.join(test, im)) * 255.0\n",
    "    kernel = np.ones((3, 3), \"uint8\")\n",
    "    dilate_img = cv2.dilate(img, kernel, iterations=1)\n",
    "    img1_bg = dilate_img - img\n",
    "    img1 = img1_bg[:, :, 0]\n",
    "    clipped = np.clip(img1, 1, 6)  # weight edges by factor (e.g. 6)\n",
    "    print(np.min(clipped))\n",
    "    cv2.imwrite(os.path.join(ab, im), clipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78dc66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch.losses import DiceLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af932e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DiceLoss(reduce=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8963cd",
   "metadata": {},
   "source": [
    "# Classification inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf46979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from skimage import io\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f6ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = timm.create_model(\"resnext50_32x4d\", pretrained=True, num_classes=5).cuda()\n",
    "# second = timm.create_model(\"resnext50d_32x4d\", pretrained=True, num_classes = 5).cuda()\n",
    "first.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\resnext50_32x4d.pt\"))\n",
    "# second.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\resnext50d_32x4d.pt\"))\n",
    "first.eval()\n",
    "# second.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a827dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = timm.create_model(\"resnext50_32x4d\", pretrained=True, num_classes=5).cuda()\n",
    "second = timm.create_model(\"efficientnet_b4\", pretrained=True, num_classes=5).cuda()\n",
    "first.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\resnext50_32x4d.pt\"))\n",
    "# second.load_state_dict(torch.load(r\"E:\\source\\repos\\VisualDL\\efficientnet_b4.pt\"))\n",
    "first.eval()\n",
    "# second.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30bc34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(width=512, height=512),\n",
    "    ]\n",
    ")\n",
    "counter = 0\n",
    "counterxd = 0\n",
    "names = [\"NA\", \"TRG0\", \"TRG1\", \"TRG2\", \"TRG3\"]\n",
    "for name in names:\n",
    "    os.mkdir(name)\n",
    "values = dict()\n",
    "for cnt, name in enumerate(names):\n",
    "    values[name] = []\n",
    "    base = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset_valid/\" + name\n",
    "    for im in os.listdir(base):\n",
    "        image = io.imread(os.path.join(base, im)).astype(np.float32)\n",
    "        image = transform(image=image)[\"image\"]\n",
    "        image = image / 255.0\n",
    "        s = torch.unsqueeze(\n",
    "            torch.tensor(image, dtype=torch.float).permute(2, 0, 1), 0\n",
    "        ).cuda()\n",
    "        preds = first(s)\n",
    "        preds = torch.argmax(preds, 1)\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        values[name].append(preds[0])\n",
    "        counterxd += 1\n",
    "        io.imsave(f\"{names[preds[0]]}/{im}.png\", image)\n",
    "        if cnt == preds:\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076f2189",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter / counterxd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de14543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"NA\": [4, 0, 0, 2, 0, 0],\n",
    "    \"TRG0\": [1, 4],\n",
    "    \"TRG1\": [2, 2, 2],\n",
    "    \"TRG2\": [3, 4, 3],\n",
    "    \"TRG3\": [4, 4, 4, 4, 4, 4, 4, 0, 4, 2, 4, 4, 4, 4, 4, 4, 2],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d2f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset_valid\\TRG2\\E93 L X20_0_1183_3925.png\"\n",
    "base = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset_valid\\TRG2\"\n",
    "\n",
    "\n",
    "image = io.imread(path)\n",
    "image = image / 255.0\n",
    "s = torch.unsqueeze(torch.tensor(image, dtype=torch.float).permute(2, 0, 1), 0).cuda()\n",
    "preds = first(s) + second(s)\n",
    "preds = torch.argmax(preds, 1)\n",
    "preds = preds.detach().cpu().numpy()\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0872d3",
   "metadata": {},
   "source": [
    "# Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab262af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "start = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\test_data\\test_data\"\n",
    "\n",
    "for im in os.listdir(os.path.join(start, \"images\")):\n",
    "    img = cv2.imread(os.path.join(os.path.join(start, \"images\"), im), 0)\n",
    "    mask = cv2.imread(os.path.join(os.path.join(start, \"masks\"), im), 0)\n",
    "    mask[img == 0] = 0\n",
    "    mask[mask == 0] = 0\n",
    "    mask[mask == 29] = 1\n",
    "    mask[mask == 105] = 2\n",
    "    mask[mask == 117] = 3\n",
    "    mask[mask == 189] = 4\n",
    "    mask[mask == 225] = 5\n",
    "    mask[mask > 5] = 1\n",
    "    print(os.path.join(os.path.join(start, \"labels\"), im))\n",
    "    cv2.imwrite(os.path.join(os.path.join(start, \"labels\"), im), mask)\n",
    "    # img[img > 0] = 1\n",
    "    # cv2.imwrite(os.path.join(start, im), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebdae52",
   "metadata": {},
   "source": [
    "# Trian test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2884ed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = r\"E:\\source\\repos\\Daten\\PLA\\train\\images\"\n",
    "to = r\"E:\\source\\repos\\Daten\\PLA\\val\\images\"\n",
    "import random\n",
    "from shutil import copyfile\n",
    "import os\n",
    "\n",
    "split = os.listdir(start)\n",
    "random.shuffle(split)\n",
    "\n",
    "test = split[0:35]\n",
    "train = split[35:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c565f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in test:\n",
    "    copyfile(os.path.join(start, file), os.path.join(to, file))\n",
    "    copyfile(\n",
    "        os.path.join(start, file).replace(\"images\", \"labels\"),\n",
    "        os.path.join(to, file).replace(\"images\", \"labels\"),\n",
    "    )\n",
    "    os.remove(os.path.join(start, file))\n",
    "    os.remove(os.path.join(start, file).replace(\"images\", \"labels\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b88449",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Nuclei\\Nuclei\\train\\labels\"\n",
    "out = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Nuclei\\Nuclei\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(start):\n",
    "    preds = cv2.imread(os.path.join(start, file), 0)\n",
    "    preds[preds == 1] = 50\n",
    "    preds[preds == 2] = 100\n",
    "    preds[preds == 3] = 150\n",
    "    preds[preds == 4] = 200\n",
    "    preds[preds == 5] = 250\n",
    "    cv2.imwrite(os.path.join(out, file), preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3244b1a0",
   "metadata": {},
   "source": [
    "# 255 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b2d6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = r\"E:\\source\\repos\\Daten\\HER-N\\hubt\\dataset\\Cells\\valid\\labels\"\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "for file in os.listdir(start):\n",
    "    img = cv2.imread(os.path.join(start, file), 0)\n",
    "    img[img > 0] = 1\n",
    "    cv2.imwrite(os.path.join(start, file), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0567e77b",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490756ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from uformer_pytorch import Uformer\n",
    "\n",
    "model = model = Uformer(\n",
    "    dim=64,  # initial dimensions after input projection, which increases by 2x each stage\n",
    "    stages=3,  # number of stages\n",
    "    num_blocks=2,  # number of transformer blocks per stage\n",
    "    window_size=16,  # set window size (along one side) for which to do the attention within\n",
    "    dim_head=64,\n",
    "    heads=4,\n",
    "    ff_mult=2,\n",
    ").cuda()\n",
    "\n",
    "x = torch.randn(2, 3, 128, 128).cuda()\n",
    "with torch.cuda.amp.autocast():\n",
    "    pred = model(x)  # (1, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce132281",
   "metadata": {},
   "source": [
    "# Extract single instances for obj. detec + semant. seg -> bring into yolov5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080b1a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e39689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv.imread(\n",
    "    r\"E:\\source\\repos\\Daten\\Converting\\Tumor Cells\\train\\labels\\PD-L1=2_0_37392-42804_85.png\",\n",
    "    0,\n",
    ")\n",
    "\n",
    "# Creating kernel\n",
    "# kernel = np.ones((2, 2), np.uint8)\n",
    "\n",
    "# Using cv2.erode() method\n",
    "# image = cv.erode(im, kernel)\n",
    "\n",
    "# image[image > 0] = 255\n",
    "# ret, thresh = cv.threshold(im, 127, 255, 0)\n",
    "# contours,hierachy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea0d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "im[im != 1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b1581",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(contours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e416ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.imwrite(\"xd.png\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46782004",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_without = 0\n",
    "count_with = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43df744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_without)\n",
    "print(count_with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba61ddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = {}\n",
    "start = r\"G:\\Windows\\Hsa\\temp\\projects\\fe8de687-96da-4cf3-9362-46aeca452635\\dataset\\valid\\labels\"\n",
    "all_files = os.listdir(start)\n",
    "nc = 2\n",
    "for img in os.listdir(start):\n",
    "    for i in range(1, nc + 1):\n",
    "        im = cv.imread(os.path.join(start, img), 0)\n",
    "        im[im > 0] = 1.0\n",
    "        kernel = np.ones((2, 2), np.uint8)\n",
    "        # im = cv.erode(im, kernel)\n",
    "        # im = cv.dilate(im, kernel)\n",
    "        tmp = im.copy()\n",
    "        tmp[tmp != i] = 0\n",
    "        tmp[tmp == i] = 255\n",
    "        # if img == \"05__1_3130-9263_11.png\":\n",
    "        #    cv2.imwrite(\"xd.png\", tmp)\n",
    "        contours, hierachy = cv.findContours(tmp, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "        blank = np.zeros_like(tmp)\n",
    "        for cnt, cont in enumerate(contours):\n",
    "            xmin, ymin, width, height = cv.boundingRect(cont)\n",
    "            if width <= 3 or height <= 3:\n",
    "                continue\n",
    "            # cv.rectangle(im,(x,y),(x+width,y+height),(255),1)\n",
    "            # cv2.imwrite(\"xd.png\", im)\n",
    "            image_width = im.shape[0]\n",
    "            xcenter, ycenter = xmin + width / 2, ymin + height / 2\n",
    "            xcenter, ycenter, width, height = (\n",
    "                xcenter / image_width,\n",
    "                ycenter / image_width,\n",
    "                width / image_width,\n",
    "                height / image_width,\n",
    "            )\n",
    "            if not img in files:\n",
    "                files[img] = [\n",
    "                    (str(i - 1), str(xcenter), str(ycenter), str(width), str(height))\n",
    "                ]\n",
    "            else:\n",
    "\n",
    "                files[img] += [\n",
    "                    (str(i - 1), str(xcenter), str(ycenter), str(width), str(height))\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a93a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = cv.imread(\n",
    "    r\"F:\\source\\repos\\Daten\\FinalHer2512\\train\\images\\05__1_4192-7336_15.png\"\n",
    ")\n",
    "\n",
    "for rec in files[\"05__1_4192-7336_15.png\"]:\n",
    "    xcenter, ycenter, width, height = [int(float(xx) * base.shape[0]) for xx in rec[1:]]\n",
    "    x = int(xcenter - width / 2)\n",
    "    y = int(ycenter - height / 2)\n",
    "    va = int(rec[0] * 50)\n",
    "    print(rec)\n",
    "    print(\"\\nXD\")\n",
    "    cv.circle(base, (xcenter, ycenter), 1, (255, 255, 255), -1)\n",
    "    # cv.rectangle(base,(x,y),(x+width,y+height),(50,va * 50,va * 50),1)\n",
    "cv.imwrite(\"xd4.png\", base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c82d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv.imread(\n",
    "    os.path.join(r\"E:\\source\\repos\\Daten\\Cells\\train\\images\", \"05__1_3115_10030.png\"), 0\n",
    ")\n",
    "kernel = np.ones((2, 2), np.uint8)\n",
    "image = cv.erode(im, kernel)\n",
    "contours, hierachy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "blank = np.zeros_like(im)\n",
    "for cnt, cont in enumerate(contours):\n",
    "    x, y, width, height = cv.boundingRect(cont)\n",
    "    cv.rectangle(im, (x, y), (x + width, y + height), (255), 1)\n",
    "cv2.imwrite(\"xd.png\", im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51cb372",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_anno = [item for item in all_files if item not in list(files.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d35ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e9b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"labels\")\n",
    "for cnt, (key, val) in enumerate(files.items()):\n",
    "    with open(\"labels/\" + key.replace(\".png\", \".txt\"), \"w\") as handle:\n",
    "        for va in val:\n",
    "            handle.write(\" \".join(list(va)) + \"\\n\")\n",
    "\n",
    "for name in no_anno:\n",
    "    with open(\"labels/\" + name.replace(\".png\", \".txt\"), \"w\") as handle:\n",
    "        handle.write(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891f647a",
   "metadata": {},
   "source": [
    "# Export for instance segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50876230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abacfd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"labels\")\n",
    "os.mkdir(\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86410f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_rec(img, rec):\n",
    "    x, y, w, h = rec\n",
    "    return img[y : y + h, x : x + w]\n",
    "\n",
    "\n",
    "def add_rec(orig, img, rec):\n",
    "    x, y, w, h = rec\n",
    "    # r,g,b = [random.randint(20, 255) for i in range(3)]\n",
    "    img[img > 0] = random.randint(20, 255)\n",
    "    tmp = np.expand_dims(img.astype(np.uint8), axis=-1)\n",
    "    # tmp[np.all(tmp == (255, 255, 255), axis=-1)] = (b,g,r)\n",
    "    orig[y : y + h, x : x + w] += tmp\n",
    "    return orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e98f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = A.Compose(\n",
    "    [\n",
    "        A.PadIfNeeded(64, 64, value=0, border_mode=0),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c44bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "labels = r\"E:\\source\\repos\\Daten\\Cells\\train\\labels\"\n",
    "images = r\"E:\\source\\repos\\Daten\\Cells\\train\\images\"\n",
    "to_labels = r\"E:\\source\\repos\\VisualDL\\custom_experiments\\labels\"\n",
    "to_images = r\"E:\\source\\repos\\VisualDL\\custom_experiments\\images\"\n",
    "all_files = os.listdir(labels)\n",
    "adding = 0\n",
    "for img in os.listdir(labels):\n",
    "    im = cv.imread(os.path.join(labels, img), 0)\n",
    "    original = cv.imread(os.path.join(images, img))\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    image = cv.erode(im, kernel)\n",
    "    contours, hierachy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    for cnt, cont in enumerate(contours):\n",
    "        blank = np.zeros_like(im)\n",
    "        xmin, ymin, width, height = cv.boundingRect(cont)\n",
    "        xmin -= 5\n",
    "        ymin -= 5\n",
    "        width += 5\n",
    "        height += 5\n",
    "        xmin = min(0, xmin)\n",
    "        ymin = min(0, ymin)\n",
    "        cv.drawContours(blank, [cont], -1, 1, -1)\n",
    "        final = cut_rec(blank, (xmin, ymin, width, height))\n",
    "        orig_final = cut_rec(original, (xmin, ymin, width, height))\n",
    "        trans = pad(image=orig_final, mask=final)\n",
    "        final = trans[\"mask\"]\n",
    "        orig_final = trans[\"image\"]\n",
    "        cv.imwrite(os.path.join(to_labels, img.replace(\".png\", f\"{cnt}.png\")), final)\n",
    "        cv.imwrite(\n",
    "            os.path.join(to_images, img.replace(\".png\", f\"{cnt}.png\")), orig_final\n",
    "        )\n",
    "        # cv.rectangle(im,(x,y),(x+width,y+height),(255),1)\n",
    "\n",
    "        # cv2.imwrite(\"xd.png\", im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b4cab0",
   "metadata": {},
   "source": [
    "# instance segmentation inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc80695",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.UnetPlusPlus(encoder_name=\"resnext50_32x4d\", classes=2, in_channels=3)\n",
    "model.load_state_dict(\n",
    "    torch.load(r\"E:\\source\\repos\\VisualDL\\resnext50_32x4d, UnetPlusPlus.pt\")\n",
    ")\n",
    "model.eval()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9078c0",
   "metadata": {},
   "source": [
    "## simple predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed2ae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose(\n",
    "    [\n",
    "        # A.PadIfNeeded(64,64, value = 0, border_mode = 0),\n",
    "        A.Resize(width=64, height=64),\n",
    "    ]\n",
    ")\n",
    "test = r\"E:\\source\\repos\\VisualDL\\custom_experiments\\valid\\images\"\n",
    "for cnt, im in enumerate(os.listdir(test)):\n",
    "    image = io.imread(os.path.join(test, im))\n",
    "    image = transform(image=image)[\"image\"]\n",
    "    image = image / 255.0\n",
    "    s = torch.unsqueeze(\n",
    "        torch.tensor(image, dtype=torch.float).permute(2, 0, 1), 0\n",
    "    ).cuda()\n",
    "    with torch.cuda.amp.autocast():\n",
    "        preds = model(s)\n",
    "    preds = torch.argmax(preds, 1)\n",
    "    preds *= 255\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    io.imsave(f\"{im}.png\", preds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf9c6b5",
   "metadata": {},
   "source": [
    "# use rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8452363",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose(\n",
    "    [\n",
    "        A.PadIfNeeded(64, 64, value=0, border_mode=0),\n",
    "        A.Resize(width=64, height=64),\n",
    "    ]\n",
    ")\n",
    "\n",
    "files = {}\n",
    "labels = r\"E:\\source\\repos\\Daten\\Cells\\valid\\labels\"\n",
    "images = r\"E:\\source\\repos\\Daten\\Cells\\valid\\images\"\n",
    "to = r\"E:\\source\\repos\\VisualDL\\custom_experiments\\test\"\n",
    "all_files = os.listdir(labels)\n",
    "adding = 0\n",
    "for ii, img in enumerate(os.listdir(labels)):\n",
    "    im = cv.imread(os.path.join(labels, img), 0)\n",
    "    print(img)\n",
    "    original = cv.imread(os.path.join(images, img))\n",
    "    blank = np.zeros_like(original)\n",
    "    original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    image = cv.erode(im, kernel)\n",
    "    contours, hierachy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "    for cnt, cont in enumerate(contours):\n",
    "\n",
    "        xmin, ymin, width, height = cv.boundingRect(cont)\n",
    "        orig_final = cut_rec(original, (xmin, ymin, width, height))\n",
    "        tt = orig_final.copy()\n",
    "        orig_final = transform(image=orig_final)[\"image\"]\n",
    "        image = orig_final / 255.0\n",
    "        s = torch.unsqueeze(\n",
    "            torch.tensor(image, dtype=torch.float).permute(2, 0, 1), 0\n",
    "        ).cuda()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            preds = model(s)\n",
    "        preds = torch.argmax(preds, 1)\n",
    "        preds *= 255\n",
    "        preds = preds.detach().cpu().numpy()[0]\n",
    "\n",
    "        preds = cut_rec(\n",
    "            preds, (32 - int(width / 2), 32 - int(height / 2), width, height)\n",
    "        )\n",
    "        # contours,hierachy = cv.findContours(image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "        blank = add_rec(blank, preds, (xmin, ymin, width, height))\n",
    "        cv2.imwrite(os.path.join(to, f\"{img}{cnt}.png\"), preds)\n",
    "        cv2.imwrite(os.path.join(to, f\"{img}{cnt}orig.png\"), tt)\n",
    "    cv2.imwrite(os.path.join(to, f\"{img}{cnt}xdddd.png\"), blank)\n",
    "    if ii == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658f0779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a3aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "skimage.skimage.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de6e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = U2NET(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aa5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_in = torch.randn(1, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0201e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, einsum\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from custom import ViT, AxialImageTransformer, AxialAttention\n",
    "\n",
    "# helpers\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "\n",
    "def divisible_by(val, divisor):\n",
    "    return (val % divisor) == 0\n",
    "\n",
    "\n",
    "def unfold_output_size(image_size, kernel_size, stride, padding):\n",
    "    return int(((image_size - kernel_size + (2 * padding)) / stride) + 1)\n",
    "\n",
    "\n",
    "# classes\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult=4, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, *, dim, heads=8, dim_head=64, dropout=0.0):\n",
    "        super().__init__()\n",
    "        inner_dim = heads * dim_head\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head**-0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(nn.Linear(inner_dim, dim), nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, d, h = *x.shape, self.heads\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> (b h) n d\", h=h), (q, k, v))\n",
    "\n",
    "        sim = einsum(\"b i d, b j d -> b i j\", q, k) * self.scale\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        out = einsum(\"b i j, b j d -> b i d\", attn, v)\n",
    "        out = rearrange(out, \"(b h) n d -> b n (h d)\", h=h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "# main class\n",
    "\n",
    "\n",
    "class TNT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        image_size,\n",
    "        patch_dim,\n",
    "        pixel_dim,\n",
    "        patch_size,\n",
    "        pixel_size,\n",
    "        depth,\n",
    "        heads=8,\n",
    "        dim_head=64,\n",
    "        ff_dropout=0.0,\n",
    "        attn_dropout=0.0,\n",
    "        channels=3,\n",
    "        unfold_args=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert divisible_by(\n",
    "            image_size, patch_size\n",
    "        ), \"image size must be divisible by patch size\"\n",
    "        assert divisible_by(\n",
    "            patch_size, pixel_size\n",
    "        ), \"patch size must be divisible by pixel size for now\"\n",
    "\n",
    "        num_patch_tokens = (image_size // patch_size) ** 2\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patch_tokens = nn.Parameter(torch.randn(num_patch_tokens + 1, patch_dim))\n",
    "\n",
    "        unfold_args = default(unfold_args, (pixel_size, pixel_size, 0))\n",
    "        unfold_args = (*unfold_args, 0) if len(unfold_args) == 2 else unfold_args\n",
    "        kernel_size, stride, padding = unfold_args\n",
    "\n",
    "        pixel_width = unfold_output_size(patch_size, kernel_size, stride, padding)\n",
    "        num_pixels = pixel_width**2\n",
    "\n",
    "        self.to_pixel_tokens = nn.Sequential(\n",
    "            Rearrange(\n",
    "                \"b c (h p1) (w p2) -> (b h w) c p1 p2\", p1=patch_size, p2=patch_size\n",
    "            ),\n",
    "            nn.Unfold(kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            Rearrange(\"... c n -> ... n c\"),\n",
    "            nn.Linear(channels * kernel_size**2, pixel_dim),\n",
    "        )\n",
    "\n",
    "        self.patch_pos_emb = nn.Parameter(torch.randn(num_patch_tokens + 1, patch_dim))\n",
    "        self.pixel_pos_emb = nn.Parameter(torch.randn(num_pixels, pixel_dim))\n",
    "\n",
    "        layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "\n",
    "            pixel_to_patch = nn.Sequential(\n",
    "                nn.LayerNorm(pixel_dim),\n",
    "                Rearrange(\"... n d -> ... (n d)\"),\n",
    "                nn.Linear(pixel_dim * num_pixels, patch_dim),\n",
    "            )\n",
    "\n",
    "            layers.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        PreNorm(\n",
    "                            pixel_dim,\n",
    "                            Attention(\n",
    "                                dim=pixel_dim,\n",
    "                                heads=heads,\n",
    "                                dim_head=dim_head,\n",
    "                                dropout=attn_dropout,\n",
    "                            ),\n",
    "                        ),\n",
    "                        PreNorm(\n",
    "                            pixel_dim, FeedForward(dim=pixel_dim, dropout=ff_dropout)\n",
    "                        ),\n",
    "                        pixel_to_patch,\n",
    "                        PreNorm(\n",
    "                            patch_dim,\n",
    "                            Attention(\n",
    "                                dim=patch_dim,\n",
    "                                heads=heads,\n",
    "                                dim_head=dim_head,\n",
    "                                dropout=attn_dropout,\n",
    "                            ),\n",
    "                        ),\n",
    "                        PreNorm(\n",
    "                            patch_dim, FeedForward(dim=patch_dim, dropout=ff_dropout)\n",
    "                        ),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, _, h, w, patch_size, image_size = *x.shape, self.patch_size, self.image_size\n",
    "        assert divisible_by(h, patch_size) and divisible_by(\n",
    "            w, patch_size\n",
    "        ), f\"height {h} and width {w} of input must be divisible by the patch size\"\n",
    "\n",
    "        num_patches_h = h // patch_size\n",
    "        num_patches_w = w // patch_size\n",
    "        n = num_patches_w * num_patches_h\n",
    "\n",
    "        pixels = self.to_pixel_tokens(x)\n",
    "        patches = repeat(self.patch_tokens[: (n + 1)], \"n d -> b n d\", b=b)\n",
    "\n",
    "        patches += rearrange(self.patch_pos_emb[: (n + 1)], \"n d -> () n d\")\n",
    "        pixels += rearrange(self.pixel_pos_emb, \"n d -> () n d\")\n",
    "\n",
    "        for (\n",
    "            pixel_attn,\n",
    "            pixel_ff,\n",
    "            pixel_to_patch_residual,\n",
    "            patch_attn,\n",
    "            patch_ff,\n",
    "        ) in self.layers:\n",
    "\n",
    "            pixels = pixel_attn(pixels) + pixels\n",
    "            pixels = pixel_ff(pixels) + pixels\n",
    "\n",
    "            patches_residual = pixel_to_patch_residual(pixels)\n",
    "\n",
    "            patches_residual = rearrange(\n",
    "                patches_residual,\n",
    "                \"(b h w) d -> b (h w) d\",\n",
    "                h=num_patches_h,\n",
    "                w=num_patches_w,\n",
    "            )\n",
    "            patches_residual = F.pad(\n",
    "                patches_residual, (0, 0, 1, 0), value=0\n",
    "            )  # cls token gets residual of 0\n",
    "            patches = patches + patches_residual\n",
    "\n",
    "            patches = patch_attn(patches) + patches\n",
    "            patches = patch_ff(patches) + patches\n",
    "        hidden_states = patches[:, 1:, :]\n",
    "        (\n",
    "            B,\n",
    "            n_patch,\n",
    "            hidden,\n",
    "        ) = (\n",
    "            hidden_states.size()\n",
    "        )  # reshape from (B, n_patch, hidden) to (B, h, w, hidden)\n",
    "        h, w = int(np.sqrt(n_patch)), int(np.sqrt(n_patch))\n",
    "        x = hidden_states.permute(0, 2, 1)\n",
    "        x = x.contiguous().view(B, hidden, h, w)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv2dReLU(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        padding=0,\n",
    "        stride=1,\n",
    "        use_batchnorm=True,\n",
    "    ):\n",
    "\n",
    "        if use_batchnorm == \"inplace\" and InPlaceABN is None:\n",
    "            raise RuntimeError(\n",
    "                \"In order to use `use_batchnorm='inplace'` inplace_abn package must be installed. \"\n",
    "                + \"To install see: https://github.com/mapillary/inplace_abn\"\n",
    "            )\n",
    "\n",
    "        conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=not (use_batchnorm),\n",
    "        )\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        if use_batchnorm == \"inplace\":\n",
    "            bn = InPlaceABN(out_channels, activation=\"leaky_relu\", activation_param=0.0)\n",
    "            relu = nn.Identity()\n",
    "\n",
    "        elif use_batchnorm and use_batchnorm != \"inplace\":\n",
    "            bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        else:\n",
    "            bn = nn.Identity()\n",
    "\n",
    "        super(Conv2dReLU, self).__init__(conv, bn, relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1990a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "init_dim = 16\n",
    "conv = Conv2dReLU(3, init_dim, 1).cuda()\n",
    "inst = AxialImageTransformer(\n",
    "    dim=init_dim, depth=2, axial_pos_emb_shape=(image_size, image_size)\n",
    ").cuda()\n",
    "\n",
    "first = TNT(\n",
    "    image_size=image_size,\n",
    "    patch_size=2,\n",
    "    pixel_dim=16,\n",
    "    pixel_size=1,\n",
    "    patch_dim=init_dim * 2,\n",
    "    depth=2,\n",
    "    heads=4,\n",
    "    channels=init_dim,\n",
    ").cuda()\n",
    "second = TNT(\n",
    "    image_size=image_size // 2,\n",
    "    patch_size=2,\n",
    "    pixel_dim=16,\n",
    "    pixel_size=1,\n",
    "    patch_dim=init_dim * 4,\n",
    "    depth=4,\n",
    "    heads=6,\n",
    "    channels=init_dim * 2,\n",
    ").cuda()\n",
    "third = TNT(\n",
    "    image_size=image_size // 4,\n",
    "    patch_size=2,\n",
    "    pixel_dim=16,\n",
    "    pixel_size=1,\n",
    "    patch_dim=init_dim * 8,\n",
    "    depth=6,\n",
    "    heads=8,\n",
    "    channels=init_dim * 4,\n",
    ").cuda()\n",
    "fourth = TNT(\n",
    "    image_size=image_size // 8,\n",
    "    patch_size=2,\n",
    "    pixel_dim=16,\n",
    "    pixel_size=1,\n",
    "    patch_dim=init_dim * 16,\n",
    "    depth=8,\n",
    "    heads=12,\n",
    "    channels=init_dim * 8,\n",
    ").cuda()\n",
    "cup = ViT(\n",
    "    image_size=8,\n",
    "    patch_size=1,\n",
    "    dim=init_dim * 32,\n",
    "    depth=6,\n",
    "    heads=12,\n",
    "    mlp_dim=2048,\n",
    "    dropout=0.1,\n",
    "    emb_dropout=0.1,\n",
    "    channels=init_dim * 16,\n",
    ").cuda()\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels, depth, size, heads=2):\n",
    "        super().__init__()\n",
    "        self.ax = AxialImageTransformer(\n",
    "            dim=in_channels + skip_channels,\n",
    "            heads=heads,\n",
    "            depth=depth,\n",
    "            axial_pos_emb_shape=(size, size),\n",
    "        ).cuda()\n",
    "        self.out = Conv2dReLU(in_channels + skip_channels, out_channels, 1)\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        return self.out(self.ax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2090bb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = 2\n",
    "up1 = DecoderBlock(\n",
    "    init_dim * 32, init_dim * 8, init_dim * 8, depth=2, heads=2, size=image_size // 8\n",
    ").cuda()\n",
    "up2 = DecoderBlock(\n",
    "    init_dim * 8, init_dim * 4, init_dim * 4, depth=2, heads=2, size=image_size // 4\n",
    ").cuda()\n",
    "up3 = DecoderBlock(\n",
    "    init_dim * 4, init_dim * 2, init_dim * 2, depth=2, heads=2, size=image_size // 2\n",
    ").cuda()\n",
    "up4 = DecoderBlock(init_dim * 2, init_dim, 2, depth=2, heads=2, size=image_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abe12ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = AxialAttention(32, dim_index=1).cuda()\n",
    "a = AxialImageTransformer(\n",
    "    dim=3072, depth=1, axial_pos_emb_shape=(128, 128), heads=1\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ce9eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_in = torch.randn(2, 3072, 128, 128).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ddc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "a(dummy_in).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c640cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inst(conv(dummy_in)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b26bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(hidden_states):\n",
    "    (\n",
    "        B,\n",
    "        n_patch,\n",
    "        hidden,\n",
    "    ) = hidden_states.size()  # reshape from (B, n_patch, hidden) to (B, h, w, hidden)\n",
    "    h, w = int(np.sqrt(n_patch)), int(np.sqrt(n_patch))\n",
    "    x = hidden_states.permute(0, 2, 1)\n",
    "    x = x.contiguous().view(B, hidden, h, w)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a3629",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.cuda.amp.autocast():\n",
    "    out1 = inst(conv(dummy_in))  # 128\n",
    "with torch.cuda.amp.autocast():\n",
    "    out2 = first(out1)  # 64\n",
    "with torch.cuda.amp.autocast():\n",
    "    out3 = second(out2)  # 32\n",
    "with torch.cuda.amp.autocast():\n",
    "    out4 = third(out3)  # 16\n",
    "with torch.cuda.amp.autocast():\n",
    "    out5 = fourth(out4)  # 8\n",
    "with torch.cuda.amp.autocast():\n",
    "    out6 = cup(out5)  # 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca05747f",
   "metadata": {},
   "outputs": [],
   "source": [
    "up4(up3(up2(up1(out6, out4), out3), out2), out1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2127a1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = out\n",
    "(\n",
    "    B,\n",
    "    n_patch,\n",
    "    hidden,\n",
    ") = hidden_states.size()  # reshape from (B, n_patch, hidden) to (B, h, w, hidden)\n",
    "h, w = int(np.sqrt(n_patch)), int(np.sqrt(n_patch))\n",
    "x = hidden_states.permute(0, 2, 1)\n",
    "x = x.contiguous().view(B, hidden, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ea3048",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e79fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "2**5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e0f671",
   "metadata": {},
   "source": [
    "# HRNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d30d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "from torch.nn.parallel._functions import ReduceAddCoalesced, Broadcast\n",
    "\n",
    "\n",
    "class SyncMaster(object):\n",
    "    \"\"\"An abstract `SyncMaster` object.\n",
    "    - During the replication, as the data parallel will trigger an callback of each module, all slave devices should\n",
    "    call `register(id)` and obtain an `SlavePipe` to communicate with the master.\n",
    "    - During the forward pass, master device invokes `run_master`, all messages from slave devices will be collected,\n",
    "    and passed to a registered callback.\n",
    "    - After receiving the messages, the master device should gather the information and determine to message passed\n",
    "    back to each slave devices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, master_callback):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            master_callback: a callback to be invoked after having collected messages from slave devices.\n",
    "        \"\"\"\n",
    "        self._master_callback = master_callback\n",
    "        self._queue = queue.Queue()\n",
    "        self._registry = collections.OrderedDict()\n",
    "        self._activated = False\n",
    "\n",
    "    def register_slave(self, identifier):\n",
    "        \"\"\"\n",
    "        Register an slave device.\n",
    "        Args:\n",
    "            identifier: an identifier, usually is the device id.\n",
    "        Returns: a `SlavePipe` object which can be used to communicate with the master device.\n",
    "        \"\"\"\n",
    "        if self._activated:\n",
    "            assert self._queue.empty(), \"Queue is not clean before next initialization.\"\n",
    "            self._activated = False\n",
    "            self._registry.clear()\n",
    "        future = FutureResult()\n",
    "        self._registry[identifier] = _MasterRegistry(future)\n",
    "        return SlavePipe(identifier, self._queue, future)\n",
    "\n",
    "\n",
    "class _SynchronizedBatchNorm(_BatchNorm):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.001, affine=True):\n",
    "        super(_SynchronizedBatchNorm, self).__init__(\n",
    "            num_features, eps=eps, momentum=momentum, affine=affine\n",
    "        )\n",
    "\n",
    "        self._sync_master = SyncMaster(self._data_parallel_master)\n",
    "\n",
    "        self._is_parallel = False\n",
    "        self._parallel_id = None\n",
    "        self._slave_pipe = None\n",
    "\n",
    "        # customed batch norm statistics\n",
    "        self._moving_average_fraction = 1.0 - momentum\n",
    "        self.register_buffer(\"_tmp_running_mean\", torch.zeros(self.num_features))\n",
    "        self.register_buffer(\"_tmp_running_var\", torch.ones(self.num_features))\n",
    "        self.register_buffer(\"_running_iter\", torch.ones(1))\n",
    "        self._tmp_running_mean = self.running_mean.clone() * self._running_iter\n",
    "        self._tmp_running_var = self.running_var.clone() * self._running_iter\n",
    "\n",
    "    def forward(self, input):\n",
    "        # If it is not parallel computation or is in evaluation mode, use PyTorch's implementation.\n",
    "        if not (self._is_parallel and self.training):\n",
    "            return F.batch_norm(\n",
    "                input,\n",
    "                self.running_mean,\n",
    "                self.running_var,\n",
    "                self.weight,\n",
    "                self.bias,\n",
    "                self.training,\n",
    "                self.momentum,\n",
    "                self.eps,\n",
    "            )\n",
    "\n",
    "        # Resize the input to (B, C, -1).\n",
    "        input_shape = input.size()\n",
    "        input = input.view(input.size(0), self.num_features, -1)\n",
    "\n",
    "        # Compute the sum and square-sum.\n",
    "        sum_size = input.size(0) * input.size(2)\n",
    "        input_sum = _sum_ft(input)\n",
    "        input_ssum = _sum_ft(input**2)\n",
    "\n",
    "        # Reduce-and-broadcast the statistics.\n",
    "        if self._parallel_id == 0:\n",
    "            mean, inv_std = self._sync_master.run_master(\n",
    "                _ChildMessage(input_sum, input_ssum, sum_size)\n",
    "            )\n",
    "        else:\n",
    "            mean, inv_std = self._slave_pipe.run_slave(\n",
    "                _ChildMessage(input_sum, input_ssum, sum_size)\n",
    "            )\n",
    "\n",
    "        # Compute the output.\n",
    "        if self.affine:\n",
    "            # MJY:: Fuse the multiplication for speed.\n",
    "            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(\n",
    "                inv_std * self.weight\n",
    "            ) + _unsqueeze_ft(self.bias)\n",
    "        else:\n",
    "            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std)\n",
    "\n",
    "        # Reshape it.\n",
    "        return output.view(input_shape)\n",
    "\n",
    "    def __data_parallel_replicate__(self, ctx, copy_id):\n",
    "        self._is_parallel = True\n",
    "        self._parallel_id = copy_id\n",
    "\n",
    "        # parallel_id == 0 means master device.\n",
    "        if self._parallel_id == 0:\n",
    "            ctx.sync_master = self._sync_master\n",
    "        else:\n",
    "            self._slave_pipe = ctx.sync_master.register_slave(copy_id)\n",
    "\n",
    "    def _data_parallel_master(self, intermediates):\n",
    "        \"\"\"Reduce the sum and square-sum, compute the statistics, and broadcast it.\"\"\"\n",
    "        intermediates = sorted(intermediates, key=lambda i: i[1].sum.get_device())\n",
    "\n",
    "        to_reduce = [i[1][:2] for i in intermediates]\n",
    "        to_reduce = [j for i in to_reduce for j in i]  # flatten\n",
    "        target_gpus = [i[1].sum.get_device() for i in intermediates]\n",
    "\n",
    "        sum_size = sum([i[1].sum_size for i in intermediates])\n",
    "        sum_, ssum = ReduceAddCoalesced.apply(target_gpus[0], 2, *to_reduce)\n",
    "\n",
    "        mean, inv_std = self._compute_mean_std(sum_, ssum, sum_size)\n",
    "\n",
    "        broadcasted = Broadcast.apply(target_gpus, mean, inv_std)\n",
    "\n",
    "        outputs = []\n",
    "        for i, rec in enumerate(intermediates):\n",
    "            outputs.append((rec[0], _MasterMessage(*broadcasted[i * 2 : i * 2 + 2])))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _add_weighted(self, dest, delta, alpha=1, beta=1, bias=0):\n",
    "        \"\"\"return *dest* by `dest := dest*alpha + delta*beta + bias`\"\"\"\n",
    "        return dest * alpha + delta * beta + bias\n",
    "\n",
    "    def _compute_mean_std(self, sum_, ssum, size):\n",
    "        \"\"\"Compute the mean and standard-deviation with sum and square-sum. This method\n",
    "        also maintains the moving average on the master device.\"\"\"\n",
    "        assert (\n",
    "            size > 1\n",
    "        ), \"BatchNorm computes unbiased standard-deviation, which requires size > 1.\"\n",
    "        mean = sum_ / size\n",
    "        sumvar = ssum - sum_ * mean\n",
    "        unbias_var = sumvar / (size - 1)\n",
    "        bias_var = sumvar / size\n",
    "\n",
    "        self._tmp_running_mean = self._add_weighted(\n",
    "            self._tmp_running_mean, mean.data, alpha=self._moving_average_fraction\n",
    "        )\n",
    "        self._tmp_running_var = self._add_weighted(\n",
    "            self._tmp_running_var, unbias_var.data, alpha=self._moving_average_fraction\n",
    "        )\n",
    "        self._running_iter = self._add_weighted(\n",
    "            self._running_iter, 1, alpha=self._moving_average_fraction\n",
    "        )\n",
    "\n",
    "        self.running_mean = self._tmp_running_mean / self._running_iter\n",
    "        self.running_var = self._tmp_running_var / self._running_iter\n",
    "\n",
    "        return mean, bias_var.clamp(self.eps) ** -0.5\n",
    "\n",
    "\n",
    "class SynchronizedBatchNorm2d(_SynchronizedBatchNorm):\n",
    "    r\"\"\"Applies Batch Normalization over a 4d input that is seen as a mini-batch\n",
    "    of 3d inputs\n",
    "    .. math::\n",
    "        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
    "    This module differs from the built-in PyTorch BatchNorm2d as the mean and\n",
    "    standard-deviation are reduced across all devices during training.\n",
    "    For example, when one uses `nn.DataParallel` to wrap the network during\n",
    "    training, PyTorch's implementation normalize the tensor on each device using\n",
    "    the statistics only on that device, which accelerated the computation and\n",
    "    is also easy to implement, but the statistics might be inaccurate.\n",
    "    Instead, in this synchronized version, the statistics will be computed\n",
    "    over all training samples distributed on multiple devices.\n",
    "\n",
    "    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n",
    "    as the built-in PyTorch implementation.\n",
    "    The mean and standard-deviation are calculated per-dimension over\n",
    "    the mini-batches and gamma and beta are learnable parameter vectors\n",
    "    of size C (where C is the input size).\n",
    "    During training, this layer keeps a running estimate of its computed mean\n",
    "    and variance. The running sum is kept with a default momentum of 0.1.\n",
    "    During evaluation, this running mean/variance is used for normalization.\n",
    "    Because the BatchNorm is done over the `C` dimension, computing statistics\n",
    "    on `(N, H, W)` slices, it's common terminology to call this Spatial BatchNorm\n",
    "    Args:\n",
    "        num_features: num_features from an expected input of\n",
    "            size batch_size x num_features x height x width\n",
    "        eps: a value added to the denominator for numerical stability.\n",
    "            Default: 1e-5\n",
    "        momentum: the value used for the running_mean and running_var\n",
    "            computation. Default: 0.1\n",
    "        affine: a boolean value that when set to ``True``, gives the layer learnable\n",
    "            affine parameters. Default: ``True``\n",
    "    Shape:\n",
    "        - Input: :math:`(N, C, H, W)`\n",
    "        - Output: :math:`(N, C, H, W)` (same shape as input)\n",
    "    Examples:\n",
    "        >>> # With Learnable Parameters\n",
    "        >>> m = SynchronizedBatchNorm2d(100)\n",
    "        >>> # Without Learnable Parameters\n",
    "        >>> m = SynchronizedBatchNorm2d(100, affine=False)\n",
    "        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45))\n",
    "        >>> output = m(input)\n",
    "    \"\"\"\n",
    "\n",
    "    def _check_input_dim(self, input):\n",
    "        if input.dim() != 4:\n",
    "            raise ValueError(\"expected 4D input (got {}D input)\".format(input.dim()))\n",
    "        super(SynchronizedBatchNorm2d, self)._check_input_dim(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5289bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This HRNet implementation is modified from the following repository:\n",
    "https://github.com/HRNet/HRNet-Semantic-Segmentation\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "BatchNorm2d = SynchronizedBatchNorm2d\n",
    "BN_MOMENTUM = 0.1\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "__all__ = [\"hrnetv2\"]\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    \"hrnetv2\": \"http://sceneparsing.csail.mit.edu/model/pretrained_resnet/hrnetv2_w48-imagenet.pth\",\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "    )\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            planes, planes * self.expansion, kernel_size=1, bias=False\n",
    "        )\n",
    "        self.bn3 = BatchNorm2d(planes * self.expansion, momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class HighResolutionModule(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_branches,\n",
    "        blocks,\n",
    "        num_blocks,\n",
    "        num_inchannels,\n",
    "        num_channels,\n",
    "        fuse_method,\n",
    "        multi_scale_output=True,\n",
    "    ):\n",
    "        super(HighResolutionModule, self).__init__()\n",
    "        self._check_branches(\n",
    "            num_branches, blocks, num_blocks, num_inchannels, num_channels\n",
    "        )\n",
    "\n",
    "        self.num_inchannels = num_inchannels\n",
    "        self.fuse_method = fuse_method\n",
    "        self.num_branches = num_branches\n",
    "\n",
    "        self.multi_scale_output = multi_scale_output\n",
    "\n",
    "        self.branches = self._make_branches(\n",
    "            num_branches, blocks, num_blocks, num_channels\n",
    "        )\n",
    "        self.fuse_layers = self._make_fuse_layers()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def _check_branches(\n",
    "        self, num_branches, blocks, num_blocks, num_inchannels, num_channels\n",
    "    ):\n",
    "        if num_branches != len(num_blocks):\n",
    "            error_msg = \"NUM_BRANCHES({}) <> NUM_BLOCKS({})\".format(\n",
    "                num_branches, len(num_blocks)\n",
    "            )\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "        if num_branches != len(num_channels):\n",
    "            error_msg = \"NUM_BRANCHES({}) <> NUM_CHANNELS({})\".format(\n",
    "                num_branches, len(num_channels)\n",
    "            )\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "        if num_branches != len(num_inchannels):\n",
    "            error_msg = \"NUM_BRANCHES({}) <> NUM_INCHANNELS({})\".format(\n",
    "                num_branches, len(num_inchannels)\n",
    "            )\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "    def _make_one_branch(self, branch_index, block, num_blocks, num_channels, stride=1):\n",
    "        downsample = None\n",
    "        if (\n",
    "            stride != 1\n",
    "            or self.num_inchannels[branch_index]\n",
    "            != num_channels[branch_index] * block.expansion\n",
    "        ):\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    self.num_inchannels[branch_index],\n",
    "                    num_channels[branch_index] * block.expansion,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                BatchNorm2d(\n",
    "                    num_channels[branch_index] * block.expansion, momentum=BN_MOMENTUM\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.num_inchannels[branch_index],\n",
    "                num_channels[branch_index],\n",
    "                stride,\n",
    "                downsample,\n",
    "            )\n",
    "        )\n",
    "        self.num_inchannels[branch_index] = num_channels[branch_index] * block.expansion\n",
    "        for i in range(1, num_blocks[branch_index]):\n",
    "            layers.append(\n",
    "                block(self.num_inchannels[branch_index], num_channels[branch_index])\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n",
    "        branches = []\n",
    "\n",
    "        for i in range(num_branches):\n",
    "            branches.append(self._make_one_branch(i, block, num_blocks, num_channels))\n",
    "\n",
    "        return nn.ModuleList(branches)\n",
    "\n",
    "    def _make_fuse_layers(self):\n",
    "        if self.num_branches == 1:\n",
    "            return None\n",
    "\n",
    "        num_branches = self.num_branches\n",
    "        num_inchannels = self.num_inchannels\n",
    "        fuse_layers = []\n",
    "        for i in range(num_branches if self.multi_scale_output else 1):\n",
    "            fuse_layer = []\n",
    "            for j in range(num_branches):\n",
    "                if j > i:\n",
    "                    fuse_layer.append(\n",
    "                        nn.Sequential(\n",
    "                            nn.Conv2d(\n",
    "                                num_inchannels[j],\n",
    "                                num_inchannels[i],\n",
    "                                1,\n",
    "                                1,\n",
    "                                0,\n",
    "                                bias=False,\n",
    "                            ),\n",
    "                            BatchNorm2d(num_inchannels[i], momentum=BN_MOMENTUM),\n",
    "                        )\n",
    "                    )\n",
    "                elif j == i:\n",
    "                    fuse_layer.append(None)\n",
    "                else:\n",
    "                    conv3x3s = []\n",
    "                    for k in range(i - j):\n",
    "                        if k == i - j - 1:\n",
    "                            num_outchannels_conv3x3 = num_inchannels[i]\n",
    "                            conv3x3s.append(\n",
    "                                nn.Sequential(\n",
    "                                    nn.Conv2d(\n",
    "                                        num_inchannels[j],\n",
    "                                        num_outchannels_conv3x3,\n",
    "                                        3,\n",
    "                                        2,\n",
    "                                        1,\n",
    "                                        bias=False,\n",
    "                                    ),\n",
    "                                    BatchNorm2d(\n",
    "                                        num_outchannels_conv3x3, momentum=BN_MOMENTUM\n",
    "                                    ),\n",
    "                                )\n",
    "                            )\n",
    "                        else:\n",
    "                            num_outchannels_conv3x3 = num_inchannels[j]\n",
    "                            conv3x3s.append(\n",
    "                                nn.Sequential(\n",
    "                                    nn.Conv2d(\n",
    "                                        num_inchannels[j],\n",
    "                                        num_outchannels_conv3x3,\n",
    "                                        3,\n",
    "                                        2,\n",
    "                                        1,\n",
    "                                        bias=False,\n",
    "                                    ),\n",
    "                                    BatchNorm2d(\n",
    "                                        num_outchannels_conv3x3, momentum=BN_MOMENTUM\n",
    "                                    ),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                )\n",
    "                            )\n",
    "                    fuse_layer.append(nn.Sequential(*conv3x3s))\n",
    "            fuse_layers.append(nn.ModuleList(fuse_layer))\n",
    "\n",
    "        return nn.ModuleList(fuse_layers)\n",
    "\n",
    "    def get_num_inchannels(self):\n",
    "        return self.num_inchannels\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.num_branches == 1:\n",
    "            return [self.branches[0](x[0])]\n",
    "\n",
    "        for i in range(self.num_branches):\n",
    "            x[i] = self.branches[i](x[i])\n",
    "\n",
    "        x_fuse = []\n",
    "        for i in range(len(self.fuse_layers)):\n",
    "            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n",
    "            for j in range(1, self.num_branches):\n",
    "                if i == j:\n",
    "                    y = y + x[j]\n",
    "                elif j > i:\n",
    "                    width_output = x[i].shape[-1]\n",
    "                    height_output = x[i].shape[-2]\n",
    "                    y = y + F.interpolate(\n",
    "                        self.fuse_layers[i][j](x[j]),\n",
    "                        size=(height_output, width_output),\n",
    "                        mode=\"bilinear\",\n",
    "                        align_corners=False,\n",
    "                    )\n",
    "                else:\n",
    "                    y = y + self.fuse_layers[i][j](x[j])\n",
    "            x_fuse.append(self.relu(y))\n",
    "\n",
    "        return x_fuse\n",
    "\n",
    "\n",
    "blocks_dict = {\"BASIC\": BasicBlock, \"BOTTLENECK\": Bottleneck}\n",
    "\n",
    "\n",
    "class Conv2dReLU(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        padding=0,\n",
    "        stride=1,\n",
    "        use_batchnorm=True,\n",
    "    ):\n",
    "\n",
    "        if use_batchnorm == \"inplace\" and InPlaceABN is None:\n",
    "            raise RuntimeError(\n",
    "                \"In order to use `use_batchnorm='inplace'` inplace_abn package must be installed. \"\n",
    "                + \"To install see: https://github.com/mapillary/inplace_abn\"\n",
    "            )\n",
    "\n",
    "        conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=not (use_batchnorm),\n",
    "        )\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        if use_batchnorm == \"inplace\":\n",
    "            bn = InPlaceABN(out_channels, activation=\"leaky_relu\", activation_param=0.0)\n",
    "            relu = nn.Identity()\n",
    "\n",
    "        elif use_batchnorm and use_batchnorm != \"inplace\":\n",
    "            bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        else:\n",
    "            bn = nn.Identity()\n",
    "\n",
    "        super(Conv2dReLU, self).__init__(conv, bn, relu)\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_batchnorm=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2dReLU(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        self.conv2 = Conv2dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "relu_inplace = True\n",
    "\n",
    "\n",
    "class HRNetV2(nn.Module):\n",
    "    def __init__(self, n_class, **kwargs):\n",
    "        super(HRNetV2, self).__init__()\n",
    "        extra = {\n",
    "            \"STAGE1\": {\n",
    "                \"NUM_MODULES\": 1,\n",
    "                \"NUM_BRANCHES\": 1,\n",
    "                \"BLOCK\": \"BOTTLENECK\",\n",
    "                \"NUM_BLOCKS\": (4),\n",
    "                \"NUM_CHANNELS\": (64),\n",
    "                \"FUSE_METHOD\": \"SUM\",\n",
    "            },\n",
    "            \"STAGE2\": {\n",
    "                \"NUM_MODULES\": 1,\n",
    "                \"NUM_BRANCHES\": 2,\n",
    "                \"BLOCK\": \"BASIC\",\n",
    "                \"NUM_BLOCKS\": (4, 4),\n",
    "                \"NUM_CHANNELS\": (48, 96),\n",
    "                \"FUSE_METHOD\": \"SUM\",\n",
    "            },\n",
    "            \"STAGE3\": {\n",
    "                \"NUM_MODULES\": 4,\n",
    "                \"NUM_BRANCHES\": 3,\n",
    "                \"BLOCK\": \"BASIC\",\n",
    "                \"NUM_BLOCKS\": (4, 4, 4),\n",
    "                \"NUM_CHANNELS\": (48, 96, 192),\n",
    "                \"FUSE_METHOD\": \"SUM\",\n",
    "            },\n",
    "            \"STAGE4\": {\n",
    "                \"NUM_MODULES\": 3,\n",
    "                \"NUM_BRANCHES\": 4,\n",
    "                \"BLOCK\": \"BASIC\",\n",
    "                \"NUM_BLOCKS\": (4, 4, 4, 4),\n",
    "                \"NUM_CHANNELS\": (48, 96, 192, 384),\n",
    "                \"FUSE_METHOD\": \"SUM\",\n",
    "            },\n",
    "            \"FINAL_CONV_KERNEL\": 1,\n",
    "        }\n",
    "        ALIGN_CORNERS = False\n",
    "        relu_inplace = True\n",
    "        # stem net\n",
    "        # stem net\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.stage1_cfg = extra[\"STAGE1\"]\n",
    "        num_channels = self.stage1_cfg[\"NUM_CHANNELS\"]\n",
    "        block = blocks_dict[self.stage1_cfg[\"BLOCK\"]]\n",
    "        num_blocks = self.stage1_cfg[\"NUM_BLOCKS\"]\n",
    "        self.layer1 = self._make_layer(block, 64, num_channels, num_blocks)\n",
    "        stage1_out_channel = block.expansion * num_channels\n",
    "\n",
    "        self.stage2_cfg = extra[\"STAGE2\"]\n",
    "        num_channels = self.stage2_cfg[\"NUM_CHANNELS\"]\n",
    "        block = blocks_dict[self.stage2_cfg[\"BLOCK\"]]\n",
    "        num_channels = [\n",
    "            num_channels[i] * block.expansion for i in range(len(num_channels))\n",
    "        ]\n",
    "        self.transition1 = self._make_transition_layer(\n",
    "            [stage1_out_channel], num_channels\n",
    "        )\n",
    "        self.stage2, pre_stage_channels = self._make_stage(\n",
    "            self.stage2_cfg, num_channels\n",
    "        )\n",
    "\n",
    "        self.stage3_cfg = extra[\"STAGE3\"]\n",
    "        num_channels = self.stage3_cfg[\"NUM_CHANNELS\"]\n",
    "        block = blocks_dict[self.stage3_cfg[\"BLOCK\"]]\n",
    "        num_channels = [\n",
    "            num_channels[i] * block.expansion for i in range(len(num_channels))\n",
    "        ]\n",
    "        self.transition2 = self._make_transition_layer(pre_stage_channels, num_channels)\n",
    "        self.stage3, pre_stage_channels = self._make_stage(\n",
    "            self.stage3_cfg, num_channels\n",
    "        )\n",
    "\n",
    "        self.stage4_cfg = extra[\"STAGE4\"]\n",
    "        num_channels = self.stage4_cfg[\"NUM_CHANNELS\"]\n",
    "        block = blocks_dict[self.stage4_cfg[\"BLOCK\"]]\n",
    "        num_channels = [\n",
    "            num_channels[i] * block.expansion for i in range(len(num_channels))\n",
    "        ]\n",
    "        self.transition3 = self._make_transition_layer(pre_stage_channels, num_channels)\n",
    "        self.stage4, pre_stage_channels = self._make_stage(\n",
    "            self.stage4_cfg, num_channels, multi_scale_output=True\n",
    "        )\n",
    "\n",
    "        last_inp_channels = np.int(np.sum(pre_stage_channels))\n",
    "        self.up1 = DecoderBlock(720, 512)\n",
    "        self.up2 = DecoderBlock(512, n_class)\n",
    "\n",
    "        self.last_layer = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=last_inp_channels,\n",
    "                out_channels=last_inp_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "            ),\n",
    "            BatchNorm2d(last_inp_channels, momentum=BN_MOMENTUM),\n",
    "            nn.ReLU(inplace=relu_inplace),\n",
    "            nn.Conv2d(\n",
    "                in_channels=last_inp_channels,\n",
    "                out_channels=n_class,\n",
    "                kernel_size=extra[\"FINAL_CONV_KERNEL\"],\n",
    "                stride=1,\n",
    "                padding=1 if extra[\"FINAL_CONV_KERNEL\"] == 3 else 0,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def _make_transition_layer(self, num_channels_pre_layer, num_channels_cur_layer):\n",
    "        num_branches_cur = len(num_channels_cur_layer)\n",
    "        num_branches_pre = len(num_channels_pre_layer)\n",
    "\n",
    "        transition_layers = []\n",
    "        for i in range(num_branches_cur):\n",
    "            if i < num_branches_pre:\n",
    "                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n",
    "                    transition_layers.append(\n",
    "                        nn.Sequential(\n",
    "                            nn.Conv2d(\n",
    "                                num_channels_pre_layer[i],\n",
    "                                num_channels_cur_layer[i],\n",
    "                                3,\n",
    "                                1,\n",
    "                                1,\n",
    "                                bias=False,\n",
    "                            ),\n",
    "                            BatchNorm2d(\n",
    "                                num_channels_cur_layer[i], momentum=BN_MOMENTUM\n",
    "                            ),\n",
    "                            nn.ReLU(inplace=relu_inplace),\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    transition_layers.append(None)\n",
    "            else:\n",
    "                conv3x3s = []\n",
    "                for j in range(i + 1 - num_branches_pre):\n",
    "                    inchannels = num_channels_pre_layer[-1]\n",
    "                    outchannels = (\n",
    "                        num_channels_cur_layer[i]\n",
    "                        if j == i - num_branches_pre\n",
    "                        else inchannels\n",
    "                    )\n",
    "                    conv3x3s.append(\n",
    "                        nn.Sequential(\n",
    "                            nn.Conv2d(inchannels, outchannels, 3, 2, 1, bias=False),\n",
    "                            BatchNorm2d(outchannels, momentum=BN_MOMENTUM),\n",
    "                            nn.ReLU(inplace=relu_inplace),\n",
    "                        )\n",
    "                    )\n",
    "                transition_layers.append(nn.Sequential(*conv3x3s))\n",
    "\n",
    "        return nn.ModuleList(transition_layers)\n",
    "\n",
    "    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    inplanes,\n",
    "                    planes * block.expansion,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(inplanes, planes, stride, downsample))\n",
    "        inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_stage(self, layer_config, num_inchannels, multi_scale_output=True):\n",
    "        num_modules = layer_config[\"NUM_MODULES\"]\n",
    "        num_branches = layer_config[\"NUM_BRANCHES\"]\n",
    "        num_blocks = layer_config[\"NUM_BLOCKS\"]\n",
    "        num_channels = layer_config[\"NUM_CHANNELS\"]\n",
    "        block = blocks_dict[layer_config[\"BLOCK\"]]\n",
    "        fuse_method = layer_config[\"FUSE_METHOD\"]\n",
    "\n",
    "        modules = []\n",
    "        for i in range(num_modules):\n",
    "            # multi_scale_output is only used last module\n",
    "            if not multi_scale_output and i == num_modules - 1:\n",
    "                reset_multi_scale_output = False\n",
    "            else:\n",
    "                reset_multi_scale_output = True\n",
    "            modules.append(\n",
    "                HighResolutionModule(\n",
    "                    num_branches,\n",
    "                    block,\n",
    "                    num_blocks,\n",
    "                    num_inchannels,\n",
    "                    num_channels,\n",
    "                    fuse_method,\n",
    "                    reset_multi_scale_output,\n",
    "                )\n",
    "            )\n",
    "            num_inchannels = modules[-1].get_num_inchannels()\n",
    "\n",
    "        return nn.Sequential(*modules), num_inchannels\n",
    "\n",
    "    def forward(self, x, return_feature_maps=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "\n",
    "        x_list = []\n",
    "        for i in range(self.stage2_cfg[\"NUM_BRANCHES\"]):\n",
    "            if self.transition1[i] is not None:\n",
    "                x_list.append(self.transition1[i](x))\n",
    "            else:\n",
    "                x_list.append(x)\n",
    "        y_list = self.stage2(x_list)\n",
    "\n",
    "        x_list = []\n",
    "        for i in range(self.stage3_cfg[\"NUM_BRANCHES\"]):\n",
    "            if self.transition2[i] is not None:\n",
    "                if i < self.stage2_cfg[\"NUM_BRANCHES\"]:\n",
    "                    x_list.append(self.transition2[i](y_list[i]))\n",
    "                else:\n",
    "                    x_list.append(self.transition2[i](y_list[-1]))\n",
    "            else:\n",
    "                x_list.append(y_list[i])\n",
    "        y_list = self.stage3(x_list)\n",
    "\n",
    "        x_list = []\n",
    "        for i in range(self.stage4_cfg[\"NUM_BRANCHES\"]):\n",
    "            if self.transition3[i] is not None:\n",
    "                if i < self.stage3_cfg[\"NUM_BRANCHES\"]:\n",
    "                    x_list.append(self.transition3[i](y_list[i]))\n",
    "                else:\n",
    "                    x_list.append(self.transition3[i](y_list[-1]))\n",
    "            else:\n",
    "                x_list.append(y_list[i])\n",
    "        x = self.stage4(x_list)\n",
    "\n",
    "        # Upsampling\n",
    "        x0_h, x0_w = x[0].size(2), x[0].size(3)\n",
    "        x1 = F.interpolate(\n",
    "            x[1], size=(x0_h, x0_w), mode=\"bilinear\", align_corners=False\n",
    "        )\n",
    "        x2 = F.interpolate(\n",
    "            x[2], size=(x0_h, x0_w), mode=\"bilinear\", align_corners=False\n",
    "        )\n",
    "        x3 = F.interpolate(\n",
    "            x[3], size=(x0_h, x0_w), mode=\"bilinear\", align_corners=False\n",
    "        )\n",
    "\n",
    "        x = torch.cat([x[0], x1, x2, x3], 1)\n",
    "\n",
    "        # x = self.up2(self.up1(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def hrnetv2(pretrained=False, **kwargs):\n",
    "    model = HRNetV2(n_class=2, **kwargs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e93e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "import collections\n",
    "import threading\n",
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# model = hrnetv2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13df8596",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = eval(\n",
    "    f'smp.create_model(arch=\"{\"FullAxialUnet\"}\", encoder_name=\"{\"resnet18\"}\", encoder_weights=\"imagenet\", in_channels={3}, classes = {2}, image_size = {256})'\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48af99b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_in = torch.randn(2, 3, 256, 256).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8703cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(dummy_in).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1539c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77bcacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "li = [\n",
    "    {\n",
    "        \"Id\": 1,\n",
    "        \"ParentId\": 0,\n",
    "        \"ClassificationSubtype\": False,\n",
    "        \"Label\": \"Base ROI\",\n",
    "        \"Color\": \"rgba(0, 0, 0, 0.0)\",\n",
    "        \"Tools\": [\n",
    "            {\n",
    "                \"Name\": \"iam_base_roi\",\n",
    "                \"Parameters\": {\n",
    "                    \"invert_result\": False,\n",
    "                    \"min_intensity\": 28.2,\n",
    "                    \"smoothing\": 23.55,\n",
    "                    \"minsize\": 5,\n",
    "                    \"paramLabels\": {\n",
    "                        \"invert_result\": \"Invert Base ROI\",\n",
    "                        \"min_intensity\": \"Minimum Intensity (%)\",\n",
    "                        \"smoothing\": \"Smoothing\",\n",
    "                        \"minsize\": \"Minimum Size (%)\",\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        \"SelectedToolName\": None,\n",
    "        \"Dynamic\": False,\n",
    "        \"HasChild\": False,\n",
    "        \"IsSubtype\": False,\n",
    "        \"SubtypeLevel\": 0,\n",
    "        \"CriticalClass\": 0,\n",
    "        \"CriticalPercentage\": 0,\n",
    "        \"ClassFrequencies\": {\n",
    "            \"Class_0\": 0,\n",
    "            \"Class_1\": 0,\n",
    "            \"Class_2\": 0,\n",
    "            \"Class_3\": 0,\n",
    "            \"Class_4\": 0,\n",
    "            \"Class_5\": 0,\n",
    "            \"Class_6\": 0,\n",
    "        },\n",
    "        \"AvgClassFrequency\": 0,\n",
    "        \"ToolParams\": [],\n",
    "        \"DefaultSelected\": False,\n",
    "    },\n",
    "    {\n",
    "        \"Id\": 2,\n",
    "        \"ParentId\": 0,\n",
    "        \"ClassificationSubtype\": False,\n",
    "        \"Label\": \"Tumor Cells\",\n",
    "        \"Color\": \"#e6194b\",\n",
    "        \"Tools\": [],\n",
    "        \"SelectedToolName\": None,\n",
    "        \"Dynamic\": True,\n",
    "        \"HasChild\": True,\n",
    "        \"IsSubtype\": False,\n",
    "        \"SubtypeLevel\": 0,\n",
    "        \"CriticalClass\": 0,\n",
    "        \"CriticalPercentage\": 0,\n",
    "        \"ClassFrequencies\": {\n",
    "            \"Class_0\": 0,\n",
    "            \"Class_1\": 0,\n",
    "            \"Class_2\": 0,\n",
    "            \"Class_3\": 0,\n",
    "            \"Class_4\": 0,\n",
    "            \"Class_5\": 0,\n",
    "            \"Class_6\": 0,\n",
    "        },\n",
    "        \"AvgClassFrequency\": 0,\n",
    "        \"ToolParams\": None,\n",
    "        \"DefaultSelected\": False,\n",
    "    },\n",
    "    {\n",
    "        \"Id\": 4,\n",
    "        \"ParentId\": 2,\n",
    "        \"ClassificationSubtype\": True,\n",
    "        \"Label\": \"Positive\",\n",
    "        \"Color\": \"#ffe119\",\n",
    "        \"Tools\": [],\n",
    "        \"SelectedToolName\": None,\n",
    "        \"Dynamic\": True,\n",
    "        \"HasChild\": False,\n",
    "        \"IsSubtype\": True,\n",
    "        \"SubtypeLevel\": 1,\n",
    "        \"CriticalClass\": 0,\n",
    "        \"CriticalPercentage\": 0,\n",
    "        \"ClassFrequencies\": {\n",
    "            \"Class_0\": 0,\n",
    "            \"Class_1\": 0,\n",
    "            \"Class_2\": 0,\n",
    "            \"Class_3\": 0,\n",
    "            \"Class_4\": 0,\n",
    "            \"Class_5\": 0,\n",
    "            \"Class_6\": 0,\n",
    "        },\n",
    "        \"AvgClassFrequency\": 0,\n",
    "        \"ToolParams\": None,\n",
    "        \"DefaultSelected\": False,\n",
    "    },\n",
    "    {\n",
    "        \"Id\": 3,\n",
    "        \"ParentId\": 2,\n",
    "        \"ClassificationSubtype\": True,\n",
    "        \"Label\": \"Negative\",\n",
    "        \"Color\": \"#3cb44b\",\n",
    "        \"Tools\": [],\n",
    "        \"SelectedToolName\": None,\n",
    "        \"Dynamic\": True,\n",
    "        \"HasChild\": False,\n",
    "        \"IsSubtype\": True,\n",
    "        \"SubtypeLevel\": 1,\n",
    "        \"CriticalClass\": 0,\n",
    "        \"CriticalPercentage\": 0,\n",
    "        \"ClassFrequencies\": {\n",
    "            \"Class_0\": 0,\n",
    "            \"Class_1\": 0,\n",
    "            \"Class_2\": 0,\n",
    "            \"Class_3\": 0,\n",
    "            \"Class_4\": 0,\n",
    "            \"Class_5\": 0,\n",
    "            \"Class_6\": 0,\n",
    "        },\n",
    "        \"AvgClassFrequency\": 0,\n",
    "        \"ToolParams\": None,\n",
    "        \"DefaultSelected\": False,\n",
    "    },\n",
    "    {\n",
    "        \"Id\": 5,\n",
    "        \"ParentId\": 0,\n",
    "        \"ClassificationSubtype\": False,\n",
    "        \"Label\": \"Immun Cells\",\n",
    "        \"Color\": \"#4363d8\",\n",
    "        \"Tools\": [],\n",
    "        \"SelectedToolName\": None,\n",
    "        \"Dynamic\": True,\n",
    "        \"HasChild\": True,\n",
    "        \"IsSubtype\": False,\n",
    "        \"SubtypeLevel\": 0,\n",
    "        \"CriticalClass\": 0,\n",
    "        \"CriticalPercentage\": 0,\n",
    "        \"ClassFrequencies\": {\n",
    "            \"Class_0\": 0,\n",
    "            \"Class_1\": 0,\n",
    "            \"Class_2\": 0,\n",
    "            \"Class_3\": 0,\n",
    "            \"Class_4\": 0,\n",
    "            \"Class_5\": 0,\n",
    "            \"Class_6\": 0,\n",
    "        },\n",
    "        \"AvgClassFrequency\": 0,\n",
    "        \"ToolParams\": None,\n",
    "        \"DefaultSelected\": False,\n",
    "    },\n",
    "    {\n",
    "        \"Id\": 7,\n",
    "        \"ParentId\": 5,\n",
    "        \"ClassificationSubtype\": True,\n",
    "        \"Label\": \"Lymphozyten\",\n",
    "        \"Color\": \"#911eb4\",\n",
    "        \"Tools\": [],\n",
    "        \"SelectedToolName\": None,\n",
    "        \"Dynamic\": True,\n",
    "        \"HasChild\": True,\n",
    "        \"IsSubtype\": True,\n",
    "        \"SubtypeLevel\": 1,\n",
    "        \"CriticalClass\": 0,\n",
    "        \"CriticalPercentage\": 0,\n",
    "        \"ClassFrequencies\": {\n",
    "            \"Class_0\": 0,\n",
    "            \"Class_1\": 0,\n",
    "            \"Class_2\": 0,\n",
    "            \"Class_3\": 0,\n",
    "            \"Class_4\": 0,\n",
    "            \"Class_5\": 0,\n",
    "            \"Class_6\": 0,\n",
    "        },\n",
    "        \"AvgClassFrequency\": 0,\n",
    "        \"ToolParams\": None,\n",
    "        \"DefaultSelected\": False,\n",
    "    },\n",
    "    {\n",
    "        \"Id\": 11,\n",
    "        \"ParentId\": 7,\n",
    "        \"ClassificationSubtype\": True,\n",
    "        \"Label\": \"Lymphozyten +\",\n",
    "        \"Color\": \"#fabebe\",\n",
    "        \"Tools\": [],\n",
    "        \"SelectedToolName\": None,\n",
    "        \"Dynamic\": True,\n",
    "        \"HasChild\": False,\n",
    "        \"IsSubtype\": True,\n",
    "        \"SubtypeLevel\": 2,\n",
    "        \"CriticalClass\": 0,\n",
    "        \"CriticalPercentage\": 0,\n",
    "        \"ClassFrequencies\": {\n",
    "            \"Class_0\": 0,\n",
    "            \"Class_1\": 0,\n",
    "            \"Class_2\": 0,\n",
    "            \"Class_3\": 0,\n",
    "            \"Class_4\": 0,\n",
    "            \"Class_5\": 0,\n",
    "            \"Class_6\": 0,\n",
    "        },\n",
    "        \"AvgClassFrequency\": 0,\n",
    "        \"ToolParams\": None,\n",
    "        \"DefaultSelected\": False,\n",
    "    },\n",
    "    {\n",
    "        \"Id\": 12,\n",
    "        \"ParentId\": 7,\n",
    "        \"ClassificationSubtype\": True,\n",
    "        \"Label\": \"Lymphozyten -\",\n",
    "        \"Color\": \"#008080\",\n",
    "        \"Tools\": [],\n",
    "        \"SelectedToolName\": None,\n",
    "        \"Dynamic\": True,\n",
    "        \"HasChild\": False,\n",
    "        \"IsSubtype\": True,\n",
    "        \"SubtypeLevel\": 2,\n",
    "        \"CriticalClass\": 0,\n",
    "        \"CriticalPercentage\": 0,\n",
    "        \"ClassFrequencies\": {\n",
    "            \"Class_0\": 0,\n",
    "            \"Class_1\": 0,\n",
    "            \"Class_2\": 0,\n",
    "            \"Class_3\": 0,\n",
    "            \"Class_4\": 0,\n",
    "            \"Class_5\": 0,\n",
    "            \"Class_6\": 0,\n",
    "        },\n",
    "        \"AvgClassFrequency\": 0,\n",
    "        \"ToolParams\": None,\n",
    "        \"DefaultSelected\": False,\n",
    "    },\n",
    "    {\n",
    "        \"Id\": 6,\n",
    "        \"ParentId\": 5,\n",
    "        \"ClassificationSubtype\": True,\n",
    "        \"Label\": \"Granulozyten\",\n",
    "        \"Color\": \"#f58231\",\n",
    "        \"Tools\": [],\n",
    "        \"SelectedToolName\": None,\n",
    "        \"Dynamic\": True,\n",
    "        \"HasChild\": True,\n",
    "        \"IsSubtype\": True,\n",
    "        \"SubtypeLevel\": 1,\n",
    "        \"CriticalClass\": 0,\n",
    "        \"CriticalPercentage\": 0,\n",
    "        \"ClassFrequencies\": {\n",
    "            \"Class_0\": 0,\n",
    "            \"Class_1\": 0,\n",
    "            \"Class_2\": 0,\n",
    "            \"Class_3\": 0,\n",
    "            \"Class_4\": 0,\n",
    "            \"Class_5\": 0,\n",
    "            \"Class_6\": 0,\n",
    "        },\n",
    "        \"AvgClassFrequency\": 0,\n",
    "        \"ToolParams\": None,\n",
    "        \"DefaultSelected\": False,\n",
    "    },\n",
    "    {\n",
    "        \"Id\": 14,\n",
    "        \"ParentId\": 6,\n",
    "        \"ClassificationSubtype\": True,\n",
    "        \"Label\": \"Granulozyten +\",\n",
    "        \"Color\": \"#9a6324\",\n",
    "        \"Tools\": [],\n",
    "        \"SelectedToolName\": None,\n",
    "        \"Dynamic\": True,\n",
    "        \"HasChild\": False,\n",
    "        \"IsSubtype\": True,\n",
    "        \"SubtypeLevel\": 2,\n",
    "        \"CriticalClass\": 0,\n",
    "        \"CriticalPercentage\": 0,\n",
    "        \"ClassFrequencies\": {\n",
    "            \"Class_0\": 0,\n",
    "            \"Class_1\": 0,\n",
    "            \"Class_2\": 0,\n",
    "            \"Class_3\": 0,\n",
    "            \"Class_4\": 0,\n",
    "            \"Class_5\": 0,\n",
    "            \"Class_6\": 0,\n",
    "        },\n",
    "        \"AvgClassFrequency\": 0,\n",
    "        \"ToolParams\": None,\n",
    "        \"DefaultSelected\": False,\n",
    "    },\n",
    "    {\n",
    "        \"Id\": 13,\n",
    "        \"ParentId\": 6,\n",
    "        \"ClassificationSubtype\": True,\n",
    "        \"Label\": \"Granulozyten -\",\n",
    "        \"Color\": \"#e6beff\",\n",
    "        \"Tools\": [],\n",
    "        \"SelectedToolName\": None,\n",
    "        \"Dynamic\": True,\n",
    "        \"HasChild\": False,\n",
    "        \"IsSubtype\": True,\n",
    "        \"SubtypeLevel\": 2,\n",
    "        \"CriticalClass\": 0,\n",
    "        \"CriticalPercentage\": 0,\n",
    "        \"ClassFrequencies\": {\n",
    "            \"Class_0\": 0,\n",
    "            \"Class_1\": 0,\n",
    "            \"Class_2\": 0,\n",
    "            \"Class_3\": 0,\n",
    "            \"Class_4\": 0,\n",
    "            \"Class_5\": 0,\n",
    "            \"Class_6\": 0,\n",
    "        },\n",
    "        \"AvgClassFrequency\": 0,\n",
    "        \"ToolParams\": None,\n",
    "        \"DefaultSelected\": False,\n",
    "    },\n",
    "    {\n",
    "        \"Id\": 10,\n",
    "        \"ParentId\": 5,\n",
    "        \"ClassificationSubtype\": True,\n",
    "        \"Label\": \"Dendritische Zellen\",\n",
    "        \"Color\": \"#bcf60c\",\n",
    "        \"Tools\": [],\n",
    "        \"SelectedToolName\": None,\n",
    "        \"Dynamic\": True,\n",
    "        \"HasChild\": False,\n",
    "        \"IsSubtype\": True,\n",
    "        \"SubtypeLevel\": 1,\n",
    "        \"CriticalClass\": 0,\n",
    "        \"CriticalPercentage\": 0,\n",
    "        \"ClassFrequencies\": {\n",
    "            \"Class_0\": 0,\n",
    "            \"Class_1\": 0,\n",
    "            \"Class_2\": 0,\n",
    "            \"Class_3\": 0,\n",
    "            \"Class_4\": 0,\n",
    "            \"Class_5\": 0,\n",
    "            \"Class_6\": 0,\n",
    "        },\n",
    "        \"AvgClassFrequency\": 0,\n",
    "        \"ToolParams\": None,\n",
    "        \"DefaultSelected\": False,\n",
    "    },\n",
    "    {\n",
    "        \"Id\": 9,\n",
    "        \"ParentId\": 5,\n",
    "        \"ClassificationSubtype\": True,\n",
    "        \"Label\": \"Makrophagen\",\n",
    "        \"Color\": \"#f032e6\",\n",
    "        \"Tools\": [],\n",
    "        \"SelectedToolName\": None,\n",
    "        \"Dynamic\": True,\n",
    "        \"HasChild\": False,\n",
    "        \"IsSubtype\": True,\n",
    "        \"SubtypeLevel\": 1,\n",
    "        \"CriticalClass\": 0,\n",
    "        \"CriticalPercentage\": 0,\n",
    "        \"ClassFrequencies\": {\n",
    "            \"Class_0\": 0,\n",
    "            \"Class_1\": 0,\n",
    "            \"Class_2\": 0,\n",
    "            \"Class_3\": 0,\n",
    "            \"Class_4\": 0,\n",
    "            \"Class_5\": 0,\n",
    "            \"Class_6\": 0,\n",
    "        },\n",
    "        \"AvgClassFrequency\": 0,\n",
    "        \"ToolParams\": None,\n",
    "        \"DefaultSelected\": False,\n",
    "    },\n",
    "    {\n",
    "        \"Id\": 8,\n",
    "        \"ParentId\": 5,\n",
    "        \"ClassificationSubtype\": True,\n",
    "        \"Label\": \"Plasmazellen\",\n",
    "        \"Color\": \"#46f0f0\",\n",
    "        \"Tools\": [],\n",
    "        \"SelectedToolName\": None,\n",
    "        \"Dynamic\": True,\n",
    "        \"HasChild\": False,\n",
    "        \"IsSubtype\": True,\n",
    "        \"SubtypeLevel\": 1,\n",
    "        \"CriticalClass\": 0,\n",
    "        \"CriticalPercentage\": 0,\n",
    "        \"ClassFrequencies\": {\n",
    "            \"Class_0\": 0,\n",
    "            \"Class_1\": 0,\n",
    "            \"Class_2\": 0,\n",
    "            \"Class_3\": 0,\n",
    "            \"Class_4\": 0,\n",
    "            \"Class_5\": 0,\n",
    "            \"Class_6\": 0,\n",
    "        },\n",
    "        \"AvgClassFrequency\": 0,\n",
    "        \"ToolParams\": None,\n",
    "        \"DefaultSelected\": False,\n",
    "    },\n",
    "    {\n",
    "        \"Id\": 15,\n",
    "        \"ParentId\": 0,\n",
    "        \"ClassificationSubtype\": False,\n",
    "        \"Label\": \"Grid\",\n",
    "        \"Color\": \"#00000000\",\n",
    "        \"Tools\": [],\n",
    "        \"SelectedToolName\": None,\n",
    "        \"Dynamic\": True,\n",
    "        \"HasChild\": False,\n",
    "        \"IsSubtype\": False,\n",
    "        \"SubtypeLevel\": 0,\n",
    "        \"CriticalClass\": 0,\n",
    "        \"CriticalPercentage\": 0,\n",
    "        \"ClassFrequencies\": {\n",
    "            \"Class_0\": 0,\n",
    "            \"Class_1\": 0,\n",
    "            \"Class_2\": 0,\n",
    "            \"Class_3\": 0,\n",
    "            \"Class_4\": 0,\n",
    "            \"Class_5\": 0,\n",
    "            \"Class_6\": 0,\n",
    "        },\n",
    "        \"AvgClassFrequency\": 0,\n",
    "        \"ToolParams\": None,\n",
    "        \"DefaultSelected\": False,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373d0a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in list(filter(lambda x: x['HasChild'], li)):\n",
    "    if layer['ParentId'] == idd:\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e1a149",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt, layer in enumerate(\n",
    "    list(filter(lambda x: not x[\"HasChild\"] and x[\"ParentId\"] == selectedId, li))\n",
    "):\n",
    "    print(layer[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b0429",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedId"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7395f6d",
   "metadata": {},
   "source": [
    "# Paul Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf2f0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "fi = cv2.imread(r\"E:\\source\\repos\\VisualDL\\custom_experiments\\2.png\")\n",
    "se = cv2.imread(r\"E:\\source\\repos\\VisualDL\\custom_experiments\\3.png\")\n",
    "res = cv2.absdiff(fi, se)\n",
    "res = res.astype(np.uint8)\n",
    "# --- find percentage difference based on number of pixels that are not zero ---\n",
    "percentage = (np.count_nonzero(res) * 100) / res.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343a8acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01da989",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = cv2.subtract(\n",
    "    cv2.imread(r\"E:\\source\\repos\\VisualDL\\custom_experiments\\2.png\"),\n",
    "    cv2.imread(r\"E:\\source\\repos\\VisualDL\\custom_experiments\\3.png\"),\n",
    ")\n",
    "difference = cv2.cvtColor(difference, cv2.COLOR_BGR2GRAY)\n",
    "difference[difference > 0] = 255\n",
    "print(1 - (np.count_nonzero(difference) / (1920 * 1080)))\n",
    "\n",
    "cv2.imwrite(\"Xd.png\", difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a3aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(im1, im2):\n",
    "    res = cv2.absdiff(im1, im2)\n",
    "    res = res.astype(np.uint8)\n",
    "    return (np.count_nonzero(res) * 100) / res.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbc1df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video = cv2.VideoCapture(r\"C:\\Users\\phili\\Downloads\\ALM scam.mp4\")\n",
    "frames_per_second = int(input_video.get(cv2.CAP_PROP_FPS))\n",
    "frame_width = int(input_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(input_video.get(cv2.CAP_PROP_FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6ecd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcd687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "last_image = None\n",
    "threshold = 0.9991\n",
    "# Loop through all the frames in the video\n",
    "\n",
    "while 1:\n",
    "    # Read the video to retrieve individual frames. 'frame' will reference the inidivdual frames read from the video.\n",
    "    ret, frame = input_video.read()\n",
    "    if not ret:\n",
    "        print(\"Processed all frames\")\n",
    "        break\n",
    "    if last_image is None:\n",
    "        last_image = frame\n",
    "        continue\n",
    "\n",
    "    di = diff(frame, last_image)\n",
    "    last_image = frame\n",
    "    if di > threshold:\n",
    "        count += 1\n",
    "        cv2.imwrite(f\"{count}.png\", frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab14dc32",
   "metadata": {},
   "source": [
    "# Convert MakeSenseAi Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c798da6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "image_dir = r\"C:\\Users\\phili\\Videos\\Call of Duty  Modern Warfare 2019\\trainnig\"\n",
    "csv_file = r\"C:\\Users\\phili\\Videos\\Call of Duty  Modern Warfare 2019\\labels_my-project-name_2021-07-31-07-17-36.csv\"\n",
    "label_dir = \"labels\"\n",
    "with open(csv_file, \"r\", encoding=\"utf-8\") as handle:\n",
    "    lines = handle.readlines()\n",
    "files = {}\n",
    "for line in lines:\n",
    "    x_min, y_min, width, height = [int(x) for x in line.split(\",\")[1:5]]\n",
    "    image_width = int(line.split(\",\")[-2])\n",
    "    image_height = int(line.split(\",\")[-1])\n",
    "    name = line.split(\",\")[-3]\n",
    "\n",
    "    middle_x = (x_min + width / 2) / image_width\n",
    "    middle_y = (y_min + height / 2) / image_height\n",
    "    width /= image_width\n",
    "    height /= image_height\n",
    "    if not name in files:\n",
    "        files[name] = [f\"0 {middle_x} {middle_y} {width} {height}\\n\"]\n",
    "    else:\n",
    "        files[name].append(f\"0 {middle_x} {middle_y} {width} {height}\\n\")\n",
    "all_images = os.listdir(image_dir)\n",
    "no_annotations = list(set(all_images) - set(files.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bdc6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in files.items():\n",
    "    with open(os.path.join(label_dir, key).replace(\".png\", \".txt\"), \"w\") as handle:\n",
    "        for it in val:\n",
    "            handle.write(it)\n",
    "for file in no_annotations:\n",
    "    with open(os.path.join(label_dir, file).replace(\".png\", \".txt\"), \"w\") as handle:\n",
    "        handle.write(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103986cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = r\"F:\\source\\repos\\Daten\\HER-N\\Pdl1Combined\\Tumor Cells 512\\train\\labels\"\n",
    "co = set()\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "for file in os.listdir(start):\n",
    "    im = cv2.imread(os.path.join(start, file), 0)\n",
    "    un = np.unique(im)\n",
    "    for u in un:\n",
    "        co.add(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f080b253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "np.count_nonzero(\n",
    "    cv2.imread(\n",
    "        r\"F:\\source\\repos\\VisualDL\\custom_experiments\\bayer\\valid\\images\\1_ 1_07__1s_0r_412x_824y_.png\",\n",
    "        0,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a54a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.load(r\"F:\\source\\repos\\VisualDL\\tu-resnest50d, Unet.pt\")  # load your model\n",
    "# t is a simple python dictionary\n",
    "t[\"custom_data\"] = {\n",
    "    \"structure_indices\": [4],\n",
    "    \"image_size\": 128,\n",
    "    \"modeltype\": \"segmentation\",\n",
    "    \"object_based\": False,\n",
    "    \"physical_tile_size\": (226.388852600035, 226.388852600035),\n",
    "    \"project_type\": \"dummy\",\n",
    "    \"pyramid_level\": 0,\n",
    "    \"datetime\": \"21/01/2022 15:33\",\n",
    "    \"structures\": \"Glomerulus\",\n",
    "    \"objects_count\": 312,\n",
    "    \"model\": \"[{'backbone': 'tu-resnest26d', 'decoder': 'Unet'}]\",\n",
    "    \"files\": {\n",
    "        \"File\": [\n",
    "            \"05_.czi\",\n",
    "            \"06_.czi\",\n",
    "            \"07_.czi\",\n",
    "            \"12_.czi\",\n",
    "            \"13_.czi\",\n",
    "            \"14_.czi\",\n",
    "            \"17_.czi\",\n",
    "            \"18_.czi\",\n",
    "            \"19_.czi\",\n",
    "            \"05_.czi\",\n",
    "            \"06_.czi\",\n",
    "            \"07_.czi\",\n",
    "            \"12_.czi\",\n",
    "            \"13_.czi\",\n",
    "            \"14_.czi\",\n",
    "            \"17_.czi\",\n",
    "            \"18_.czi\",\n",
    "            \"19_.czi\",\n",
    "        ],\n",
    "        \"Scene\": [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
    "    },\n",
    "    \"calculate_weight_map\": False,\n",
    "}  # Change the custom_data dictionary to whatever you want and add keys if you want\n",
    "torch.save(t, \"changed_keys_model.pt\")  # save the new model in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26736ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56eb57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.load(r\"../maskrcnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00556bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "t[\"custom_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32024869",
   "metadata": {},
   "outputs": [],
   "source": [
    "t[\"custom_data\"] = {\n",
    "    \"structure_indices\": [15],\n",
    "    \"image_size\": 1024,\n",
    "    \"object_based\": False,\n",
    "    \"new_key\": \"irgendwas\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db69afd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t[\"custom_data\"][\"structure_indices\"] = [4, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b6a0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(t, \"changed_keys_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a37ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "t[\"custom_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f57412",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.load(r\"001.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baa5282",
   "metadata": {},
   "outputs": [],
   "source": [
    "t[\"custom_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d953a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7421ceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Test\\Cells128\\train\\labels\"\n",
    "for im in os.listdir(start):\n",
    "    img = io.imread(os.path.join(start, im), as_gray=True).astype(np.float32)\n",
    "    print(np.unique(img))\n",
    "\n",
    "    cv2.imwrite(os.path.join(start, im), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6a5f63",
   "metadata": {},
   "source": [
    "# Single class per contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a497d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45b8e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06bff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\n",
    "    r\"F:\\source\\repos\\Daten\\Nuclei\\Nuclei\\Nuclei\\val\\images\\06_PAS_1_8911_5853.png\", 0\n",
    ")\n",
    "orig = img.copy()\n",
    "img[img > 0] = 255\n",
    "\n",
    "contours, hierarchy = cv2.findContours(\n",
    "    image=img, mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE\n",
    ")\n",
    "for i, cnt in enumerate(contours):\n",
    "    area = cv2.contourArea(cnt)\n",
    "    # if area > threshold_area:\n",
    "\n",
    "    mask = np.zeros_like(orig)\n",
    "    cv2.drawContours(mask, [cnt], -1, 255, -1)\n",
    "    cv2.imwrite(f\"dsa{i}dsa.png\", mask)\n",
    "\n",
    "    pts = np.where(mask == 255)\n",
    "    vals = np.unique(orig[pts[0], pts[1]], return_counts=True)\n",
    "\n",
    "    orig[pts[0], pts[1]] = vals[0][np.argmax(vals[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d862714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"orig.png\", orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd6a94f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b90313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualdl import vdl\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.cvtColor(\n",
    "    cv2.imread(\n",
    "        r\"F:\\source\\repos\\Daten\\Nuclei\\Nuclei\\Nuclei\\val\\images\\06_PAS_1_8911_5853.png\"\n",
    "    ),\n",
    "    cv2.COLOR_BGR2RGB,\n",
    ")\n",
    "img = cv2.resize(img, (512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2098ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = vdl.get_inference_model(r\"C:\\Users\\phili\\Documents\\002.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f80d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pred.predict([img], True, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e4ccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"preds1.png\", preds[0] * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dd804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddaf80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6b02d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25350e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337e61db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load(\n",
    "    \"ultralytics/yolov5\",\n",
    "    \"custom\",\n",
    "    path=r\"F:\\source\\repos\\hsayolo\\runs\\train\\exp5\\weights\\best.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b0c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from visualdl import vdl\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "imgs = [\n",
    "    cv2.imread(\n",
    "        r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\test_2\\test_2\\2016_02_24__0067_0s_0r_992x_6944y_.png\"\n",
    "    )[..., ::-1]\n",
    "]\n",
    "model = vdl.get_inference_model(\n",
    "    r\"F:\\source\\repos\\VisualDL\\runss\\exp5\\weights\\best.pt\", type=\"od\"\n",
    ")\n",
    "confi = 0.01\n",
    "# print(model.predict(imgs, confidence = 0.1))\n",
    "orig = imgs[0]\n",
    "out = model.predict(imgs, confidence=confi)\n",
    "# print(out)\n",
    "for i in range(len(out)):\n",
    "    for box in out[i]:\n",
    "        print(box)\n",
    "        orig = imgs[i]\n",
    "        topleft = (int(box[0]), int(box[1]))\n",
    "        bottomright = (int(box[2]), int(box[3]))\n",
    "        cv2.rectangle(orig, topleft, bottomright, (255, 255, 255), 2)\n",
    "cv2.imwrite(\"xd.png\", orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8e5065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe245c08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d600392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(weightpath, imgs, confidence=0.45):\n",
    "    model = torch.hub.load(\n",
    "        \"ultralytics/yolov5\", \"custom\", path=weightpath, force_reload=True\n",
    "    )\n",
    "    size = imgs[0].shape[0]\n",
    "    model.conf = confidence\n",
    "    preds = model(imgs, size=size)\n",
    "    finals = []\n",
    "    for cnt, img in enumerate(imgs):\n",
    "        tmp = []\n",
    "        boxes = preds.xyxy[cnt]\n",
    "        for box in boxes:\n",
    "            middlex = int(box[0] + (box[2] - box[0]) / 2)\n",
    "            middley = int(box[1] + (box[3] - box[1]) / 2)\n",
    "            data = list(box.detach().cpu().numpy())\n",
    "            data.append((middlex, middley))\n",
    "            tmp.append(tuple(data))\n",
    "        finals.append(tmp)\n",
    "    return finals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83841932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "imgs = [\n",
    "    cv2.imread(\n",
    "        r\"F:\\source\\repos\\Daten\\ObjectDetection\\Her1\\train\\images\\PD-L1=2_0_41328-42312_75.png\"\n",
    "    )[..., ::-1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edaa91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualdl import vdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1031513",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = vdl.get_inference_model(\n",
    "    r\"F:\\source\\repos\\hsayolo\\runs\\train\\exp5\\weights\\001.pt\", type=\"od\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5569eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vdl.get_inference_model(\n",
    "    r\"F:\\source\\repos\\VisualDL\\tu-resnet18, UnetPlusPlus.pt\", type=\"segmentation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34538d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.predict(imgs, confidence=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8caceb5",
   "metadata": {},
   "source": [
    "# Create valid folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec111f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from shutil import copyfile\n",
    "from random import shuffle\n",
    "\n",
    "# copyfile(src, dst)\n",
    "folder = r\"F:\\source\\repos\\Daten\\knochenpartikel\\knochenpartikel\"\n",
    "to = r\"F:\\source\\repos\\Daten\\knochenpartikel\\knochenpartikel\\valid\"\n",
    "all_files = os.listdir(os.path.join(folder, \"images\"))\n",
    "shuffle(all_files)\n",
    "le = int(len(all_files) * 0.1)\n",
    "all_images = all_images[:le]\n",
    "for file in all_images:\n",
    "    image = os.path.join(folder, \"images\")\n",
    "    label = os.path.join(folder, \"labels\")\n",
    "    image = os.path.join(image, file)\n",
    "    label = os.path.join(label, file)\n",
    "    # print(os.path.join(os.path.join(to, \"images\"), image))\n",
    "    # print(os.path.join(os.path.join(to, \"images\"), file))\n",
    "    copyfile(image, os.path.join(os.path.join(to, \"images\"), file))\n",
    "    copyfile(label, os.path.join(os.path.join(to, \"labels\"), file))\n",
    "    os.remove(image)\n",
    "    os.remove(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b73211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5705c249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad687010",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load(r\"F:\\source\\repos\\VisualDL\\tu-resnest50d, UnetPlusPlus.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ec7eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"validation_metrics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8610c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dog:\n",
    "    def __init__(self):\n",
    "        s = \"s\"\n",
    "        self.a = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1557a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Dog()\n",
    "for i in range(5):\n",
    "    a.a.append(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5221a357",
   "metadata": {},
   "outputs": [],
   "source": [
    "del a.a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afa6881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2799df",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np.delete(np.array(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dbbbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21afa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = cv2.imread(r\"F:\\source\\repos\\VisualDL\\0a.png\", 0)\n",
    "a = cv2.distanceTransform(a, cv2.DIST_L2, 5)\n",
    "a = cv2.normalize(a, a, 0, 1.0, cv2.NORM_MINMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161200cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd.png\", a * 255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bb00ba",
   "metadata": {},
   "source": [
    "# Create distance map for each object 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459b329",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = (\n",
    "    cv2.imread(\n",
    "        r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells her2 128\\train\\labels\\05__1_3120-9167_15.png\",\n",
    "        0,\n",
    "    )\n",
    "    * 255.0\n",
    ")\n",
    "to = np.zeros_like(img, dtype=np.float32)\n",
    "img = img.astype(np.uint8)\n",
    "orig = img.copy()\n",
    "img[img > 0] = 255\n",
    "contours, hierarchy = cv2.findContours(\n",
    "    image=img, mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE\n",
    ")\n",
    "for i, cnt in enumerate(contours):\n",
    "    area = cv2.contourArea(cnt)\n",
    "    mask = np.zeros_like(orig)\n",
    "    cv2.drawContours(mask, [cnt], -1, 255, -1)\n",
    "    dist = cv2.distanceTransform(mask, cv2.DIST_L2, 5)\n",
    "    ab = cv2.normalize(dist, dist, 0, 1.0, cv2.NORM_MINMAX)\n",
    "    ab[ab < 0.7] = 0\n",
    "    pts = np.where(ab > 0)\n",
    "    to[pts[0], pts[1]] = ab[pts[0], pts[1]]\n",
    "to = to * 255.0\n",
    "cv2.imwrite(\"xd.png\", to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3030dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4390240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file = r\"../visualdl/dependencies/yolov5/data/coco128.yaml\"\n",
    "with open(yaml_file, \"r\", encoding=\"utf-8\") as handle:\n",
    "    a = yaml.load(handle, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cf1d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"train\"] = r\"F:\\source\\repos\\Daten\\ObjectDetection\\tumor\\train\"\n",
    "a[\"val\"] = r\"F:\\source\\repos\\Daten\\ObjectDetection\\tumor\\valid\"\n",
    "a[\"names\"] = [\"xd\", \"bc\", \"bd\", \"ab\"]\n",
    "a[\"nc\"] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62754097",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"../visualdl/dependencies/yolov5/data/hsa.yaml\", \"w\") as yaml_file:\n",
    "    yaml.dump(a, yaml_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93be3a12",
   "metadata": {},
   "source": [
    "# Test parse argmeunts from code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843c4635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"img_size\", type=int, default=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f770cf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430dc33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abc(opt):\n",
    "    print(opt.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82767518",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc(parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63660a47",
   "metadata": {},
   "source": [
    "# Bring mask into yolo format on the fly\n",
    " The starting point for this are the labels that are used to extract the contours and finally the bounding boxes of those.\n",
    " After that they only have to be put in the right format for yolo and saved in an appropiate folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8fd0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe40cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "train_folder = (\n",
    "    r\"D:\\Hsa\\temp\\projects\\f8621bee-f37f-4f15-92f3-3c2891198e7a\\dataset\\train\"\n",
    ")\n",
    "valid_folder = (\n",
    "    r\"D:\\Hsa\\temp\\projects\\f8621bee-f37f-4f15-92f3-3c2891198e7a\\dataset\\train\"\n",
    ")\n",
    "test_folder = r\"D:\\Hsa\\temp\\projects\\f8621bee-f37f-4f15-92f3-3c2891198e7a\\dataset\\valid\"\n",
    "\n",
    "\n",
    "def create_files(folder, nc=1, single_class=False):\n",
    "    start = os.path.join(folder, \"labels\")\n",
    "    all_files = os.listdir(start)\n",
    "    if single_class:\n",
    "        nc = 1\n",
    "    for img in os.listdir(start):\n",
    "        for i in range(0, nc):\n",
    "            im = cv2.imread(os.path.join(start, img), 0)\n",
    "            # kernel = np.ones((2, 2), np.uint8)\n",
    "            # im = cv.erode(im, kernel)\n",
    "            # im = cv.dilate(im, kernel)\n",
    "            tmp = im.copy()\n",
    "            if not single_class:\n",
    "                tmp[tmp != (i + 1)] = 0\n",
    "                tmp[tmp == (i + 1)] = 255\n",
    "            else:\n",
    "                tmp[tmp > 0] = 255\n",
    "            contours, hierachy = cv2.findContours(\n",
    "                tmp, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE\n",
    "            )\n",
    "            blank = np.zeros_like(tmp)\n",
    "            for cnt, cont in enumerate(contours):\n",
    "                xmin, ymin, width, height = cv2.boundingRect(cont)\n",
    "                # cv.rectangle(im,(x,y),(x+width,y+height),(255),1)\n",
    "                # cv2.imwrite(\"xd.png\", im)\n",
    "                image_width = im.shape[0]\n",
    "                xcenter, ycenter = xmin + width / 2, ymin + height / 2\n",
    "                xcenter, ycenter, width, height = (\n",
    "                    xcenter / image_width,\n",
    "                    ycenter / image_width,\n",
    "                    width / image_width,\n",
    "                    height / image_width,\n",
    "                )\n",
    "                if not img in files:\n",
    "                    files[img] = [\n",
    "                        (str(i), str(xcenter), str(ycenter), str(width), str(height))\n",
    "                    ]\n",
    "                else:\n",
    "                    files[img] += [\n",
    "                        (str(i), str(xcenter), str(ycenter), str(width), str(height))\n",
    "                    ]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9df1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_files(train_folder, 4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06740a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"labels\")\n",
    "for cnt, (key, val) in enumerate(files.items()):\n",
    "    with open(\"labels/\" + key.replace(\".png\", \".txt\"), \"w\") as handle:\n",
    "        for va in val:\n",
    "            handle.write(\" \".join(list(va)) + \"\\n\")\n",
    "\n",
    "for name in no_anno:\n",
    "    with open(\"labels/\" + name.replace(\".png\", \".txt\"), \"w\") as handle:\n",
    "        handle.write(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6994bb9d",
   "metadata": {},
   "source": [
    "# Test ob inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ba4f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualdl import vdl\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage import morphology\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.segmentation import watershed\n",
    "from torch import nn\n",
    "import torch\n",
    "import os\n",
    "from skimage.feature import peak_local_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a403afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inf = vdl.get_inference_model(r\"F:\\source\\repos\\VisualDL\\runs\\exp110\\weights\\best.pt\",type = \"od\")\n",
    "# inf = vdl.get_inference_model(r\"F:\\source\\repos\\VisualDL\\maskrcnn.pt\",type = \"instance\")\n",
    "inf2 = vdl.get_inference_model(r\"F:\\source\\repos\\VisualDL\\tu-resnest50d, Unet.pt\")\n",
    "# inf3 = vdl.get_inference_model(r\"C:\\Users\\phili\\Documents\\001.pt\",type = \"segmentation_od\", watershed_od = r\"F:\\source\\repos\\VisualDL\\runs\\exp14\\weights\\001.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce71b1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89dc924",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf2.state[\"validation_metrics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44cf4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea648cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(pretrained=True)\n",
    "target_layers = [model.layer4[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ce1342",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc5cab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf2.model.encoder.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0020d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualdl.utils.model_utils import visualize\n",
    "import torch\n",
    "import cv2\n",
    "from visualdl import vdl\n",
    "from pytorch_grad_cam import GradCAM, AblationCAM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd7289",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = torch.load(r\"F:\\source\\repos\\VisualDL\\maskrcnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666a302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b9d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf2 = vdl.get_inference_model(r\"F:\\source\\repos\\VisualDL\\PLASEGMENTATION.pt\")\n",
    "# inf3 = vdl.get_inference_model(r\"F:\\source\\repos\\VisualDL\\maskrcnn.pt\", type = \"instance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5820ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = cv2.imread(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f21427",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa287cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = r\"F:\\source\\repos\\Bachelorarbeit\\valid\\images\\amos_015964.png\"\n",
    "\n",
    "rgb = cv2.imread(src)[:, :, ::-1]\n",
    "gt = cv2.imread(src.replace(\"images\", \"labels\"))[:, :, ::-1]\n",
    "rgb = cv2.resize(rgb, (768, 768))\n",
    "gt = cv2.resize(gt, (768, 768))\n",
    "xd = torch.tensor(rgb, dtype=torch.float).permute(2, 0, 1)\n",
    "# vis = visualize(inf2.model, inf2.model.encoder.model['layer4'][-2], xd)\n",
    "maps = inf2.predict([rgb])[0][0]\n",
    "cv2.imwrite(\"xd.png\", maps * 15.0)\n",
    "cv2.imwrite(\"xdlabel.png\", gt * 15.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816effad",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb319f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = (\n",
    "    cv2.imread(\n",
    "        r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset (12)\\dataset\\valid\\images\\08__1s_0r_1416x_2124y_.png\",\n",
    "        0,\n",
    "    )\n",
    "    * 255.0\n",
    ")\n",
    "# 08__1s_0r_1416x_2124y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103b7775",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd.png\", rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1d30c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd.png\", maps * 255.0)\n",
    "cv2.imwrite(\n",
    "    \"xd2.png\",\n",
    "    cv2.resize(\n",
    "        cv2.imread(\n",
    "            r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\PLATrain\\valid\\images\\C_M_2020_OT-022_C3b-CFB_0_17954_22949.png\"\n",
    "        ),\n",
    "        (1024, 1024),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31ee84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cam = GradCAM(model=inf2.model, target_layer=inf2.model.encoder.model['layer4'][-1], use_cuda=False)\n",
    "# im = cam(input_tensor=xd.unsqueeze(0), target_category = 0)\n",
    "cam = GradCAM(\n",
    "    model=inf2.model, target_layer=inf2.model.decoder.blocks.x_0_4, use_cuda=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad9983",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = cv2.applyColorMap(\n",
    "    np.uint8(255 * cam(input_tensor=xd.unsqueeze(0), target_category=None)[0, :]),\n",
    "    cv2.COLORMAP_JET,\n",
    ")\n",
    "# heatmap = np.float32(heatmap) / 255\n",
    "# cam = heatmap + xd.permute(1,2,0).numpy()\n",
    "# cam = cam / np.max(cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a08aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = heatmap + cv2.cvtColor(maps.astype(np.uint8), cv2.COLOR_GRAY2RGB)\n",
    "u = u / np.max(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe443ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = cv2.addWeighted(\n",
    "    heatmap, 0.5, cv2.cvtColor(maps.astype(np.uint8), cv2.COLOR_GRAY2RGB) * 50, 0.7, 0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a36c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd5.png\", u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f371c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(maps, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dc8c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "start = r\"G:\\Windows\\Datasets\\Dataset Liver gH2Ax Signals\\dataset\\valid\\images\"\n",
    "out_folder = \"outputs\"\n",
    "out_folder = \"outputs2\"\n",
    "for im in os.listdir(start):\n",
    "    rgb = cv2.imread(os.path.join(start, im))[:, :, ::-1]\n",
    "    maps = inf2.predict([rgb])[0][0]\n",
    "    sss = (\n",
    "        cv2.imread(os.path.join(start.replace(\"images\", \"labels\"), im))[:, :, ::-1]\n",
    "        * 255\n",
    "    )\n",
    "    # sss = cv2.cvtColor((maps * 255.).astype(np.uint8),cv2.COLOR_GRAY2RGB)\n",
    "    final = cv2.addWeighted(rgb, 1, sss, 1, 0.0)\n",
    "    cv2.imwrite(os.path.join(out_folder, im), final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20282bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = cv2.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\input (2).png\")[:, :, ::-1]\n",
    "# rgb = cv2.resize(rgb, (512,512))\n",
    "maps = inf2.predict([rgb])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c70f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(maps.shape)\n",
    "sss = cv2.cvtColor((maps * 255.0).astype(np.uint8), cv2.COLOR_GRAY2RGB)\n",
    "final = cv2.addWeighted(rgb, 1, sss, 1, 0.0)\n",
    "print(final.shape)\n",
    "cv2.imwrite(\"xd7.png\", final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c5fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd7.png\", maps * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbd2a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = (\n",
    "    cv2.imread(\n",
    "        r\"G:\\Windows\\Datasets\\Dataset Liver gH2Ax Signals\\dataset\\valid\\labels\\12_Gr5T2_1s_28r_1200x_0y_.png\"\n",
    "    )\n",
    "    * 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e1a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = cv2.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\imgs\\imgs\\3_img.png\")[\n",
    "    :, :, ::-1\n",
    "]\n",
    "rgb = cv2.resize(rgb, (512, 512))\n",
    "maps = inf2.predict([rgb])[0][0]\n",
    "box_prediction = inf.predict([rgb], confidence=0.5)[0]\n",
    "\n",
    "label_map = np.zeros_like(maps).astype(np.int32)\n",
    "p = 1\n",
    "for b in box_prediction:\n",
    "    cv2.circle\n",
    "    label_map = cv2.circle(label_map, b[-1], 1, 255, -1)\n",
    "    p += 1\n",
    "\n",
    "cv2.imwrite(\n",
    "    \"xd2.png\",\n",
    "    label_map\n",
    "    + cv2.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\imgs\\imgs\\3_img.png\", 0),\n",
    ")\n",
    "# print(map.shape)\n",
    "# rgb_mask = cv2.cvtColor(maps.astype(np.float32) * 10., cv2.COLOR_GRAY2RGB).astype(np.uint8)\n",
    "# cv2.imwrite(\"xd.png\", rgb_mask + cv2.cvtColor(label_map.astype(np.float32), cv2.COLOR_GRAY2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = cv2.imread(\n",
    "    r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset_14810all\\dataset\\train\\images\\05__1s_0r_0x_256y_.png\"\n",
    ")[:, :, ::-1]\n",
    "rgb = cv2.resize(rgb, (512, 512))\n",
    "maps = inf.predict([rgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce599c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd1.png\", maps * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e4595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, sure_fg = cv2.threshold(maps, 0.7 * maps.max(), 255, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d39ad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd1.png\", sure_fg * 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d61d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_mask = cv2.cvtColor(maps.astype(np.uint8), cv2.COLOR_GRAY2RGB)\n",
    "markers = cv2.watershed(rgb_mask, label_map)\n",
    "empty = np.zeros_like(markers).astype(np.uint8)\n",
    "empty[markers == -1] = 255\n",
    "kernel = np.ones((2, 2), np.uint8)\n",
    "labels = cv2.dilate(empty, kernel)\n",
    "maps[labels == 255] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a30627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_greate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928262ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6aeb34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30436e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd1.png\", maps * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2561d715",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(markers, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833090b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb00d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, markers = cv2.connectedComponents(label_map)\n",
    "markers += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772d6949",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = cv2.watershed(rgb_mask, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f085fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "maps[labels == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cf0a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd2.png\", np.float32(maps) * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6d3d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((2, 2), np.uint8)\n",
    "labels = cv2.erode(np.uint8(labels), kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406408ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096d5c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c77ca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = (\n",
    "    cv2.imread(\n",
    "        r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\DatasetViktorFixed\\dataset\\train\\labels\\05__1_0256_1536.png\"\n",
    "    )\n",
    "    * 255\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a00e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd.png\", cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d006602",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a015f4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = rgb.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dbe809",
   "metadata": {},
   "outputs": [],
   "source": [
    "for box in test:\n",
    "    cv2.circle(im, box[-1], 1, (255, 255, 255), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d97a699",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd4.png\", im[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bfa357",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = cv2.imread(\n",
    "    r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\datasetClassified\\dataset\\valid\\images\\4.png\"\n",
    ")[:, :, ::-1]\n",
    "# rgb = cv2.resize(rgb, (512,512))\n",
    "# test = inf3.predict([rgb], confidence = 0.65, fill_holes = False)\n",
    "maps = inf2.predict([rgb])\n",
    "# cv2.imwrite(\"xd.png\", np.float32(test[0]) * 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36035827",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd1.png\", np.float32(maps[0][0]) * 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42265956",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\imgs\\imgs\"\n",
    "folder = (\n",
    "    r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset_14810all\\dataset\\valid\\images\"\n",
    ")\n",
    "# folder = r\"D:\\Hsa\\temp\\projects\\713a5ef7-8eb6-4d47-8e63-d70889ea91a2\\dataset\\train\\images\"\n",
    "# folder = r\"D:\\Hsa\\temp\\projects\\acf3ea56-507f-4578-9aa2-5f712a1d24e8\\dataset\\train\\images\"\n",
    "# folder = r\"D:\\Hsa\\temp\\projects\\ef91a20f-f5fb-4fda-b316-fee78913fcb0\\dataset\\train\\images\"\n",
    "# folder = r\"F:\\source\\repos\\Daten\\Nuclei\\Nuclei\\Nuclei\\val\\images\"\n",
    "with_box = False\n",
    "for im in os.listdir(folder):\n",
    "    # image = cv2.imread(os.path.join(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset_14810\\dataset\\train\\labels\", im)) * 255\n",
    "    # cv2.imwrite(os.path.join(\"bayer\", im), image)\n",
    "    rgb = cv2.imread(os.path.join(folder, im))[:, :, ::-1]\n",
    "    rgb = cv2.resize(rgb, (512, 512))\n",
    "    maps = inf2.predict([rgb])[0][0]\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    # coords = peak_local_max(np.float32(maps[1][0]) * 255., footprint=np.ones((3, 3)), labels=np.int32(maps[0][0]) * 255)\n",
    "    # mask = np.zeros(np.float32(maps[1][0]).shape, dtype=bool)\n",
    "    # mask[tuple(coords.T)] = True\n",
    "    # markers, _ = ndi.label(mask)\n",
    "    # markers[markers > 0] = 255\n",
    "    maps_copy = maps.copy()\n",
    "    if with_box:\n",
    "        box_prediction = inf.predict([rgb], confidence=0.45)[0]\n",
    "        label_map = np.zeros_like(maps).astype(np.int32)\n",
    "        p = 1\n",
    "        for b in box_prediction:\n",
    "            label_map = cv2.circle(label_map, b[-1], 1, p, -1)\n",
    "            p += 1\n",
    "        rgb_mask = cv2.cvtColor(maps.astype(np.uint8), cv2.COLOR_GRAY2RGB)\n",
    "        markers = cv2.watershed(rgb_mask, label_map)\n",
    "        empty = np.zeros_like(markers).astype(np.uint8)\n",
    "        empty[markers == -1] = 255\n",
    "        kernel = np.ones((2, 2), np.uint8)\n",
    "        labels = cv2.dilate(empty, kernel)\n",
    "        maps[labels == 255] = 0\n",
    "\n",
    "    # cv2.imwrite(os.path.join(\"bayer\", f\"dis{im}\"), np.float32(markers) * 255.)\n",
    "    cv2.imwrite(os.path.join(\"bayer\", f\"seg_{im}\"), np.float32(maps_copy) * 255.0)\n",
    "    if with_box:\n",
    "        cv2.imwrite(os.path.join(\"bayer\", f\"segbox_{im}\"), np.float32(maps) * 255)\n",
    "    # cv2.imwrite(os.path.join(\"bayer\", f\"dis{im}\"), np.float32(maps[1][0]) * 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a879bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = cv2.imread(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\imgs\\imgs\\3_img.png\")[\n",
    "    :, :, ::-1\n",
    "]\n",
    "rgb = cv2.resize(rgb, (512, 512))\n",
    "maps = inf2.predict([rgb])\n",
    "kernel = np.ones((2, 2), np.uint8)\n",
    "cv2.imwrite(\"xd2.png\", np.float32(maps[0][0]) * 255)\n",
    "cv2.imwrite(\"xd4.png\", cv2.erode(np.float32(maps[0][0]) * 255, kernel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acbba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd1.png\", np.float32(maps[1][0]) * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faca650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage as ndi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71e94d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = maps[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7befff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = np.unique(predictions)\n",
    "all_classes = np.zeros_like(predictions)\n",
    "for val in unique_values:\n",
    "    if val == 0:\n",
    "        continue\n",
    "    print(val)\n",
    "    tmp = np.zeros_like(predictions)\n",
    "    tmp[predictions == val] = 1\n",
    "    tmp = np.uint8(ndi.binary_fill_holes(tmp))\n",
    "    tmp = tmp.astype(np.int32)\n",
    "    all_classes[tmp == 1] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ca173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd.png\", all_classes * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cbdf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.float32(maps[1][0]) * 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67615a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[a < 175] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0981504",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd1.png\", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c4c424",
   "metadata": {},
   "outputs": [],
   "source": [
    "maps[1][0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38da6e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd.png\", np.float32(maps[1][0]) * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e58b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = cv2.imread(\n",
    "    r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Cells her2 256\\valid\\images\\05__1_5352-10677_170.png\"\n",
    ")  # [:,:,::-1]\n",
    "orig = rgb.copy()\n",
    "box_image = np.zeros_like(rgb)\n",
    "box_image = cv2.cvtColor(box_image, cv2.COLOR_BGR2GRAY)\n",
    "boxes = inf.predict([rgb[:, :, ::-1]], confidence=0.45)[0]\n",
    "maps = inf2.predict([rgb[:, :, ::-1]])\n",
    "dist = maps[1][0]\n",
    "maps = maps[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fb1807",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca82a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "maps[maps == 1] = 50\n",
    "maps[maps == 2] = 100\n",
    "maps[maps == 3] = 150\n",
    "maps[maps == 4] = 200\n",
    "cv2.imwrite(\"xd2.png\", maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53cc54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = np.int32(np.zeros_like(maps))\n",
    "p = 1\n",
    "used = []\n",
    "for box in boxes:\n",
    "    tmp = np.random.randint(10, 255)\n",
    "    while tmp in used:\n",
    "        tmp = np.random.randint(10, 255)\n",
    "    used.append(tmp)\n",
    "    label_map = cv2.circle(label_map, box[-1], 1, tmp, -1)\n",
    "    p += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c249826",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d689bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapss = np.uint8(ndi.binary_fill_holes(maps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2f4733",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(mapss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dc74af",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = ndi.distance_transform_edt(maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aebda49",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = watershed(-distance, label_map, mask=maps, watershed_line=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b401d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "watershed_lines = np.zeros(shape=np.shape(labels))\n",
    "# watershed_lines(labels==0)=1 # ws lines are labeled as 0 in markers\n",
    "# watershed_lines_thick = dilation(bright_pixel, square(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03bfdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "watershed_lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126bf61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdd = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f3b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdd = cv2.add(np.uint8(labels), np.uint8(label_map * 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6f80ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((2, 2), np.uint8)\n",
    "import cv2\n",
    "\n",
    "# Using cv2.erode() method\n",
    "# xdd = cv2.erode(xdd, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ab04f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, len(np.unique(maps)) + 1):\n",
    "    maps[maps == i] = i + 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1246079",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd1.png\", orig)\n",
    "cv2.imwrite(\"xd3.png\", xdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885e362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9f8127",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdd = np.uint8(xdd)\n",
    "maps = np.uint8(maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf5c9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdd[xdd > 0] = maps[xdd > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb5beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd.png\", xdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69184ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute(arr):\n",
    "    import numpy as np\n",
    "    from scipy.ndimage import label, binary_dilation\n",
    "    from collections import Counter\n",
    "\n",
    "    imputed_array = np.copy(arr)\n",
    "\n",
    "    mask = np.isnan(arr)\n",
    "    labels, count = label(mask)\n",
    "    for idx in range(1, count + 1):\n",
    "        hole = labels == idx\n",
    "        surrounding_values = arr[binary_dilation(hole) & ~hole]\n",
    "        most_frequent = Counter(surrounding_values).most_common(1)[0][0]\n",
    "        imputed_array[hole] = most_frequent\n",
    "\n",
    "    return imputed_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147efa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdd = impute(xdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35dcb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdd[xdd == 1] = 25\n",
    "xdd[xdd == 2] = 75\n",
    "xdd[xdd == 3] = 125\n",
    "xdd[xdd == 4] = 150\n",
    "xdd[xdd == 5] = 250\n",
    "cv2.imwrite(\"xd2.png\", xdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f94ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(xdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cce3a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "from skimage.segmentation import watershed\n",
    "from skimage.feature import peak_local_max\n",
    "\n",
    "\n",
    "# Generate an initial image with two overlapping circles\n",
    "x, y = np.indices((80, 80))\n",
    "x1, y1, x2, y2 = 28, 28, 44, 52\n",
    "r1, r2 = 16, 20\n",
    "mask_circle1 = (x - x1) ** 2 + (y - y1) ** 2 < r1**2\n",
    "mask_circle2 = (x - x2) ** 2 + (y - y2) ** 2 < r2**2\n",
    "image = np.logical_or(mask_circle1, mask_circle2)\n",
    "\n",
    "# Now we want to separate the two objects in image\n",
    "# Generate the markers as local maxima of the distance to the background\n",
    "distance = ndi.distance_transform_edt(image)\n",
    "coords = peak_local_max(distance, footprint=np.ones((3, 3)), labels=image)\n",
    "mask = np.zeros(distance.shape, dtype=bool)\n",
    "mask[tuple(coords.T)] = True\n",
    "markers, _ = ndi.label(mask)\n",
    "labels = watershed(-distance, markers, mask=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e984ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22dfc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fe9c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ce88d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea22dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705871d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = np.uint8(maps[0] * 255)\n",
    "# kernel = np.ones((2,2),np.uint8)\n",
    "# sure_bg = cv2.dilate(maps,kernel,iterations=3)\n",
    "sure_bg = maps\n",
    "\n",
    "sure_fg = np.uint8(maps)\n",
    "unknown = cv2.subtract(sure_bg, sure_fg)\n",
    "ret, markers = cv2.connectedComponents(sure_fg)\n",
    "markers = box_image\n",
    "# Add one to all labels so that sure background is not 0, but 1\n",
    "markers = markers + 1\n",
    "markers[unknown == 255] = 0\n",
    "markers = cv2.watershed(orig, markers)\n",
    "orig[markers == -1] = [255, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6c9234",
   "metadata": {},
   "source": [
    "# watershedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a9ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "from skimage.segmentation import watershed\n",
    "from skimage.feature import peak_local_max\n",
    "\n",
    "\n",
    "# Generate an initial image with two overlapping circles\n",
    "x, y = np.indices((80, 80))\n",
    "x1, y1, x2, y2 = 28, 28, 44, 52\n",
    "r1, r2 = 16, 20\n",
    "mask_circle1 = (x - x1) ** 2 + (y - y1) ** 2 < r1**2\n",
    "mask_circle2 = (x - x2) ** 2 + (y - y2) ** 2 < r2**2\n",
    "image = np.logical_or(mask_circle1, mask_circle2)\n",
    "\n",
    "# Now we want to separate the two objects in image\n",
    "# Generate the markers as local maxima of the distance to the background\n",
    "distance = ndi.distance_transform_edt(image)\n",
    "coords = peak_local_max(distance, footprint=np.ones((3, 3)), labels=image)\n",
    "mask = np.zeros(distance.shape, dtype=bool)\n",
    "mask[tuple(coords.T)] = True\n",
    "markers, _ = ndi.label(mask)\n",
    "labels = watershed(-distance, markers, mask=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325e6694",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(9, 3), sharex=True, sharey=True)\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(image, cmap=plt.cm.gray)\n",
    "ax[0].set_title(\"Overlapping objects\")\n",
    "ax[1].imshow(-distance, cmap=plt.cm.gray)\n",
    "ax[1].set_title(\"Distances\")\n",
    "ax[2].imshow(labels, cmap=plt.cm.nipy_spectral)\n",
    "ax[2].set_title(\"Separated objects\")\n",
    "\n",
    "for a in ax:\n",
    "    a.set_axis_off()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3b49f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(distance, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50e6c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9905c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualdl import vdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b8f09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdl.train(r\"F:\\source\\repos\\VisualDL\\visualdl\\trainer\\detection\\detection.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4c70da",
   "metadata": {},
   "source": [
    "# Roche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978f505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a6cada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c79a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "count_zero = 0\n",
    "count_one = 0\n",
    "start = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\ROCHE\\dataset\\train\\labels\"\n",
    "all_files = os.listdir(start)\n",
    "nc = 2\n",
    "cl = 0\n",
    "for img in os.listdir(start):\n",
    "    for i in range(1, nc):\n",
    "        im = cv.imread(os.path.join(start, img), 0)\n",
    "        co = im.copy()\n",
    "        im[im > 0] = 1.0\n",
    "        kernel = np.ones((2, 2), np.uint8)\n",
    "        # im = cv.erode(im, kernel)\n",
    "        # im = cv.dilate(im, kernel)\n",
    "        tmp = im.copy()\n",
    "        # tmp[tmp != i] = 0\n",
    "        # tmp[tmp == i] = 255\n",
    "        tmp[tmp > 0] = 255\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        tmp = cv2.dilate(tmp, kernel)\n",
    "        # cv2.imwrite(\"xd.png\", tmp)\n",
    "\n",
    "        # if img == \"05__1_3130-9263_11.png\":\n",
    "        #    cv2.imwrite(\"xd.png\", tmp)\n",
    "        contours, hierachy = cv.findContours(tmp, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        for cnt, cont in enumerate(contours):\n",
    "            blank = np.zeros_like(tmp)\n",
    "            xmin, ymin, width, height = cv.boundingRect(cont)\n",
    "            if width <= 3 or height <= 3:\n",
    "                continue\n",
    "            cv2.drawContours(blank, [cont], -1, 255, -1)\n",
    "            # cv.rectangle(im,(x,y),(x+width,y+height),(255),1)\n",
    "            # cv2.imwrite(\"xd.png\", im)\n",
    "            image_height = im.shape[0]\n",
    "            image_width = im.shape[1]\n",
    "            pts = np.where(blank > 0)\n",
    "\n",
    "            if len(np.unique(co[pts[0], pts[1]])) == 2:\n",
    "                cl = 0\n",
    "                count_zero += 1\n",
    "            else:\n",
    "                cl = 1\n",
    "                count_one += 1\n",
    "\n",
    "            xcenter, ycenter = xmin + width / 2, ymin + height / 2\n",
    "            xcenter, ycenter, width, height = (\n",
    "                xcenter / image_width,\n",
    "                ycenter / image_height,\n",
    "                width / image_width,\n",
    "                height / image_height,\n",
    "            )\n",
    "            if not img in files:\n",
    "                files[img] = [\n",
    "                    (str(cl), str(xcenter), str(ycenter), str(width), str(height))\n",
    "                ]\n",
    "            else:\n",
    "\n",
    "                files[img] += [\n",
    "                    (str(cl), str(xcenter), str(ycenter), str(width), str(height))\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec2884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in files.items():\n",
    "    base = cv.imread(\n",
    "        os.path.join(\n",
    "            r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\ROCHE\\dataset\\train/images\", key\n",
    "        )\n",
    "    )\n",
    "    label = cv.imread(\n",
    "        os.path.join(\n",
    "            r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\ROCHE\\dataset\\train/labels\", key\n",
    "        )\n",
    "    )\n",
    "    for cnt, rec in enumerate(val):\n",
    "        xcenter, ycenter, width, height = [float(xx) for xx in rec[1:]]\n",
    "        xcenter *= base.shape[1]\n",
    "        width *= base.shape[1]\n",
    "        ycenter *= base.shape[0]\n",
    "        height *= base.shape[0]\n",
    "        xcenter = int(xcenter)\n",
    "        ycenter = int(ycenter)\n",
    "        width = int(width)\n",
    "        height = int(height)\n",
    "        x = int(xcenter - width / 2)\n",
    "        y = int(ycenter - height / 2)\n",
    "        va = int((int(rec[0]) + 1) * 100)\n",
    "        # cv.circle(base, (xcenter,ycenter), 1, (255,255,255), -1)\n",
    "        cv.rectangle(\n",
    "            base, (int(x), int(y)), (int(x + width), int(y + height)), (va, va, va), 1\n",
    "        )\n",
    "        xd1 = base[y : y + height, x : x + width]\n",
    "        xd2 = label[y : y + height, x : x + width]\n",
    "        cv.imwrite(\n",
    "            os.path.join(\n",
    "                r\"F:\\source\\repos\\VisualDL\\custom_experiments\\CutTrainingData\\images\",\n",
    "                f\"{cnt}{key}.png\",\n",
    "            ),\n",
    "            xd1,\n",
    "        )\n",
    "        cv.imwrite(\n",
    "            os.path.join(\n",
    "                r\"F:\\source\\repos\\VisualDL\\custom_experiments\\CutTrainingData\\labels\",\n",
    "                f\"{cnt}{key}.png\",\n",
    "            ),\n",
    "            xd2,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f5c1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = cv.imread(\n",
    "    r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\ROCHE\\dataset\\valid/images\\A065 - 20211005_111945_0.png\"\n",
    ")\n",
    "\n",
    "for cnt, rec in enumerate(files[\"A065 - 20211005_111945_0.png\"]):\n",
    "    xcenter, ycenter, width, height = [float(xx) for xx in rec[1:]]\n",
    "    xcenter *= base.shape[1]\n",
    "    width *= base.shape[1]\n",
    "    ycenter *= base.shape[0]\n",
    "    height *= base.shape[0]\n",
    "    xcenter = int(xcenter)\n",
    "    ycenter = int(ycenter)\n",
    "    width = int(width)\n",
    "    height = int(height)\n",
    "\n",
    "    x = int(xcenter - width / 2)\n",
    "    y = int(ycenter - height / 2)\n",
    "    va = int((int(rec[0]) + 1) * 100)\n",
    "\n",
    "    # cv.circle(base, (xcenter,ycenter), 1, (255,255,255), -1)\n",
    "    cv.rectangle(\n",
    "        base, (int(x), int(y)), (int(x + width), int(y + height)), (va, va, va), 1\n",
    "    )\n",
    "    xd = base[y : y + height, x : x + width]\n",
    "    cv.imwrite(f\"{cnt}.png\", xd)\n",
    "label = cv.imread(\n",
    "    r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\ROCHE\\dataset\\valid\\labels\\A065 - 20211005_111945_0.png\"\n",
    ")\n",
    "cv2.imwrite(\"xd5.png\", label * 100)\n",
    "cv.imwrite(\"xd4.png\", base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64393df",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"labels\")\n",
    "for cnt, (key, val) in enumerate(files.items()):\n",
    "    with open(\"labels/\" + key.replace(\".png\", \".txt\"), \"w\") as handle:\n",
    "        for va in val:\n",
    "            handle.write(\" \".join(list(va)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82042cd1",
   "metadata": {},
   "source": [
    "# od inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5985d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualdl import vdl\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef031c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf = vdl.get_inference_model(\n",
    "    r\"F:\\source\\repos\\VisualDL\\runs\\exp62\\weights\\best.pt\", type=\"od\"\n",
    ")\n",
    "inf2 = vdl.get_inference_model(\n",
    "    r\"F:\\source\\repos\\VisualDL\\tu-resnest50d, UnetPlusPlus.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a223f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_folder = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\ROCHE\\dataset\\valid/images\"\n",
    "for c, im in enumerate(os.listdir(val_folder)):\n",
    "    label = cv2.imread(os.path.join(val_folder.replace(\"images\", \"labels\"), im))\n",
    "    rgb = cv2.imread(os.path.join(val_folder, im))[:, :, ::-1]\n",
    "    rgb = cv2.resize(rgb, (512, 512))\n",
    "    label = cv2.resize(label, (512, 512))\n",
    "    test = inf.predict([rgb], confidence=0.35)[0]\n",
    "    maps = inf2.predict([rgb])[0][0]\n",
    "    to_remove = []\n",
    "    for box in test:\n",
    "        for b in test:\n",
    "            if box != b:\n",
    "                cnt = 0\n",
    "                for i in range(4):\n",
    "                    if abs(box[i] - b[i]) <= 5:\n",
    "                        cnt += 1\n",
    "                if cnt == 4:\n",
    "                    if box[-3] < b[-3]:\n",
    "                        to_remove.append(b)\n",
    "                    else:\n",
    "                        to_remove.append(box)\n",
    "    to_remove = list(set(to_remove))\n",
    "    # for val in to_remove:\n",
    "    #    test.remove(val)\n",
    "    for box in test:\n",
    "        print(box)\n",
    "        if box[-2] == 1.0:\n",
    "            val = (255, 0, 0)\n",
    "        else:\n",
    "            val = (0, 255, 0)\n",
    "\n",
    "    cv2.imwrite(f\"label{im}.png\", label * 150)\n",
    "    cv2.imwrite(f\"seg{im}.png\", maps * 50)\n",
    "    cv2.imwrite(f\"{im}.png\", rgb[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c91fe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.rectangle(base, (int(x), int(y)), (int(x + width), int(y + height)), (va, va, va), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c5598a",
   "metadata": {},
   "source": [
    "# Roche inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d1697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506fb8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a6d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualdl import vdl\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage import morphology\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.segmentation import watershed\n",
    "from torch import nn\n",
    "import torch\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "inf2 = vdl.get_inference_model(\n",
    "    r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\ROCHE\\001.pt\"\n",
    ")\n",
    "val_folder = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\ROCHE\\dataset\\valid/images\"\n",
    "for c, im in enumerate(os.listdir(val_folder)):\n",
    "    img = cv2.imread(os.path.join(val_folder, im))[:, :, ::-1]\n",
    "\n",
    "    img = cv2.resize(img, (512, 512))\n",
    "    orig = img.copy()\n",
    "    maps = inf2.predict([img])[0][0]\n",
    "\n",
    "    liquid = (maps == 1).astype(\"uint8\")\n",
    "    bubble = (maps == 2).astype(\"uint8\")\n",
    "\n",
    "    contours_liquid, hierarchy = cv2.findContours(\n",
    "        liquid, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE\n",
    "    )\n",
    "\n",
    "    contours_bubble, hierarchy = cv2.findContours(\n",
    "        bubble, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE\n",
    "    )\n",
    "\n",
    "    overlay = img.copy()\n",
    "    cv2.fillPoly(overlay, contours_liquid, (255, 0, 0))\n",
    "    cv2.fillPoly(overlay, contours_bubble, (0, 0, 255))\n",
    "    cv2.addWeighted(overlay, 0.4, img, 1 - 0.4, 0, img)\n",
    "    fig = plt.figure(figsize=(15, 12))\n",
    "    rows = 1\n",
    "    columns = 3\n",
    "    fig.add_subplot(rows, columns, 1)\n",
    "\n",
    "    # showing image\n",
    "    plt.imshow(orig)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Original\")\n",
    "\n",
    "    fig.add_subplot(rows, columns, 2)\n",
    "\n",
    "    # showing image\n",
    "    plt.imshow(maps)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Prediction\")\n",
    "\n",
    "    fig.add_subplot(rows, columns, 3)\n",
    "\n",
    "    # showing image\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Overlayed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc632fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93c4bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\n",
    "    \"xd.png\",\n",
    "    cv2.imread(\n",
    "        r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\datasetClassified\\dataset\\train\\labels\\05__1_256_0.png\"\n",
    "    )\n",
    "    * 255,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c07747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\WirklichNeu\\dataset\\valid\\images\"\n",
    "\n",
    "for cnt, im in enumerate(os.listdir(folder)):\n",
    "    os.rename(os.path.join(folder, im), os.path.join(folder, f\"big{cnt}.png\"))\n",
    "    # cv2.imwrite(os.path.join(\"255\", im), cv2.imread(os.path.join(folder, im)) * 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e7f416",
   "metadata": {},
   "source": [
    "# Test marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e42fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.morphology import skeletonize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7071002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_map_fixed(mask):\n",
    "    # mask[mask > 0] = 255 #set all classes to the same value\n",
    "    distances = []\n",
    "    for cnt2, img in enumerate(mask):\n",
    "        to = np.zeros_like(img, dtype=np.float32)\n",
    "        contours, hierarchy = cv2.findContours(\n",
    "            image=img, mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE\n",
    "        )\n",
    "        for i, cnt in enumerate(contours):\n",
    "            mask2 = np.zeros_like(img)\n",
    "            cv2.drawContours(mask2, [cnt], -1, 1, -1)\n",
    "            skeleton = skeletonize(mask2)\n",
    "            dist = cv2.distanceTransform(mask2, cv2.DIST_L2, 5)\n",
    "            ab = cv2.normalize(dist, dist, 0, 1.0, cv2.NORM_MINMAX)\n",
    "            pts = np.where(skeleton > 0)\n",
    "            to[pts[0], pts[1]] = ab[pts[0], pts[1]]\n",
    "        distances.append(to)\n",
    "\n",
    "    a = np.stack(np.array(distances), axis=0)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ee3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_input = (\n",
    "    cv2.imread(\n",
    "        r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset_14810all\\dataset\\valid\\labels\\05__1s_0r_0x_768y_.png\",\n",
    "        0,\n",
    "    ).astype(np.uint8)\n",
    "    * 255\n",
    ")\n",
    "marker_input = marker_input * 255\n",
    "im = (\n",
    "    cv2.imread(\n",
    "        r\"F:\\source\\repos\\VisualDL\\custom_experiments\\bayer\\seg_05__1s_0r_0x_768y_.png\",\n",
    "        0,\n",
    "    ).astype(np.uint8)\n",
    "    * 255\n",
    ")\n",
    "# im = im * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1754be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((2, 2), \"uint8\")\n",
    "bb = cv2.dilate(im, kernel, iterations=3)\n",
    "sure_fg = bb.copy()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a41fc76",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15076d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = get_distance_map_fixed([marker_input])[0]\n",
    "tt[tt < 0.4] = 0\n",
    "tt[tt > 0] = 255\n",
    "ret, markers = cv2.connectedComponents(tt.astype(np.uint8))\n",
    "# markers += 1\n",
    "# markers = tt.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b79b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_mask = cv2.cvtColor(sure_fg.astype(np.uint8) * 255, cv2.COLOR_GRAY2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bb3031",
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = cv2.watershed(rgb_mask, markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556ed77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = np.zeros_like(markers).astype(np.uint8)\n",
    "empty[markers == -1] = 255\n",
    "kernel = np.ones((2, 2), np.uint8)\n",
    "labels = cv2.dilate(empty, kernel)\n",
    "sure_fg[empty == 255] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45690fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"orig.png\", im * 255.0)\n",
    "cv2.imwrite(\"marker_input.png\", tt * 255)\n",
    "cv2.imwrite(\"segmentation.png\", sure_fg * 255)\n",
    "cv2.imwrite(\"original_image.png\", marker_input * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ab0630",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(markers, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e25bf27",
   "metadata": {},
   "source": [
    "# Caranet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ef3883",
   "metadata": {},
   "outputs": [],
   "source": [
    "class caranet(nn.Module):\n",
    "    def __init__(self, channel=32):\n",
    "        super().__init__()\n",
    "\n",
    "        # ---- ResNet Backbone ----\n",
    "        self.resnet = res2net101_v1b_26w_4s(pretrained=True)\n",
    "\n",
    "        # Receptive Field Block\n",
    "        self.rfb2_1 = Conv(512, 32, 3, 1, padding=1, bn_acti=True)\n",
    "        self.rfb3_1 = Conv(1024, 32, 3, 1, padding=1, bn_acti=True)\n",
    "        self.rfb4_1 = Conv(2048, 32, 3, 1, padding=1, bn_acti=True)\n",
    "\n",
    "        # Partial Decoder\n",
    "        self.agg1 = aggregation(channel)\n",
    "\n",
    "        self.CFP_1 = CFPModule(32, d=8)\n",
    "        self.CFP_2 = CFPModule(32, d=8)\n",
    "        self.CFP_3 = CFPModule(32, d=8)\n",
    "        ###### dilation rate 4, 62.8\n",
    "\n",
    "        self.ra1_conv1 = Conv(32, 32, 3, 1, padding=1, bn_acti=True)\n",
    "        self.ra1_conv2 = Conv(32, 32, 3, 1, padding=1, bn_acti=True)\n",
    "        self.ra1_conv3 = Conv(32, 1, 3, 1, padding=1, bn_acti=True)\n",
    "\n",
    "        self.ra2_conv1 = Conv(32, 32, 3, 1, padding=1, bn_acti=True)\n",
    "        self.ra2_conv2 = Conv(32, 32, 3, 1, padding=1, bn_acti=True)\n",
    "        self.ra2_conv3 = Conv(32, 1, 3, 1, padding=1, bn_acti=True)\n",
    "\n",
    "        self.ra3_conv1 = Conv(32, 32, 3, 1, padding=1, bn_acti=True)\n",
    "        self.ra3_conv2 = Conv(32, 32, 3, 1, padding=1, bn_acti=True)\n",
    "        self.ra3_conv3 = Conv(32, 1, 3, 1, padding=1, bn_acti=True)\n",
    "\n",
    "        self.aa_kernel_1 = AA_kernel(32, 32)\n",
    "        self.aa_kernel_2 = AA_kernel(32, 32)\n",
    "        self.aa_kernel_3 = AA_kernel(32, 32)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)  # bs, 64, 88, 88\n",
    "\n",
    "        # ----------- low-level features -------------\n",
    "\n",
    "        x1 = self.resnet.layer1(x)  # bs, 256, 88, 88\n",
    "        x2 = self.resnet.layer2(x1)  # bs, 512, 44, 44\n",
    "\n",
    "        x3 = self.resnet.layer3(x2)  # bs, 1024, 22, 22\n",
    "        x4 = self.resnet.layer4(x3)  # bs, 2048, 11, 11\n",
    "\n",
    "        x2_rfb = self.rfb2_1(x2)  # 512 - 32\n",
    "        x3_rfb = self.rfb3_1(x3)  # 1024 - 32\n",
    "        x4_rfb = self.rfb4_1(x4)  # 2048 - 32\n",
    "\n",
    "        decoder_1 = self.agg1(x4_rfb, x3_rfb, x2_rfb)  # 1,44,44\n",
    "        lateral_map_1 = F.interpolate(decoder_1, scale_factor=8, mode=\"bilinear\")\n",
    "\n",
    "        # ------------------- atten-one -----------------------\n",
    "        decoder_2 = F.interpolate(decoder_1, scale_factor=0.25, mode=\"bilinear\")\n",
    "        cfp_out_1 = self.CFP_3(x4_rfb)  # 32 - 32\n",
    "        decoder_2_ra = -1 * (torch.sigmoid(decoder_2)) + 1\n",
    "        aa_atten_3 = self.aa_kernel_3(cfp_out_1)\n",
    "        aa_atten_3_o = decoder_2_ra.expand(-1, 32, -1, -1).mul(aa_atten_3)\n",
    "\n",
    "        ra_3 = self.ra3_conv1(aa_atten_3_o)  # 32 - 32\n",
    "        ra_3 = self.ra3_conv2(ra_3)  # 32 - 32\n",
    "        ra_3 = self.ra3_conv3(ra_3)  # 32 - 1\n",
    "\n",
    "        x_3 = ra_3 + decoder_2\n",
    "        lateral_map_2 = F.interpolate(x_3, scale_factor=32, mode=\"bilinear\")\n",
    "\n",
    "        # ------------------- atten-two -----------------------\n",
    "        decoder_3 = F.interpolate(x_3, scale_factor=2, mode=\"bilinear\")\n",
    "        cfp_out_2 = self.CFP_3(x3_rfb)  # 32 - 32\n",
    "        decoder_3_ra = -1 * (torch.sigmoid(decoder_3)) + 1\n",
    "        aa_atten_2 = self.aa_kernel_2(cfp_out_2)\n",
    "        aa_atten_2_o = decoder_3_ra.expand(-1, 32, -1, -1).mul(aa_atten_2)\n",
    "\n",
    "        ra_2 = self.ra2_conv1(aa_atten_2_o)  # 32 - 32\n",
    "        ra_2 = self.ra2_conv2(ra_2)  # 32 - 32\n",
    "        ra_2 = self.ra2_conv3(ra_2)  # 32 - 1\n",
    "\n",
    "        x_2 = ra_2 + decoder_3\n",
    "        lateral_map_3 = F.interpolate(x_2, scale_factor=16, mode=\"bilinear\")\n",
    "\n",
    "        # ------------------- atten-three -----------------------\n",
    "        decoder_4 = F.interpolate(x_2, scale_factor=2, mode=\"bilinear\")\n",
    "        cfp_out_3 = self.CFP_1(x2_rfb)  # 32 - 32\n",
    "        decoder_4_ra = -1 * (torch.sigmoid(decoder_4)) + 1\n",
    "        aa_atten_1 = self.aa_kernel_1(cfp_out_3)\n",
    "        aa_atten_1_o = decoder_4_ra.expand(-1, 32, -1, -1).mul(aa_atten_1)\n",
    "\n",
    "        ra_1 = self.ra1_conv1(aa_atten_1_o)  # 32 - 32\n",
    "        ra_1 = self.ra1_conv2(ra_1)  # 32 - 32\n",
    "        ra_1 = self.ra1_conv3(ra_1)  # 32 - 1\n",
    "\n",
    "        x_1 = ra_1 + decoder_4\n",
    "        lateral_map_5 = F.interpolate(x_1, scale_factor=8, mode=\"bilinear\")\n",
    "\n",
    "        return lateral_map_5, lateral_map_3, lateral_map_2, lateral_map_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054fadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "start = r\"C:/Users/phili/Downloads/Telegram Desktop/MegNeueDaten/dataset/valid/labels\"\n",
    "all_files = os.listdir(start)\n",
    "nc = 2\n",
    "for img in os.listdir(start):\n",
    "    for i in range(1, nc):\n",
    "        im = cv.imread(os.path.join(start, img), 0)\n",
    "        im[im > 0] = 1.0\n",
    "        kernel = np.ones((2, 2), np.uint8)\n",
    "        # im = cv.erode(im, kernel)\n",
    "        # im = cv.dilate(im, kernel)\n",
    "        tmp = im.copy()\n",
    "        tmp[tmp != i] = 0\n",
    "        tmp[tmp == i] = 255\n",
    "        # if img == \"05__1_3130-9263_11.png\":\n",
    "        #    cv2.imwrite(\"xd.png\", tmp)\n",
    "        contours, hierachy = cv.findContours(tmp, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "        blank = np.zeros_like(tmp)\n",
    "        for cnt, cont in enumerate(contours):\n",
    "            xmin, ymin, width, height = cv.boundingRect(cont)\n",
    "            if width <= 3 or height <= 3:\n",
    "                continue\n",
    "            # cv.rectangle(im,(x,y),(x+width,y+height),(255),1)\n",
    "            # cv2.imwrite(\"xd.png\", im)\n",
    "            image_width = im.shape[0]\n",
    "            xcenter, ycenter = xmin + width / 2, ymin + height / 2\n",
    "            xcenter, ycenter, width, height = (\n",
    "                xcenter / image_width,\n",
    "                ycenter / image_width,\n",
    "                width / image_width,\n",
    "                height / image_width,\n",
    "            )\n",
    "            if not img in files:\n",
    "                files[img] = [\n",
    "                    (str(i - 1), str(xcenter), str(ycenter), str(width), str(height))\n",
    "                ]\n",
    "            else:\n",
    "\n",
    "                files[img] += [\n",
    "                    (str(i - 1), str(xcenter), str(ycenter), str(width), str(height))\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4e53f3",
   "metadata": {},
   "source": [
    "# Create maskrcnn dataset from\n",
    "- https://www.kaggle.com/rluethy/sartorius-torch-mask-r-cnn\n",
    "- https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239f26e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d628e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = io.imread(\n",
    "    r\"D:\\Hsa\\temp\\projects\\168714ca-2337-44cb-b384-36215f416198\\dataset\\train\\labels\\05__1s_0r_0x_4532y_.png\",\n",
    "    as_gray=True,\n",
    ")\n",
    "tmp = label.copy()\n",
    "tmp[tmp > 0] = 1\n",
    "contours, hierachy = cv2.findContours(tmp, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "boxes = []\n",
    "labels = []\n",
    "masks = [np.zeros_like(label)] * len(contours)\n",
    "# image_id = torch.tensor([idx])\n",
    "area = []\n",
    "is_crowd = [0] * len(contours)\n",
    "for cnt, cont in enumerate(contours):\n",
    "    rect = cv2.boundingRect(cont)\n",
    "    xmin, ymin, xmax, ymax = rect[0], rect[1], rect[0] + rect[2], rect[1] + rect[3]\n",
    "    rect = (xmin, ymin, xmax, ymax)\n",
    "    mask2 = np.zeros_like(tmp)\n",
    "    cv2.drawContours(mask2, [cont], -1, 255, -1)\n",
    "    pts = np.where(mask2 > 0)\n",
    "    _cls = label[pts[0], pts[1]]\n",
    "    cls = np.unique(_cls)[np.argmax(np.unique(_cls, return_counts=True)[1])]\n",
    "    is_crowd = 0\n",
    "    area = (rect[3] - rect[1]) * (rect[2] - rect[0])\n",
    "    mask2[pts[0], pts[1]] = label[pts[0], pts[1]]\n",
    "    boxes.append(rect)\n",
    "    labels.append(cls)\n",
    "    masks[cnt] = mask2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ced2dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd.png\", masks[32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54737b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84506c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "folder = r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\ddddddddddddd\\dataset\\train\\labels\"\n",
    "out_folder = \"bayer2\"\n",
    "for im in os.listdir(folder):\n",
    "    image_255 = cv2.imread(os.path.join(folder, im), 0) * 255.0\n",
    "    cv2.imwrite(os.path.join(out_folder, im), image_255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eabba6b",
   "metadata": {},
   "source": [
    "# maskrcnn inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "624ee96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "import cv2\n",
    "import random\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from visualdl import vdl\n",
    "\n",
    "%matplotlib inline\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72e5b3b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ModelInference' object has no attribute 'cfg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f9aa854c26ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#model.eval()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#maskrcnn512OhneCustomAnchor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvdl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_inference_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"F:\\source\\repos\\VisualDL\\maskrcnn.pt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"instance\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\visualdl-0.0.9-py3.8.egg\\visualdl\\vdl.py\u001b[0m in \u001b[0;36mget_inference_model\u001b[1;34m(weights, type, watershed_od)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_inference_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"segmentation\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwatershed_od\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mModelInference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwatershed_od\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwatershed_od\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\visualdl-0.0.9-py3.8.egg\\visualdl\\inference\\inference.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, weight_path, device, type, watershed_od)\u001b[0m\n\u001b[0;32m     93\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mGeneralizedRCNNTransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_mean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.485\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.456\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.406\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_std\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.229\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.225\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m800\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1333\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bilinear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m\"in_channels\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m                 self.model.backbone.body.conv1 = torch.nn.Conv2d(self.cfg['settings']['in_channels'],\n\u001b[0m\u001b[0;32m     96\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_channels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                                 \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ModelInference' object has no attribute 'cfg'"
     ]
    }
   ],
   "source": [
    "# model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False, box_detections_per_img = 800,rpn_anchor_generator =\n",
    "#                        AnchorGenerator(((8,), (16,), (32,), (64,), (128,)), ((0.5, 1.0, 2.0),) * 5\n",
    "#                    ))\n",
    "# model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False, box_detections_per_img = 800)\n",
    "# model.roi_heads.mask_predictor.mask_fcn_logits = Conv2d(256, 2, 1)\n",
    "# model.load_state_dict(torch.load(r\"F:\\source\\repos\\VisualDL\\maskrcnn.pt\")['model_state_dict'])\n",
    "# model.eval()\n",
    "# maskrcnn512OhneCustomAnchor\n",
    "model = vdl.get_inference_model(\n",
    "    r\"F:\\source\\repos\\VisualDL\\maskrcnn.pt\", type=\"instance\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb14cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9cebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state[\"custom_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2067e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.load(\n",
    "    r\"D:\\Hsa\\temp\\custom_models\\verified_models\\Custom - tubuli\\instance_seg\\001.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46d7f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model[\"custom_data\"][\"image_size\"] = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b36ca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[\"custom_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5088d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\n",
    "    r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\dataset (21)\\dataset\\train\\images\\10_1_bm_annonymisiert_0s_0r_0x_442y_.png\"\n",
    ")[:, :, ::-1]\n",
    "out = model.predict([img])[0]\n",
    "# cv2.imwrite(\"xd.png\",out[0] * 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076ee33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6440dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.load(r\"F:\\source\\repos\\VisualDL\\maskrcnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9149d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8030a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[\n",
    "    \"model\"\n",
    "] = \"torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True, box_detections_per_img = 800)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d491989",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[\"custom_data\"] = {\n",
    "    \"structure_indices\": [3],\n",
    "    \"image_size\": 2048,\n",
    "    \"modeltype\": \"instance segmentation\",\n",
    "    \"object_based\": False,\n",
    "    \"physical_tile_size\": (0.0, 0.0),\n",
    "    \"project_type\": \"dummy\",\n",
    "    \"pyramid_level\": -1,\n",
    "    \"datetime\": \"14/12/2021 11:49\",\n",
    "    \"structures\": \"Tubulus\",\n",
    "    \"objects_count\": 3606,\n",
    "    \"model\": \"maskrcnn\",\n",
    "    \"files\": {\"File\": [\"14.1.jpg\", \"15.1.jpg\"], \"Scene\": [1, 1]},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8720033",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(x, \"glomeruli2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66021be",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.load(\n",
    "    r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Custom - Erythrozyten\\Custom - Erythrozyten\\maskrcnn.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd83880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(r\"F:\\downloads\\10.1.jpg\")\n",
    "\n",
    "# Dimensions of the image\n",
    "sizeX = img.shape[1]\n",
    "sizeY = img.shape[0]\n",
    "block_size = 512\n",
    "\n",
    "nRows = int(sizeY / block_size)\n",
    "# Number of columns\n",
    "mCols = int(sizeX / block_size)\n",
    "\n",
    "# Reading image\n",
    "\n",
    "# print img\n",
    "\n",
    "# cv2.imshow('image',img)\n",
    "\n",
    "\n",
    "print(img.shape)\n",
    "\n",
    "\n",
    "for i in range(0, nRows):\n",
    "    for j in range(0, mCols):\n",
    "        roi = img[\n",
    "            int(i * sizeY / nRows) : int(i * sizeY / nRows + sizeY / nRows),\n",
    "            int(j * sizeX / mCols) : int(j * sizeX / mCols + sizeX / mCols),\n",
    "        ]\n",
    "        cv2.imwrite(\"patches/10patch_\" + str(i) + str(j) + \".png\", roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5141cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imread(r\"C:\\Users\\phili\\Downloads\\testimgage.PNG\")[:, :, ::-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becffcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_box(y, x, empty, size=2):\n",
    "    if np.count_nonzero(empty[y - size : y + size, x - size : x + size]) != 0:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d20dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52dc93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = vdl.get_inference_model(r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\glomeruli.pt\", type = \"instance\")\n",
    "img = cv2.imread(\n",
    "    r\"G:\\Windows\\Datasets\\Liver\\Dataset Liver gH2Ax Signals 256 v3\\dataset_128\\valid\\images\\19_Gr6T4_1s_2r_2728x_1488y_.png\"\n",
    ")[\n",
    "    :, :, ::-1\n",
    "]  # [:,:,::-1]#.astype(np.float32)\n",
    "img = cv2.resize(img, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "start = time.time()\n",
    "out = model.predict([img])[0]\n",
    "th = 0.25\n",
    "vals = 50\n",
    "final = np.zeros((img.shape[1], img.shape[1]))\n",
    "# for cnt, (mask, sc) in enumerate(zip(out[0]['masks'], out[0]['scores'])):\n",
    "for cnt, (box, mask, sc) in enumerate(zip(out[0], out[3], out[2])):\n",
    "    if sc.item() > 0.35:\n",
    "        mas = mask\n",
    "        box = list(map(int, box))\n",
    "        mas[mas < th] = 0\n",
    "        mas[mas >= th] = 255\n",
    "        if np.count_nonzero(final[mas[0] == 255]) <= 5000:\n",
    "            final[mas[0] == 255] = random.randint(50, 255)\n",
    "        # cv2.rectangle(final, (box[0], box[1]), (box[2], box[3]), 255, 1)\n",
    "        vals += 1\n",
    "print(time.time() - start)\n",
    "cv2.imwrite(\"xd.png\", final)\n",
    "cv2.imwrite(\"out_img.png\", img)\n",
    "plt.imshow(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a31f7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6af1a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict([img], confidence=0.15)[0]\n",
    "final = np.zeros((img.shape[1], img.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b853dd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pred[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187fb851",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\n",
    "    r\"D:\\Hsa\\temp\\custom_models\\verified_models\\Custom - test_4_PHH3\\instance_seg\\006.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b3126",
   "metadata": {},
   "outputs": [],
   "source": [
    "model[\"custom_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9222aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "th = 0.5\n",
    "for cnt, (out_mask, label, sc) in enumerate(zip(pred[3], pred[1], pred[2])):\n",
    "    if sc.item() > 0:\n",
    "        mas = out_mask\n",
    "        mas[mas < th] = 0\n",
    "        mas[mas >= th] = 255\n",
    "        # fix contour in contour\n",
    "        if np.count_nonzero(final[mas[0] == 255]) <= 100:\n",
    "            final[mas[0] == 255] = random.randint(50, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fa62e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd.png\", final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e4e193",
   "metadata": {},
   "source": [
    "# Evaluation for a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffb5798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder = r\"F:\\source\\repos\\VisualDL\\custom_experiments\\EMSErlangen\\imgs\"\n",
    "for im in os.listdir(folder):\n",
    "    img = io.imread(os.path.join(folder, im)).astype(np.float32)\n",
    "    # img = cv2.resize(img, (512, 512), interpolation = cv2.INTER_AREA)\n",
    "    img = img / 255.0\n",
    "    inp = torch.tensor(img, dtype=torch.float).permute(2, 0, 1).unsqueeze(0)\n",
    "    with torch.cuda.amp.autocast():\n",
    "        out = model(inp)  # ab hier\n",
    "        print(len(out))\n",
    "    th = 0.5\n",
    "    vals = 50\n",
    "    final = np.zeros((img.shape[1], img.shape[1]))\n",
    "    for cnt, (mask, sc) in enumerate(zip(out[0][\"masks\"], out[0][\"scores\"])):\n",
    "        if sc.item() > 0.0:\n",
    "            mas = mask.detach().numpy()\n",
    "            mas[mas < th] = 0\n",
    "            mas[mas >= th] = 255\n",
    "            if np.count_nonzero(final[mas[0] == 255]) <= 150:\n",
    "                final[mas[0] == 255] = random.randint(50, 255)\n",
    "\n",
    "            vals += 1\n",
    "    img = (img * 255.0).astype(np.uint8)\n",
    "    # final = cv2.cvtColor(final.astype(np.float32),cv2.COLOR_GRAY2RGB).astype(np.uint8)\n",
    "    img[final > 0] = 255\n",
    "    # wei = cv2.addWeighted(final, 0.6, img, 1.3, 0)\n",
    "    cv2.imwrite(os.path.join(folder, \"mask_\" + im), final)\n",
    "    # cv2.imwrite(os.path.join(folder, \"mask_\" + im), img[:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d286a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610c983e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = io.imread(\n",
    "    r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\256neu\\dataset\\valid\\images\\07__1s_0r_448x_224y_.png\"\n",
    ").astype(np.float32)\n",
    "img = img / 255.0\n",
    "inp = torch.tensor(img, dtype=torch.float).permute(2, 0, 1).unsqueeze(0)\n",
    "out = model(inp)  # ab hier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8904de60",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(out[0].values())[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0ce851",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed9002",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = len(list(filter(lambda x: x >= threshold, list(out[0].values())[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c5c8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(out[0].values())[0].cpu().detach().numpy()[0:u]\n",
    "list(out[0].values())[1].cpu().detach().numpy()[0:u]\n",
    "list(out[0].values())[0].cpu().detach().numpy()[0:u]\n",
    "list(out[0].values())[0].cpu().detach().numpy()[0:u]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4615753",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [list(out[0].values())[i].cpu().detach().numpy()[0:u] for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b69feba",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b2604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3735e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4838af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_preds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f3e7d8",
   "metadata": {},
   "source": [
    "# Split into tiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe74dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0da529",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = r\"C:/Users/phili/Downloads/Telegram Desktop/NeueZellen/dataset/train/images\"\n",
    "to = r\"F:\\source\\repos\\VisualDL\\custom_experiments\\bayer\\train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9afb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nRows = 2\n",
    "mCols = 2\n",
    "for cnt, im in enumerate(os.listdir(folder)):\n",
    "    label_img = cv2.imread(os.path.join(folder.replace(\"images\", \"labels\"), im), 0)\n",
    "    real_img = cv2.imread(os.path.join(folder, im))\n",
    "    sizeX = real_img.shape[1]\n",
    "    sizeY = real_img.shape[0]\n",
    "    for i in range(0, nRows):\n",
    "        for j in range(0, mCols):\n",
    "            roi_label = label_img[\n",
    "                int(i * sizeY / nRows) : int(i * sizeY / nRows + sizeY / nRows),\n",
    "                int(j * sizeX / mCols) : int(j * sizeX / mCols + sizeX / mCols),\n",
    "            ]\n",
    "            roi_image = real_img[\n",
    "                int(i * sizeY / nRows) : int(i * sizeY / nRows + sizeY / nRows),\n",
    "                int(j * sizeX / mCols) : int(j * sizeX / mCols + sizeX / mCols),\n",
    "            ]\n",
    "            cv2.imwrite(\n",
    "                os.path.join(os.path.join(to, \"images\"), f\"{i}_ {j}_{im}\"), roi_image\n",
    "            )\n",
    "            cv2.imwrite(\n",
    "                os.path.join(os.path.join(to, \"labels\"), f\"{i}_ {j}_{im}\"), roi_label\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa3342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6697e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\n",
    "    \"xd.png\",\n",
    "    cv2.resize(\n",
    "        cv2.imread(\n",
    "            r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\256neu\\dataset\\valid\\labels\\06__1s_3r_1344x_1120y_.png\"\n",
    "        )\n",
    "        * 255,\n",
    "        (512, 512),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87421d0f",
   "metadata": {},
   "source": [
    "# Test Smaller anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b3fc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "import torchvision\n",
    "\n",
    "anchor_sizes = ((8,), (16,), (32,), (64,), (128,))\n",
    "aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "\n",
    "rpn_anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7519a4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n",
    "    rpn_anchor_generator=rpn_anchor_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114a775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912e2cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776a50ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(r\"F:\\source\\repos\\VisualDL\\tu-resnest50d, UnetPlusPlus.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63837085",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6413789",
   "metadata": {},
   "outputs": [],
   "source": [
    "model[\"validation_metrics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641de32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualdl import vdl\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115710c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vdl.get_inference_model(\n",
    "    r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Custom - Erythrozyten\\Custom - Erythrozyten\\maskrcnn.pt\",\n",
    "    type=\"instance\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c35543",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(\n",
    "    [\n",
    "        cv2.imread(\n",
    "            r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\AbsolutFinal\\dataset\\train\\images\\05__1s_0r_3136x_448y_.png\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577df4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225c3c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.load(\n",
    "    r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\Custom - Eritrozyeten_seg\\Custom - Eritrozyeten_seg\\001.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc45565",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[\"validation_metrics\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea964c6",
   "metadata": {},
   "source": [
    "# Yas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b732d750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from h_transformer_1d import HTransformer1D\n",
    "\n",
    "model = HTransformer1D(\n",
    "    num_tokens=256,  # number of tokens\n",
    "    dim=512,  # dimension\n",
    "    depth=4,  # depth\n",
    "    causal=False,  # autoregressive or not\n",
    "    max_seq_len=65536,  # maximum sequence length\n",
    "    heads=2,  # heads\n",
    "    dim_head=64,  # dimension per head\n",
    "    block_size=128,  # block size\n",
    "    reversible=True,  # use reversibility, to save on memory with increased depth\n",
    "    shift_tokens=True,  # whether to shift half the feature space by one along the sequence dimension, for faster convergence (experimental feature)\n",
    ").cuda()\n",
    "\n",
    "x = torch.randint(0, 256, (1, 65000)).cuda()  # variable sequence length\n",
    "mask = torch.ones((1, 65000)).bool().cuda()  # variable mask length\n",
    "\n",
    "# network will automatically pad to power of 2, do hierarchical attention, etc\n",
    "\n",
    "logits = model(x, mask=mask)  # (1, 8000, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa31d6d",
   "metadata": {},
   "source": [
    "# MaskRCNN with custom backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0add29d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import torch\n",
    "\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# now get the number of input features for the mask classifier\n",
    "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "hidden_layer = 256\n",
    "# and replace the mask predictor with a new one\n",
    "model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
    "    in_features_mask, hidden_layer, num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca228052",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"erlangen.pt\")[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6755b90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2385749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaed81f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\n",
    "    r\"C:\\Users\\phili\\Downloads\\Telegram Desktop\\TuboliNew\\valid\\labels\\ANCA_002_PAS-Stufen_0s_0r_3584x_0y_.png\"\n",
    ")\n",
    "img = cv2.resize(img, (1333, 1333))\n",
    "cv2.imwrite(\"xd.png\", img * 255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513994e0",
   "metadata": {},
   "source": [
    "# Test Segformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62617a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec15beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    SegformerFeatureExtractor,\n",
    "    SegformerModel,\n",
    "    SegformerForSemanticSegmentation,\n",
    ")\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/segformer-b0-finetuned-ade-512-512\"\n",
    ")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inp = torch.rand(2, 3, 512, 512)\n",
    "# sequence_output = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051fd5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(inp).logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a96cfc",
   "metadata": {},
   "source": [
    "# test od dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6061c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# path = \"\"\n",
    "path = r\"D:\\Hsa\\temp\\projects\\2269f3d2-03fa-48f9-af39-3db27333ee4a\\dataset\\train\"\n",
    "imgs = os.listdir(os.path.join(path, \"images\"))\n",
    "if not os.path.isdir(os.path.join(path, \"results\")):\n",
    "    os.mkdir(os.path.join(path, \"results\"))\n",
    "for img_name in imgs:\n",
    "    image = cv2.imread(os.path.join(os.path.join(path, \"images\"), img_name))\n",
    "    dh, dw, _ = image.shape\n",
    "\n",
    "    fl = open(\n",
    "        os.path.join(os.path.join(path, \"labels\"), img_name.replace(\".png\", \".txt\")),\n",
    "        \"r\",\n",
    "    )\n",
    "    data = fl.readlines()\n",
    "    fl.close()\n",
    "\n",
    "    for dt in data:\n",
    "\n",
    "        # Split string to float\n",
    "        _, x, y, w, h = map(float, dt.split(\" \"))\n",
    "\n",
    "        # Taken from https://github.com/pjreddie/darknet/blob/810d7f797bdb2f021dbe65d2524c2ff6b8ab5c8b/src/image.c#L283-L291\n",
    "        # via https://stackoverflow.com/questions/44544471/how-to-get-the-coordinates-of-the-bounding-box-in-yolo-object-detection#comment102178409_44592380\n",
    "        l = int(((x - w / 2) * dw))\n",
    "        r = int(((x + w / 2) * dw))\n",
    "        t = int(((y - h / 2) * dh))\n",
    "        b = int(((y + h / 2) * dh))\n",
    "\n",
    "        if l < 0:\n",
    "            l = 0\n",
    "        if r > dw - 1:\n",
    "            r = dw - 1\n",
    "        if t < 0:\n",
    "            t = 0\n",
    "        if b > dh - 1:\n",
    "            b = dh - 1\n",
    "\n",
    "        cv2.rectangle(image, (l, t), (r, b), (0, 0, 255), 1)\n",
    "\n",
    "    new_path = os.path.join(os.path.join(path, \"results/\"), img_name)\n",
    "    cv2.imwrite(new_path, image)\n",
    "    # print(\"img created:\", new_path)\n",
    "\n",
    "print(\"finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634f3f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualdl import vdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94be9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdl.get_inference_model(\n",
    "    r\"D:\\Hsa\\temp\\custom_models\\verified_models\\Custom - mat22.09\\001.pt\"\n",
    ").state[\"custom_data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f34f39",
   "metadata": {},
   "source": [
    "# Icevision test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af97007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from icevision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1183868",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"0\", \"1\", \"2\", \"3\"]\n",
    "class_map = ClassMap(classes)\n",
    "\n",
    "train_parser = parsers.COCOBBoxParser(\n",
    "    annotations_filepath=r\"F:\\source\\repos\\YOLOX\\datasets\\testimages\\annotations\\instances_train2017.json\",\n",
    "    img_dir=r\"F:\\source\\repos\\YOLOX\\datasets\\testimages\\train2017\",\n",
    ")\n",
    "valid_parser = parsers.COCOBBoxParser(\n",
    "    annotations_filepath=r\"F:\\source\\repos\\YOLOX\\datasets\\testimages\\annotations\\instances_train2017.json\",\n",
    "    img_dir=r\"F:\\source\\repos\\YOLOX\\datasets\\testimages\\train2017\",\n",
    ")\n",
    "whole = SingleSplitSplitter()\n",
    "train_records, *_ = train_parser.parse(data_splitter=whole)\n",
    "valid_records, *_ = valid_parser.parse(data_splitter=whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a71242",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = models.ultralytics.yolov5\n",
    "backbone = model_type.backbones.small\n",
    "# The yolov5 model requires an img_size parameter\n",
    "# Instantiate the mdoel\n",
    "model = model_type.model(\n",
    "    backbone=backbone(pretrained=True), num_classes=len(classes), img_size=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b1e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "presize = 256  #\n",
    "image_size = 256  #\n",
    "\n",
    "train_tfms = tfms.A.Adapter([*tfms.A.aug_tfms(size=presize), tfms.A.Normalize()])\n",
    "valid_tfms = tfms.A.Adapter(\n",
    "    [*tfms.A.resize_and_pad(size=image_size), tfms.A.Normalize()]\n",
    ")\n",
    "\n",
    "\n",
    "train_ds = Dataset(train_records, train_tfms)\n",
    "valid_ds = Dataset(valid_records, valid_tfms)\n",
    "\n",
    "metrics = [COCOMetric(metric_type=COCOMetricType.bbox)]\n",
    "train_dl = model_type.train_dl(train_ds, batch_size=2, num_workers=0, shuffle=True)\n",
    "valid_dl = model_type.valid_dl(valid_ds, batch_size=2, num_workers=0, shuffle=False)\n",
    "model_type.show_batch(first(valid_dl), ncols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48619e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightModel(model_type.lightning.ModelAdapter):\n",
    "    def configure_optimizers(self):\n",
    "        return SGD(self.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "light_model = LightModel(model, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed8c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=20, gpus=1, log_every_n_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3445d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c7ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(light_model, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ecb72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_dl = model_type.infer_dl(valid_ds, batch_size=4, shuffle=False)\n",
    "preds = model_type.predict_from_dl(model, infer_dl, keep_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05514fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac704269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from icevision.models import *\n",
    "\n",
    "checkpoint_and_model = model_from_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dfcff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        r\"F:\\source\\repos\\YOLOX\\lightning_logs\\version_1\\checkpoints\\epoch=72-step=3430.ckpt\"\n",
    "    )[\"state_dict\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff24e351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e9e6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.nn.Conv2d(3, 6, (2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedc1e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.rand((1, 3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5b932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebabeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data = dict()\n",
    "\n",
    "\n",
    "def get_label(name):\n",
    "    if \"Chrom\" in name:\n",
    "        return 0\n",
    "    elif \"Erioglau\" in name:\n",
    "        return 1\n",
    "    return 2\n",
    "\n",
    "\n",
    "def retrieve_data(path):\n",
    "    with open(path, encoding=\"utf-8\") as handle:\n",
    "        data = pd.read_csv(path, sep=\";\", skiprows=list(range(0, 89)), header=None)\n",
    "        xx = list(\n",
    "            map(\n",
    "                lambda x: [\n",
    "                    float(x[i].replace(\",\", \".\")) if type(x[i]) is str else x[i]\n",
    "                    for i in range(2)\n",
    "                ],\n",
    "                data.values.tolist(),\n",
    "            )\n",
    "        )\n",
    "    label = get_label(path)\n",
    "    return xx, label\n",
    "\n",
    "\n",
    "train_x, train_y = [], []\n",
    "test_x, test_y = [], []\n",
    "for cnt, csv in enumerate(\n",
    "    os.listdir(r\"F:\\source\\repos\\Evonik\\Evonik\\Spektren\\Einzelsubstanzen\\csv\")\n",
    "):\n",
    "    path = os.path.join(\n",
    "        r\"F:\\source\\repos\\Evonik\\Evonik\\Spektren\\Einzelsubstanzen\\csv\", csv\n",
    "    )\n",
    "    if \"Eriogl\" in csv:\n",
    "        if csv.split(\".\")[0][-1] == \"2\":\n",
    "            val = 12.0\n",
    "        elif csv.split(\".\")[0][-1] == \"0\":\n",
    "            val = 0.5\n",
    "        elif csv.split(\".\")[0][-1] == \"1\":\n",
    "            val = 1.5\n",
    "        else:\n",
    "            val = csv.split(\".\")[0][-1]\n",
    "    else:\n",
    "        if csv.split(\",\")[0][-1] == \"2\":\n",
    "            val = 12.0\n",
    "        elif csv.split(\",\")[0][-1] == \"0\":\n",
    "            val = 0.5\n",
    "        elif csv.split(\",\")[0][-1] == \"1\":\n",
    "            val = 1.5\n",
    "        else:\n",
    "            val = csv.split(\",\")[0][-1]\n",
    "    val = float(val)\n",
    "    x, y = retrieve_data(path)\n",
    "    if cnt % 6 == 0:\n",
    "        test_x.append(x)\n",
    "        test_y.append((y, val))\n",
    "        continue\n",
    "    train_x.append(x)\n",
    "    train_y.append((y, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d28c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec71168",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict = {}\n",
    "for cnt, (x, y) in enumerate(zip(train_x, train_y)):\n",
    "    final_dict[cnt] = {\n",
    "        \"series\": x,\n",
    "        \"class\": [0.0, 1.0, 1.0],\n",
    "        \"continuous\": [y[1], y[1] + 0.5],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cb8c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"rnn2.json\", \"w\", encoding=\"utf-8\") as handle:\n",
    "    json.dump(final_dict, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9ea7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd57becf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"rnn2.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66bd3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"0\"][\"continuous\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177cbb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dced109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in final_dict.items():\n",
    "    print(val[\"class\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa74424",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bf5a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict[0][\"continuous\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab27b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eed28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.ones([10, 64], dtype=torch.float32)  # 64 classes, batch size = 10\n",
    "output = torch.full([10, 64], 1.5)  # A prediction (logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef8071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ee435",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a0a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b123649",
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099bccd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d740764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.morphology import skeletonize\n",
    "from skimage import data\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.util import invert\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv2.imread(\"tester.jpg\", 0)\n",
    "img2 = cv2.imread(\"tester.jpg\", cv2.IMREAD_COLOR)\n",
    "img2 = np.zeros_like(img)\n",
    "\n",
    "\n",
    "img[img > 0] = 1\n",
    "img = cv2.GaussianBlur(img, (5, 5), cv2.BORDER_DEFAULT)\n",
    "img = cv2.GaussianBlur(img, (5, 5), cv2.BORDER_DEFAULT)\n",
    "skel = skeletonize(img).astype(int)\n",
    "skel[skel > 0] = 255\n",
    "cv2.imwrite(\"xdd.png\", skel)\n",
    "\n",
    "contours, _ = cv2.findContours(img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "font = cv2.FONT_HERSHEY_COMPLEX  #\n",
    "\n",
    "\n",
    "def intermediates(p1, p2, nb_points=6):\n",
    "    \"\"\" \"Return a list of nb_points equally spaced points\n",
    "    between p1 and p2\"\"\"\n",
    "    # If we have 8 intermediate points, we have 8+1=9 spaces\n",
    "    # between p1 and p2\n",
    "    x_spacing = (p2[0] - p1[0]) / (nb_points + 1)\n",
    "    y_spacing = (p2[1] - p1[1]) / (nb_points + 1)\n",
    "\n",
    "    return [\n",
    "        [p1[0] + i * x_spacing, p1[1] + i * y_spacing] for i in range(1, nb_points + 1)\n",
    "    ]\n",
    "\n",
    "\n",
    "color = 0\n",
    "areas = [cv2.contourArea(c) for c in contours]\n",
    "max_index = np.argmax(areas)\n",
    "cnt = contours[max_index]\n",
    "print(cnt.shape)\n",
    "# for cnt in contours :\n",
    "flipped_cnt = np.flip(cnt, 0)\n",
    "points = []\n",
    "for pos1, pos2 in zip(cnt, flipped_cnt):\n",
    "    [[x1, y1]] = pos1\n",
    "    [[x2, y2]] = pos2\n",
    "    points.append(intermediates((x1, y1), (x2, y2)))\n",
    "points = np.array(points)\n",
    "for i in range(6):\n",
    "    old_point = None\n",
    "    for point in points[:, i]:\n",
    "        point = np.array(point).astype(int)\n",
    "        if old_point is not None:\n",
    "            cv2.line(img2, old_point, point, 255, 1)\n",
    "        old_point = point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e505c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff5deb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7d72ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"xd.png\", img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94efed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.scatter(center[0], center[1], marker=\"X\")\n",
    "plt.plot([center[0], endx], [center[1], endy])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0931517",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
